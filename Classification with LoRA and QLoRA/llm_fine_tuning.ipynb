{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "816648c3",
      "metadata": {
        "id": "816648c3"
      },
      "source": [
        "Prompt-Based Inference (No Fine-Tuning)\n",
        "Used a pre-trained instruction-tuned LLM:\n",
        "\n",
        "mistralai/Mistral-7B-Instruct-v0.1, or\n",
        "\n",
        "meta-llama/Llama-2-7b-chat-hf or any other model of your choice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to notebook --ClearOutputPreprocessor.enabled=True your_notebook.ipynb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH6B0aYEwP3-",
        "outputId": "736b6eec-e304-4409-82a2-03897b675651"
      },
      "id": "QH6B0aYEwP3-",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern 'your_notebook.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--coalesce-streams\n",
            "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    Overwrite base name use for output files.\n",
            "                Supports pattern replacements '{notebook_name}'.\n",
            "    Default: '{notebook_name}'\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "773ae283",
      "metadata": {
        "id": "773ae283",
        "outputId": "02bc66bd-cf03-46f3-86e7-6b9b7b578bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/python3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/python3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "\n",
        "# Install required libraries\n",
        "import subprocess\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"datasets\", \"torch\", \"accelerate\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4db6211a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db6211a",
        "outputId": "66ed6f9d-dbeb-4aed-8eaf-652e753f203b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset loaded: 7600 examples\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load AG News dataset from Hugging Face\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "test_data = dataset[\"test\"]\n",
        "print(f\"Test dataset loaded: {len(test_data)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4459b042",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514,
          "referenced_widgets": [
            "6119d27dfd0b47c08725411839e6017b",
            "ceb209032cf04430af6bacd7f6bead90",
            "d17495a13ac541c292daf68ae4fa60d4",
            "b213006807cd41439517197eb0ed5c64",
            "0e989162f62b494a81cf8c9b6ab8110b",
            "a623e0c54a37459b999615a60d42652f",
            "e945a4d43b4740ef897b13393654cb9e",
            "d3001c76684448a7ad2cfad0160a0105",
            "83c8b7bc9c1f43adb452875c16ec41c7",
            "f620b872fa4441ae91e5cd7b27fc5165",
            "7417052842ad4da8a5d6423473096805",
            "8e468970594b4cce9913c9860d8c5615",
            "c1c3447af7344d479bd3efb21da7f16d",
            "2b3720b14b9246b19d13b5ccbbc25b4c",
            "82ff770b4bcd4bbfb4dcf233b4265a18",
            "55b1e60856064f0ca6dc926ae5155f80",
            "8cfa1cd8466b46c4a17df3544932a329",
            "45a135580aa7414abd396c37b8fc8e9d",
            "542f192c978a471481d58f5125bbc1f5",
            "6350d857f19842b7ad535cfe9ceaf80b",
            "3432cca2bbb3464b8cdf811567991ba7",
            "8f923868ba0a46a4a6b14df4db6e1a7c",
            "94d70e57c00248ac92de27d2714fa37f",
            "27c39124aaa042188a906955de4a55b5",
            "296a7853739945d691a121598d5169ea",
            "e47b6961c5b54849aa68c359fa4a7852",
            "fbb6b783a3c048d2ab278a83eccb9114",
            "ad32a47add0b4e7d8f53cbba06433d32",
            "5046cfdd9c894f56b9748cf6913de717",
            "5865c5686c57435a9616ba2aba79550b",
            "778711ba343d46a5a3cec93aedb9790e",
            "5b202f9101de492d91145a9a68d3c532",
            "009b32ad1d1f40b38f7fde2d2df000d1",
            "b461e847326e4f17bde9f8217449c82f",
            "4af46ed46f5b48c6867fd167d127a534",
            "14dbf4f5ce644b6abc6d8a0fba07a493",
            "5581420550384fbf8a8e3ad602156213",
            "d94a01a760fd40d99b56fab2f0c22e9a",
            "29e5c96cdf1847d5aad9ad4b7417d86f",
            "6f71af5af1bf45e4bc14212ef0e9048a",
            "36406d538cbf43dd8fc9eccb66e4004f",
            "a09e7ea61cdb405d851943b2082fe16c",
            "b826d673cc05450983bebedfa87ab937",
            "dd25e3f0f8c147a689a6c24aea18d0a1",
            "d4544c5700f14e5386e693fa84526ef9",
            "2050c3ff65f8467ba6bf761320b3359f",
            "c8d84654a00441bba178e7d4df51fefa",
            "5c101a135f3643829e8562b8351d1c5b",
            "5cea17ccb7cb416198e0cc6c97cff2e2",
            "cb41df65932c4c848ef0462d6c455d37",
            "4a03ef34f28042878c614d1f0da3bc2c",
            "1839cd60794647c9941e783a70d50280",
            "cdf793efe01c408daa20732f7864b387",
            "7a48f5fe848147db8de28cedcfe13358",
            "36e2ac8776d249fc9973c2941bc9b317",
            "79096278bbe4495191b2816a7074afa4",
            "c4770d23e4be43fba7b4dbadbe931b8c",
            "8c63e92a906542e3bb071e2754725cae",
            "4fc19e5fdb6b4be49766a73dc158e411",
            "c18a87109730434db836458855a186f9",
            "cf6d672e3d704606b2a30013f2ca31dd",
            "bdd099f68e22425c9771caca123bbeef",
            "b73ae0565ced4f9493579ada09e6a3d8",
            "1bc20d71dfbf4700a8ce5daa63e4d098",
            "df573d3ccb07464bbb10e57e2fc06dab",
            "8f5707c5bcdf47749997938497added6",
            "262d0eee9f6b4af3a13717991681ef76",
            "c82d7aec3b0a46c2918e889db3bd1d2f",
            "923e3eb0e3904a7ea97ced61cdea2881",
            "a4b45c71d6034f6e92ac9b9e46b71216",
            "d7a0b53317f645a78aa8e478e20048fb",
            "08b2fa84110d42cc953960d1baa474f4",
            "d4269d5618494573bbb9a853cda3e87f",
            "665e0f6956204828a65175deaee5f6fb",
            "2e9d45a1a22c476ca3fc9088ae2f279e",
            "aee1be0a32d44308a326c5cdeb0bcf43",
            "184fdfd3de2c48ebacacf0967bdfb925",
            "532bae22a562449da6965c6ba0acff26",
            "e0b9de2676854b23ba62a44a2248adfc",
            "b8525919a3344f7ca1c57e3caa851482",
            "8f0969995a4246378cc0d97490537591",
            "d00e105459154af69ffe1d30e87ac176",
            "570cafd424fa4703b069995c48997208",
            "e99b8bc4ba384b4b8aa9558104c4ab11",
            "0af2ca69c20a458f9d08e46adbe712b5",
            "b8f31108d1674ce6853b3c72d4cafa54",
            "5bea98df8d7c4b889236e1015ae6ccd4",
            "31423921f11944eaae78522791167466",
            "b21deb7fd8624e2c862bc37e91a04251",
            "e3a6be21a7094fa5a81768586aaadb2f",
            "ecbdfb480acf4015a3ed2e39f955de1d",
            "e51ba6d2a27949099a8d87d53233b223",
            "8c625a72483247deb27b7efd563faaf8",
            "c327bbfc70e94b50b3e0aa46c96a52e6",
            "f5e6ebbba2a749d1b77f0025a1526479",
            "d2261de6586341de8defdced195c3cc7",
            "b5e82eacfaf34708b803dabc17fe9cb6",
            "88acee26436f485faab32a5290310277",
            "227b5f56533a4459aadf014542e3de6b",
            "82e00785ce75415d8e071f72cf399e55",
            "2e0cd7b295f64d4d80e90e7e81e7aa97",
            "4589f34f57624e52b1e1f35128baeeb1",
            "036446151cfd4d7ca9d302a2f544fc4e",
            "c217b9e66154473881d2aee482c1776e",
            "d44e2c9936064147a2e3e44fa62aa73c",
            "1ad26f0d0a3d49ddab3ffcb0f64c643c",
            "54fbf3fa23e1471f8b2748967fd67e57",
            "e47e709d1a6e4737b3b22bfab63d01c3",
            "9fa0a7f059ad432b807107d3e2c80c17",
            "b05e8034bf0a460a93d4c840380635dd",
            "9248f0b7c9d74a00b74cb3667c547ea2",
            "c77fde2db75f407d87a5e4b87621e5d6",
            "ee7665a97594402184809ffdee552653",
            "f68ea9d50ae6458a86f19e4d6235376f",
            "1fa3cb8390104e8b885b8329d4d162e0",
            "625e2475e4914889a03c033775d2915a",
            "afbff346ae714cd382981dc911733ddc",
            "b5e875f8001147a69f4913aee5d4c4d1",
            "a3ff5c772c604c1a9dd5ca0d90c89fff",
            "164d9ab660e64f7abbe907cc3a0bf330",
            "610dffb2f45c4a8890c193044709c936"
          ]
        },
        "id": "4459b042",
        "outputId": "a4cfde81-c8c4-4ca1-cd81-9b4a71520187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6119d27dfd0b47c08725411839e6017b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e468970594b4cce9913c9860d8c5615"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94d70e57c00248ac92de27d2714fa37f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b461e847326e4f17bde9f8217449c82f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4544c5700f14e5386e693fa84526ef9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79096278bbe4495191b2816a7074afa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "262d0eee9f6b4af3a13717991681ef76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "532bae22a562449da6965c6ba0acff26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b21deb7fd8624e2c862bc37e91a04251"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82e00785ce75415d8e071f72cf399e55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9248f0b7c9d74a00b74cb3667c547ea2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "cache_dir = \"./model_cache\"\n",
        "\n",
        "# Check if cache has valid model files\n",
        "def is_cache_valid(cache_path):\n",
        "    if not os.path.exists(cache_path):\n",
        "        return False\n",
        "    required_files = ['pytorch_model.bin', 'model.safetensors', 'config.json']\n",
        "    return any(os.path.exists(os.path.join(cache_path, f)) for f in required_files)\n",
        "\n",
        "# Load or download model\n",
        "if is_cache_valid(cache_dir):\n",
        "    print(\"Loading from cache...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cache_dir)\n",
        "    model = AutoModelForCausalLM.from_pretrained(cache_dir, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "else:\n",
        "    print(\"Downloading model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9dc7e4a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dc7e4a6",
        "outputId": "3aba375b-14df-4f77-d8a8-635e666d401c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping and prompt function created\n"
          ]
        }
      ],
      "source": [
        "# Define label mapping and create classification prompt\n",
        "label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
        "\n",
        "def create_prompt(text):\n",
        "    return f\"\"\"Classify the following news headline into one of the categories: World, Sports, Business, Sci/Tech.\n",
        "\n",
        "Text: \"{text}\"\n",
        "Label:\"\"\"\n",
        "\n",
        "print(\"Label mapping and prompt function created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f728f3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4f728f3a",
        "outputId": "79b438a8-cdc8-4e3c-9997-6050a54f1b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample example: {'text': \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\", 'label': 2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Baseline inference:   0%|          | 0/7600 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 1/7600 [00:03<8:16:43,  3.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 2/7600 [00:06<6:19:48,  3.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 3/7600 [00:08<5:52:53,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 4/7600 [00:10<5:22:38,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 5/7600 [00:13<5:18:27,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 6/7600 [00:16<5:47:41,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 7/7600 [00:18<5:23:04,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 8/7600 [00:21<5:12:10,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 9/7600 [00:23<4:59:05,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 10/7600 [00:24<4:29:31,  2.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 11/7600 [00:28<5:30:36,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 12/7600 [00:30<5:09:08,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 13/7600 [00:31<4:07:41,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 14/7600 [00:33<4:15:27,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 15/7600 [00:35<4:26:02,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 16/7600 [00:39<5:09:41,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 17/7600 [00:42<5:50:36,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 18/7600 [00:44<5:09:03,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 19/7600 [00:46<4:39:16,  2.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 20/7600 [00:46<3:41:35,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 21/7600 [00:47<3:01:47,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 22/7600 [00:49<3:09:50,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 23/7600 [00:50<3:15:40,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 24/7600 [00:51<2:45:21,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 25/7600 [00:52<2:23:10,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 26/7600 [00:53<2:47:34,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 27/7600 [00:55<3:00:38,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 28/7600 [00:56<2:34:28,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 29/7600 [00:57<2:14:52,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 30/7600 [00:57<2:01:06,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 31/7600 [00:58<1:50:56,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 32/7600 [00:59<1:43:39,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 33/7600 [00:59<1:38:44,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 34/7600 [01:01<2:12:31,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 35/7600 [01:02<1:59:04,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 36/7600 [01:02<1:51:23,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 37/7600 [01:03<1:45:53,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   0%|          | 38/7600 [01:05<2:22:53,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 39/7600 [01:06<2:06:13,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 40/7600 [01:06<1:54:22,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 41/7600 [01:07<1:46:11,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 42/7600 [01:08<1:40:58,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 43/7600 [01:09<1:36:47,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 44/7600 [01:09<1:34:11,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 45/7600 [01:10<1:32:17,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 46/7600 [01:11<1:30:28,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 47/7600 [01:11<1:29:41,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 48/7600 [01:12<1:29:18,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 49/7600 [01:13<1:28:34,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 50/7600 [01:13<1:27:55,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 51/7600 [01:14<1:27:42,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 52/7600 [01:15<1:28:24,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 53/7600 [01:16<1:30:06,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 54/7600 [01:16<1:30:27,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 55/7600 [01:17<1:33:58,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 56/7600 [01:19<2:08:51,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 57/7600 [01:19<1:56:26,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 58/7600 [01:20<1:47:30,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 59/7600 [01:21<1:41:17,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 60/7600 [01:22<1:37:06,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 61/7600 [01:22<1:33:51,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 62/7600 [01:23<1:31:15,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 63/7600 [01:24<1:32:01,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 64/7600 [01:24<1:30:43,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 65/7600 [01:25<1:29:30,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 66/7600 [01:27<2:05:51,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 67/7600 [01:27<1:55:57,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 68/7600 [01:28<1:48:30,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 69/7600 [01:29<1:44:56,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 70/7600 [01:30<1:41:25,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 71/7600 [01:31<2:13:35,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 72/7600 [01:32<2:01:49,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 73/7600 [01:34<2:28:27,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 74/7600 [01:35<2:46:30,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 75/7600 [01:37<2:59:24,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 76/7600 [01:39<3:08:06,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 77/7600 [01:39<2:38:32,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 78/7600 [01:40<2:18:45,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 79/7600 [01:42<2:44:28,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 80/7600 [01:44<3:11:42,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 81/7600 [01:45<3:05:52,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 82/7600 [01:48<3:28:36,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 83/7600 [01:49<3:08:08,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 84/7600 [01:49<2:37:42,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 85/7600 [01:50<2:16:30,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 86/7600 [01:51<2:01:48,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 87/7600 [01:51<1:51:28,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 88/7600 [01:52<1:46:19,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 89/7600 [01:53<1:44:22,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 90/7600 [01:54<1:44:29,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 91/7600 [01:55<1:39:15,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 92/7600 [01:55<1:35:19,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 93/7600 [01:56<1:32:46,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|          | 94/7600 [01:57<1:30:49,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 95/7600 [01:58<2:06:20,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 96/7600 [01:59<1:54:39,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 97/7600 [02:00<1:46:08,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 98/7600 [02:00<1:43:04,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 99/7600 [02:01<1:38:20,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 100/7600 [02:02<1:35:12,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 101/7600 [02:03<1:32:42,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 102/7600 [02:03<1:30:56,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 103/7600 [02:05<2:08:57,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 104/7600 [02:07<2:36:17,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 105/7600 [02:08<2:51:27,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 106/7600 [02:09<2:25:46,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 107/7600 [02:10<2:08:13,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 108/7600 [02:10<1:55:45,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 109/7600 [02:12<2:22:59,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 110/7600 [02:14<2:42:16,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 111/7600 [02:14<2:19:30,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 112/7600 [02:15<2:03:33,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   1%|▏         | 113/7600 [02:16<1:52:19,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 114/7600 [02:17<1:46:01,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 115/7600 [02:18<2:22:03,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 116/7600 [02:19<2:05:15,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 117/7600 [02:21<2:29:26,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 118/7600 [02:22<2:50:47,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 119/7600 [02:23<2:25:55,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 120/7600 [02:24<2:07:58,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 121/7600 [02:25<1:55:28,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 122/7600 [02:25<1:46:46,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 123/7600 [02:26<1:40:39,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 124/7600 [02:27<1:36:16,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 125/7600 [02:27<1:33:41,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 126/7600 [02:28<1:30:57,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 127/7600 [02:29<1:31:00,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 128/7600 [02:29<1:30:51,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 129/7600 [02:30<1:32:15,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 130/7600 [02:31<1:30:20,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 131/7600 [02:32<1:29:03,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 132/7600 [02:32<1:28:12,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 133/7600 [02:33<1:27:24,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 134/7600 [02:34<1:26:39,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 135/7600 [02:34<1:26:25,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 136/7600 [02:35<1:26:34,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 137/7600 [02:36<1:26:20,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 138/7600 [02:36<1:26:20,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 139/7600 [02:37<1:26:18,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 140/7600 [02:38<1:25:59,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 141/7600 [02:39<1:25:46,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 142/7600 [02:39<1:25:16,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 143/7600 [02:40<1:25:36,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 144/7600 [02:41<1:27:38,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 145/7600 [02:41<1:28:08,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 146/7600 [02:42<1:30:41,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 147/7600 [02:43<1:29:38,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 148/7600 [02:44<1:28:25,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 149/7600 [02:44<1:28:14,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 150/7600 [02:45<1:27:52,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 151/7600 [02:46<1:26:57,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 152/7600 [02:46<1:27:06,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 153/7600 [02:47<1:26:52,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 154/7600 [02:48<1:26:33,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 155/7600 [02:48<1:26:44,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 156/7600 [02:50<2:02:09,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 157/7600 [02:51<1:51:29,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 158/7600 [02:51<1:43:58,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 159/7600 [02:52<1:38:52,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 160/7600 [02:53<1:36:40,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 161/7600 [02:54<1:34:49,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 162/7600 [02:54<1:35:44,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 163/7600 [02:55<1:32:43,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 164/7600 [02:56<1:30:48,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 165/7600 [02:57<1:29:35,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 166/7600 [02:57<1:28:42,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 167/7600 [02:58<1:30:06,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 168/7600 [02:59<1:28:44,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 169/7600 [02:59<1:27:35,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 170/7600 [03:00<1:26:48,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 171/7600 [03:02<2:05:22,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 172/7600 [03:02<1:53:28,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 173/7600 [03:03<1:45:14,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 174/7600 [03:05<2:17:31,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 175/7600 [03:06<2:03:28,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 176/7600 [03:06<1:55:30,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 177/7600 [03:08<2:22:48,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 178/7600 [03:09<2:05:36,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 179/7600 [03:09<1:54:02,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 180/7600 [03:10<1:45:25,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 181/7600 [03:11<1:39:39,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 182/7600 [03:12<1:35:12,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 183/7600 [03:12<1:32:04,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 184/7600 [03:13<1:30:02,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 185/7600 [03:14<1:30:52,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 186/7600 [03:14<1:29:42,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 187/7600 [03:16<2:04:31,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 188/7600 [03:17<1:53:59,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▏         | 189/7600 [03:18<1:47:20,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   2%|▎         | 190/7600 [03:18<1:43:32,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 191/7600 [03:20<2:16:09,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 192/7600 [03:21<2:01:28,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 193/7600 [03:21<1:50:55,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 194/7600 [03:22<1:43:03,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 195/7600 [03:23<1:40:18,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 196/7600 [03:24<1:35:44,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 197/7600 [03:24<1:34:38,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 198/7600 [03:25<1:31:54,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 199/7600 [03:27<2:08:26,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 200/7600 [03:27<1:55:33,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 201/7600 [03:29<2:26:20,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 202/7600 [03:31<2:49:19,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 203/7600 [03:33<3:00:16,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 204/7600 [03:33<2:32:09,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 205/7600 [03:34<2:12:08,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 206/7600 [03:35<1:57:53,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 207/7600 [03:35<1:48:07,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 208/7600 [03:36<1:41:32,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 209/7600 [03:37<1:36:28,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 210/7600 [03:38<2:08:57,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 211/7600 [03:39<1:56:07,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 212/7600 [03:41<2:22:59,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 213/7600 [03:42<2:07:30,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 214/7600 [03:42<1:57:06,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 215/7600 [03:43<1:49:58,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 216/7600 [03:44<1:43:05,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 217/7600 [03:45<1:37:47,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 218/7600 [03:45<1:34:05,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 219/7600 [03:46<1:31:26,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 220/7600 [03:47<1:29:25,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 221/7600 [03:47<1:28:14,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 222/7600 [03:48<1:27:14,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 223/7600 [03:49<1:26:42,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 224/7600 [03:49<1:26:17,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 225/7600 [03:50<1:25:52,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 226/7600 [03:51<1:25:45,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 227/7600 [03:51<1:25:36,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 228/7600 [03:52<1:25:27,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 229/7600 [03:53<1:25:07,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 230/7600 [03:54<1:26:35,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 231/7600 [03:54<1:27:31,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 232/7600 [03:56<2:06:35,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 233/7600 [03:57<1:54:04,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 234/7600 [03:58<2:21:13,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 235/7600 [04:00<2:40:09,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 236/7600 [04:01<2:17:30,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 237/7600 [04:01<2:01:58,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 238/7600 [04:02<1:50:51,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 239/7600 [04:03<1:43:05,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 240/7600 [04:04<1:37:38,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 241/7600 [04:04<1:33:49,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 242/7600 [04:05<1:31:03,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 243/7600 [04:06<1:30:31,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 244/7600 [04:08<2:12:31,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 245/7600 [04:08<1:58:13,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 246/7600 [04:09<1:48:15,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 247/7600 [04:11<2:16:54,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 248/7600 [04:12<2:36:31,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 249/7600 [04:13<2:15:00,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 250/7600 [04:14<1:59:48,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 251/7600 [04:14<1:49:07,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 252/7600 [04:15<1:41:44,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 253/7600 [04:16<1:36:43,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 254/7600 [04:16<1:33:02,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 255/7600 [04:17<1:30:26,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 256/7600 [04:21<3:11:16,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 257/7600 [04:21<2:42:08,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 258/7600 [04:22<2:19:08,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 259/7600 [04:23<2:05:09,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 260/7600 [04:24<2:29:36,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 261/7600 [04:25<2:10:17,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 262/7600 [04:26<1:56:17,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 263/7600 [04:28<2:22:48,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 264/7600 [04:28<2:05:25,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   3%|▎         | 265/7600 [04:30<2:30:42,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 266/7600 [04:32<2:53:00,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 267/7600 [04:33<2:27:00,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 268/7600 [04:34<2:44:11,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 269/7600 [04:35<2:20:35,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 270/7600 [04:36<2:03:50,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 271/7600 [04:36<1:52:32,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 272/7600 [04:37<1:44:35,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 273/7600 [04:38<1:38:51,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 274/7600 [04:39<2:10:58,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 275/7600 [04:40<1:57:36,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 276/7600 [04:42<2:23:58,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 277/7600 [04:44<2:46:44,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 278/7600 [04:44<2:23:20,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 279/7600 [04:45<2:06:04,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 280/7600 [04:46<1:53:23,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 281/7600 [04:46<1:44:34,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 282/7600 [04:48<2:14:43,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 283/7600 [04:49<1:59:47,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▎         | 284/7600 [04:50<2:26:06,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 285/7600 [04:51<2:07:23,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 286/7600 [04:52<1:54:37,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 287/7600 [04:54<2:21:32,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 288/7600 [04:54<2:06:02,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 289/7600 [04:56<2:34:37,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 290/7600 [04:57<2:14:10,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 291/7600 [04:58<2:01:23,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 292/7600 [04:59<2:25:41,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 293/7600 [05:01<2:43:23,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 294/7600 [05:03<2:55:40,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 295/7600 [05:03<2:28:36,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 296/7600 [05:04<2:09:23,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 297/7600 [05:05<1:56:04,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 298/7600 [05:05<1:46:34,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 299/7600 [05:06<1:40:27,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 300/7600 [05:07<1:37:41,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 301/7600 [05:08<1:35:33,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 302/7600 [05:08<1:37:28,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 303/7600 [05:09<1:33:45,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 304/7600 [05:10<1:31:19,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 305/7600 [05:10<1:29:37,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 306/7600 [05:11<1:29:50,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 307/7600 [05:13<2:03:47,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 308/7600 [05:15<2:27:34,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 309/7600 [05:15<2:08:47,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 310/7600 [05:16<1:55:09,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 311/7600 [05:17<1:45:46,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 312/7600 [05:17<1:39:33,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 313/7600 [05:19<2:13:01,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 314/7600 [05:21<2:39:11,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 315/7600 [05:22<2:17:14,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 316/7600 [05:23<2:37:26,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 317/7600 [05:24<2:15:33,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 318/7600 [05:25<2:00:20,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 319/7600 [05:25<1:49:33,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 320/7600 [05:26<1:42:27,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 321/7600 [05:27<1:37:22,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 322/7600 [05:28<1:33:42,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 323/7600 [05:28<1:30:51,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 324/7600 [05:29<1:29:01,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 325/7600 [05:30<1:27:48,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 326/7600 [05:31<2:05:41,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 327/7600 [05:32<1:56:56,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 328/7600 [05:34<2:23:43,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 329/7600 [05:35<2:05:47,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 330/7600 [05:36<2:28:47,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 331/7600 [05:38<2:44:37,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 332/7600 [05:39<2:20:24,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 333/7600 [05:39<2:03:22,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 334/7600 [05:40<1:52:01,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 335/7600 [05:42<2:19:06,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 336/7600 [05:42<2:03:08,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 337/7600 [05:43<1:53:19,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 338/7600 [05:44<1:46:04,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 339/7600 [05:45<1:42:23,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 340/7600 [05:45<1:37:20,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 341/7600 [05:47<2:08:43,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   4%|▍         | 342/7600 [05:48<1:55:25,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 343/7600 [05:48<1:45:51,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 344/7600 [05:49<1:39:19,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 345/7600 [05:51<2:10:08,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 346/7600 [05:51<1:56:59,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 347/7600 [05:52<1:47:17,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 348/7600 [05:53<1:40:29,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 349/7600 [05:54<1:35:35,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 350/7600 [05:54<1:32:03,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 351/7600 [05:55<1:31:23,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 352/7600 [05:56<1:30:07,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 353/7600 [05:57<1:31:05,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 354/7600 [05:57<1:30:18,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 355/7600 [05:58<1:28:06,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 356/7600 [05:59<1:26:34,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 357/7600 [05:59<1:25:54,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 358/7600 [06:00<1:25:17,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 359/7600 [06:01<1:24:36,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 360/7600 [06:01<1:24:29,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 361/7600 [06:02<1:24:19,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 362/7600 [06:03<1:24:13,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 363/7600 [06:03<1:24:33,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 364/7600 [06:04<1:24:14,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 365/7600 [06:05<1:23:59,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 366/7600 [06:07<1:59:38,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 367/7600 [06:07<1:50:45,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 368/7600 [06:08<1:43:43,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 369/7600 [06:09<1:40:40,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 370/7600 [06:10<1:36:57,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 371/7600 [06:10<1:32:51,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 372/7600 [06:11<1:30:15,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 373/7600 [06:12<1:30:37,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 374/7600 [06:12<1:28:34,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 375/7600 [06:13<1:27:21,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 376/7600 [06:15<2:01:55,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 377/7600 [06:15<1:50:22,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 378/7600 [06:16<1:42:31,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▍         | 379/7600 [06:18<2:24:09,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 380/7600 [06:19<2:06:05,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 381/7600 [06:20<1:57:05,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 382/7600 [06:20<1:50:55,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 383/7600 [06:22<2:21:50,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 384/7600 [06:23<2:04:26,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 385/7600 [06:24<1:52:11,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 386/7600 [06:24<1:43:36,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 387/7600 [06:25<1:37:51,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 388/7600 [06:26<1:33:41,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 389/7600 [06:26<1:30:27,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 390/7600 [06:27<1:27:58,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 391/7600 [06:28<1:26:31,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 392/7600 [06:29<1:25:52,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 393/7600 [06:29<1:24:59,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 394/7600 [06:30<1:24:47,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 395/7600 [06:31<1:24:27,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 396/7600 [06:32<2:02:25,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 397/7600 [06:33<1:52:50,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 398/7600 [06:35<2:19:58,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 399/7600 [06:35<2:02:58,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 400/7600 [06:36<1:51:20,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 401/7600 [06:38<2:18:00,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 402/7600 [06:39<2:01:23,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 403/7600 [06:39<1:50:01,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 404/7600 [06:40<1:41:59,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 405/7600 [06:41<1:36:11,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 406/7600 [06:41<1:32:20,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 407/7600 [06:42<1:29:57,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 408/7600 [06:43<1:27:48,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 409/7600 [06:43<1:26:18,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 410/7600 [06:44<1:27:37,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 411/7600 [06:45<1:27:51,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 412/7600 [06:46<1:28:59,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 413/7600 [06:46<1:27:32,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 414/7600 [06:47<1:26:26,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 415/7600 [06:48<1:25:24,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 416/7600 [06:49<1:59:46,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   5%|▌         | 417/7600 [06:50<1:48:52,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 418/7600 [06:51<1:41:08,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 419/7600 [06:53<2:11:11,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 420/7600 [06:53<1:56:59,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 421/7600 [06:54<1:46:33,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 422/7600 [06:55<1:39:17,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 423/7600 [06:55<1:34:47,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 424/7600 [06:57<2:10:05,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 425/7600 [06:59<2:36:30,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 426/7600 [07:00<2:14:46,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 427/7600 [07:00<2:02:49,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 428/7600 [07:01<1:51:00,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 429/7600 [07:03<2:18:13,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 430/7600 [07:04<2:36:35,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 431/7600 [07:05<2:14:45,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 432/7600 [07:06<1:59:18,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 433/7600 [07:07<1:48:17,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 434/7600 [07:07<1:40:51,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 435/7600 [07:08<1:36:34,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 436/7600 [07:10<2:11:34,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 437/7600 [07:11<2:32:04,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 438/7600 [07:13<2:46:19,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 439/7600 [07:15<2:56:13,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 440/7600 [07:16<3:02:58,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 441/7600 [07:17<2:32:55,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 442/7600 [07:18<2:11:51,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 443/7600 [07:19<1:57:19,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 444/7600 [07:19<1:46:50,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 445/7600 [07:21<2:17:10,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 446/7600 [07:23<2:40:28,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 447/7600 [07:23<2:16:49,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 448/7600 [07:24<2:00:28,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 449/7600 [07:25<1:51:35,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 450/7600 [07:26<1:42:47,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 451/7600 [07:26<1:36:55,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 452/7600 [07:27<1:33:03,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 453/7600 [07:28<1:32:21,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 454/7600 [07:29<2:04:22,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 455/7600 [07:31<2:26:52,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 456/7600 [07:32<2:07:35,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 457/7600 [07:33<1:55:43,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 458/7600 [07:33<1:49:11,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 459/7600 [07:34<1:44:34,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 460/7600 [07:35<1:38:17,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 461/7600 [07:36<1:33:43,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 462/7600 [07:36<1:30:24,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 463/7600 [07:38<2:05:04,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 464/7600 [07:39<1:52:23,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 465/7600 [07:39<1:43:30,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 466/7600 [07:41<2:12:31,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 467/7600 [07:42<1:57:51,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 468/7600 [07:42<1:47:48,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 469/7600 [07:43<1:40:06,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 470/7600 [07:44<1:35:02,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 471/7600 [07:45<1:32:55,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 472/7600 [07:45<1:31:04,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 473/7600 [07:46<1:30:55,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▌         | 474/7600 [07:47<1:29:25,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 475/7600 [07:47<1:27:16,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 476/7600 [07:48<1:25:49,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 477/7600 [07:49<1:25:30,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 478/7600 [07:50<1:24:41,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 479/7600 [07:50<1:24:27,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 480/7600 [07:51<1:24:04,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 481/7600 [07:52<1:23:43,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 482/7600 [07:53<1:59:13,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 483/7600 [07:54<1:48:18,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 484/7600 [07:56<2:15:31,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 485/7600 [07:56<2:00:17,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 486/7600 [07:57<1:50:37,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 487/7600 [07:58<1:43:31,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 488/7600 [07:59<1:39:41,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 489/7600 [07:59<1:34:44,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 490/7600 [08:00<1:31:07,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 491/7600 [08:01<1:28:35,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 492/7600 [08:03<2:01:31,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 493/7600 [08:03<1:49:43,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   6%|▋         | 494/7600 [08:04<1:41:34,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 495/7600 [08:05<1:35:57,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 496/7600 [08:05<1:32:06,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 497/7600 [08:07<2:04:19,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 498/7600 [08:08<1:51:44,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 499/7600 [08:08<1:45:03,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 500/7600 [08:10<2:17:10,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 501/7600 [08:12<2:37:32,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 502/7600 [08:13<2:15:16,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 503/7600 [08:13<1:59:11,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 504/7600 [08:14<1:48:36,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 505/7600 [08:16<2:15:42,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 506/7600 [08:16<2:00:01,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 507/7600 [08:17<1:48:42,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 508/7600 [08:19<2:15:14,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 509/7600 [08:20<1:59:12,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 510/7600 [08:20<1:48:02,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 511/7600 [08:22<2:18:26,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 512/7600 [08:24<2:40:23,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 513/7600 [08:25<2:51:23,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 514/7600 [08:27<2:59:01,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 515/7600 [08:28<2:30:14,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 516/7600 [08:29<2:09:50,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 517/7600 [08:30<2:30:07,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 518/7600 [08:31<2:09:46,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 519/7600 [08:33<2:30:02,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 520/7600 [08:33<2:11:26,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 521/7600 [08:34<1:58:20,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 522/7600 [08:36<2:25:48,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 523/7600 [08:37<2:06:32,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 524/7600 [08:37<1:53:38,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 525/7600 [08:39<2:18:27,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 526/7600 [08:40<2:01:35,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 527/7600 [08:40<1:49:44,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 528/7600 [08:42<2:16:10,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 529/7600 [08:43<1:59:44,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 530/7600 [08:43<1:48:30,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 531/7600 [08:44<1:40:45,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 532/7600 [08:46<2:12:00,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 533/7600 [08:47<1:58:40,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 534/7600 [08:48<2:25:40,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 535/7600 [08:49<2:06:30,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 536/7600 [08:50<1:53:14,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 537/7600 [08:50<1:43:48,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 538/7600 [08:52<2:11:15,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 539/7600 [08:53<1:56:31,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 540/7600 [08:53<1:46:48,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 541/7600 [08:54<1:39:46,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 542/7600 [08:55<1:34:28,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 543/7600 [08:56<1:30:46,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 544/7600 [08:57<2:02:44,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 545/7600 [08:58<1:52:06,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 546/7600 [09:00<2:21:34,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 547/7600 [09:00<2:03:22,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 548/7600 [09:01<1:51:16,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 549/7600 [09:02<1:44:45,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 550/7600 [09:03<1:37:48,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 551/7600 [09:03<1:33:08,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 552/7600 [09:04<1:31:43,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 553/7600 [09:05<1:28:51,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 554/7600 [09:05<1:26:30,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 555/7600 [09:06<1:25:08,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 556/7600 [09:08<1:58:03,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 557/7600 [09:10<2:21:38,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 558/7600 [09:11<2:42:19,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 559/7600 [09:13<2:53:55,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 560/7600 [09:15<3:01:15,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 561/7600 [09:15<2:31:38,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 562/7600 [09:16<2:10:39,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 563/7600 [09:17<1:56:11,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 564/7600 [09:19<2:20:31,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 565/7600 [09:19<2:02:44,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 566/7600 [09:20<1:50:20,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 567/7600 [09:21<1:42:09,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 568/7600 [09:22<2:15:17,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   7%|▋         | 569/7600 [09:23<2:01:09,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 570/7600 [09:24<1:53:02,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 571/7600 [09:26<2:18:22,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 572/7600 [09:27<2:35:45,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 573/7600 [09:29<2:48:27,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 574/7600 [09:31<3:00:53,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 575/7600 [09:33<3:06:39,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 576/7600 [09:33<2:35:09,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 577/7600 [09:34<2:14:29,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 578/7600 [09:35<2:00:14,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 579/7600 [09:35<1:51:09,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 580/7600 [09:36<1:46:22,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 581/7600 [09:38<2:14:21,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 582/7600 [09:40<2:33:46,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 583/7600 [09:41<2:47:24,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 584/7600 [09:42<2:21:51,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 585/7600 [09:43<2:04:09,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 586/7600 [09:44<1:51:39,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 587/7600 [09:44<1:43:04,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 588/7600 [09:46<2:11:03,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 589/7600 [09:47<1:59:36,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 590/7600 [09:47<1:49:34,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 591/7600 [09:49<2:20:16,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 592/7600 [09:50<2:02:48,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 593/7600 [09:52<2:26:03,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 594/7600 [09:52<2:07:06,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 595/7600 [09:54<2:28:23,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 596/7600 [09:55<2:10:10,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 597/7600 [09:56<1:55:24,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 598/7600 [09:57<2:20:40,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 599/7600 [09:58<2:04:11,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 600/7600 [09:59<2:15:28,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 601/7600 [10:02<3:23:52,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 602/7600 [10:04<2:59:08,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 603/7600 [10:05<2:39:23,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 604/7600 [10:06<2:30:29,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 605/7600 [10:07<2:23:49,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 606/7600 [10:08<2:15:00,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 607/7600 [10:09<2:09:11,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 608/7600 [10:11<2:53:12,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 609/7600 [10:13<3:03:45,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 610/7600 [10:15<3:21:57,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 611/7600 [10:16<2:48:35,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 612/7600 [10:17<2:30:19,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 613/7600 [10:18<2:15:07,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 614/7600 [10:18<2:08:38,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 615/7600 [10:19<1:59:41,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 616/7600 [10:20<1:52:34,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 617/7600 [10:22<2:34:42,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 618/7600 [10:23<2:18:30,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 619/7600 [10:25<2:29:05,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 620/7600 [10:27<3:07:14,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 621/7600 [10:29<3:15:47,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 622/7600 [10:30<3:00:56,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 623/7600 [10:32<3:05:34,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 624/7600 [10:34<3:08:39,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 625/7600 [10:35<3:11:02,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 626/7600 [10:36<2:38:55,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 627/7600 [10:37<2:15:59,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 628/7600 [10:37<1:59:30,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 629/7600 [10:38<1:52:03,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 630/7600 [10:39<1:44:49,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 631/7600 [10:40<1:40:32,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 632/7600 [10:40<1:34:17,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 633/7600 [10:42<2:04:41,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 634/7600 [10:44<2:26:25,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 635/7600 [10:45<2:09:14,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 636/7600 [10:46<2:29:33,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 637/7600 [10:48<2:43:25,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 638/7600 [10:49<2:18:44,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 639/7600 [10:49<2:01:35,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 640/7600 [10:50<1:51:28,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 641/7600 [10:51<1:43:56,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 642/7600 [10:53<2:15:14,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 643/7600 [10:54<2:33:13,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 644/7600 [10:55<2:11:58,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 645/7600 [10:56<1:56:41,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   8%|▊         | 646/7600 [10:57<2:20:00,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 647/7600 [10:58<2:01:51,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 648/7600 [10:59<1:49:54,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 649/7600 [11:00<1:41:19,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 650/7600 [11:01<2:11:33,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 651/7600 [11:04<3:13:01,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 652/7600 [11:05<2:39:28,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 653/7600 [11:06<2:15:50,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 654/7600 [11:06<1:59:40,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 655/7600 [11:08<2:22:45,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 656/7600 [11:10<2:38:47,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 657/7600 [11:11<2:49:30,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 658/7600 [11:13<2:57:15,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 659/7600 [11:15<3:04:41,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 660/7600 [11:17<3:12:44,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 661/7600 [11:18<3:13:47,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 662/7600 [11:19<2:39:57,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 663/7600 [11:20<2:16:26,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▊         | 664/7600 [11:20<2:00:08,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 665/7600 [11:21<1:48:15,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 666/7600 [11:22<1:40:12,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 667/7600 [11:23<1:34:55,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 668/7600 [11:24<2:04:59,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 669/7600 [11:26<2:26:31,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 670/7600 [11:28<2:45:35,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 671/7600 [11:29<2:23:11,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 672/7600 [11:30<2:38:55,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 673/7600 [11:31<2:15:48,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 674/7600 [11:33<2:33:28,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 675/7600 [11:34<2:34:47,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 676/7600 [11:35<2:12:30,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 677/7600 [11:35<1:56:52,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 678/7600 [11:37<2:19:37,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 679/7600 [11:39<2:38:06,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 680/7600 [11:40<2:16:35,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 681/7600 [11:41<2:37:43,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 682/7600 [11:42<2:14:29,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 683/7600 [11:43<1:58:19,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 684/7600 [11:43<1:47:24,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 685/7600 [11:44<1:39:29,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 686/7600 [11:45<1:33:36,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 687/7600 [11:46<1:29:50,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 688/7600 [11:46<1:26:56,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 689/7600 [11:47<1:24:56,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 690/7600 [11:48<1:23:31,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 691/7600 [11:48<1:22:18,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 692/7600 [11:49<1:22:12,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 693/7600 [11:50<1:22:12,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 694/7600 [11:50<1:21:29,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 695/7600 [11:51<1:23:43,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 696/7600 [11:52<1:24:27,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 697/7600 [11:53<1:25:45,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 698/7600 [11:53<1:24:37,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 699/7600 [11:54<1:24:04,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 700/7600 [11:55<1:23:15,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 701/7600 [11:56<1:23:01,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 702/7600 [11:56<1:22:35,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 703/7600 [11:57<1:22:10,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 704/7600 [11:58<1:21:30,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 705/7600 [11:58<1:21:29,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 706/7600 [11:59<1:23:01,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 707/7600 [12:01<1:56:01,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 708/7600 [12:03<2:19:26,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 709/7600 [12:03<2:04:00,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 710/7600 [12:04<1:52:00,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 711/7600 [12:05<1:45:52,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 712/7600 [12:06<1:38:43,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 713/7600 [12:06<1:33:01,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 714/7600 [12:07<1:29:25,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 715/7600 [12:09<2:00:46,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 716/7600 [12:10<2:22:38,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 717/7600 [12:11<2:04:03,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 718/7600 [12:12<1:51:00,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 719/7600 [12:12<1:42:15,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 720/7600 [12:13<1:35:47,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:   9%|▉         | 721/7600 [12:14<1:31:25,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 722/7600 [12:15<1:28:15,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 723/7600 [12:16<2:03:09,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 724/7600 [12:17<1:53:05,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 725/7600 [12:18<1:43:18,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 726/7600 [12:19<1:36:14,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 727/7600 [12:20<2:04:59,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 728/7600 [12:21<1:51:34,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 729/7600 [12:22<1:42:13,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 730/7600 [12:22<1:35:36,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 731/7600 [12:23<1:31:03,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 732/7600 [12:25<2:01:30,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 733/7600 [12:26<2:23:09,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 734/7600 [12:27<2:04:19,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 735/7600 [12:28<1:53:11,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 736/7600 [12:29<1:43:55,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 737/7600 [12:30<2:14:02,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 738/7600 [12:31<1:57:57,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 739/7600 [12:32<1:46:55,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 740/7600 [12:34<2:12:44,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 741/7600 [12:34<1:57:26,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 742/7600 [12:36<2:20:42,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 743/7600 [12:37<2:02:36,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 744/7600 [12:38<2:24:31,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 745/7600 [12:39<2:05:43,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 746/7600 [12:40<1:53:20,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 747/7600 [12:41<1:47:54,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 748/7600 [12:41<1:42:40,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 749/7600 [12:43<2:11:26,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 750/7600 [12:44<1:56:01,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 751/7600 [12:45<1:47:15,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 752/7600 [12:45<1:39:13,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 753/7600 [12:46<1:33:23,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 754/7600 [12:47<1:29:24,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 755/7600 [12:47<1:26:52,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 756/7600 [12:49<1:58:54,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 757/7600 [12:50<1:47:24,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 758/7600 [12:51<1:39:13,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|▉         | 759/7600 [12:52<2:09:04,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 760/7600 [12:53<1:56:00,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 761/7600 [12:54<1:48:27,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 762/7600 [12:56<2:13:54,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 763/7600 [12:56<1:58:22,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 764/7600 [12:58<2:20:42,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 765/7600 [12:59<2:02:32,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 766/7600 [12:59<1:49:46,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 767/7600 [13:00<1:40:50,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 768/7600 [13:01<1:34:22,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 769/7600 [13:01<1:30:03,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 770/7600 [13:02<1:27:11,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 771/7600 [13:03<1:24:53,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 772/7600 [13:04<1:23:17,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 773/7600 [13:04<1:24:33,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 774/7600 [13:05<1:24:36,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 775/7600 [13:06<1:25:57,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 776/7600 [13:07<1:25:43,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 777/7600 [13:08<1:57:48,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 778/7600 [13:10<2:19:45,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 779/7600 [13:11<2:01:48,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 780/7600 [13:11<1:48:52,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 781/7600 [13:12<1:40:06,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 782/7600 [13:13<1:34:11,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 783/7600 [13:14<1:29:45,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 784/7600 [13:15<1:59:16,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 785/7600 [13:16<1:47:16,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 786/7600 [13:17<1:40:52,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 787/7600 [13:17<1:35:17,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 788/7600 [13:18<1:33:17,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 789/7600 [13:19<1:29:59,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 790/7600 [13:20<1:29:48,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 791/7600 [13:20<1:26:46,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 792/7600 [13:22<1:59:53,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 793/7600 [13:23<1:47:49,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 794/7600 [13:23<1:39:12,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 795/7600 [13:24<1:33:22,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 796/7600 [13:25<1:29:00,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 797/7600 [13:27<1:59:56,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  10%|█         | 798/7600 [13:27<1:48:11,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 799/7600 [13:28<1:39:45,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 800/7600 [13:29<1:34:40,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 801/7600 [13:29<1:31:38,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 802/7600 [13:30<1:30:29,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 803/7600 [13:31<1:28:51,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 804/7600 [13:32<1:26:06,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 805/7600 [13:32<1:24:44,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 806/7600 [13:34<1:56:58,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 807/7600 [13:36<2:19:20,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 808/7600 [13:37<2:01:28,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 809/7600 [13:38<2:22:09,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 810/7600 [13:39<2:03:15,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 811/7600 [13:40<1:51:41,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 812/7600 [13:41<2:17:01,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 813/7600 [13:43<2:38:10,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 814/7600 [13:44<2:14:32,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 815/7600 [13:45<1:57:56,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 816/7600 [13:46<2:19:56,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 817/7600 [13:48<2:35:30,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 818/7600 [13:49<2:12:30,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 819/7600 [13:49<1:58:23,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 820/7600 [13:50<1:46:32,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 821/7600 [13:51<1:38:04,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 822/7600 [13:52<1:32:23,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 823/7600 [13:52<1:28:34,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 824/7600 [13:53<1:26:02,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 825/7600 [13:54<1:25:35,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 826/7600 [13:54<1:25:35,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 827/7600 [13:55<1:25:38,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 828/7600 [13:56<1:23:29,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 829/7600 [13:58<1:56:10,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 830/7600 [13:58<1:45:16,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 831/7600 [13:59<1:37:43,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 832/7600 [14:00<1:32:22,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 833/7600 [14:00<1:28:33,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 834/7600 [14:02<1:59:11,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 835/7600 [14:03<1:47:26,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 836/7600 [14:04<1:38:42,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 837/7600 [14:04<1:32:59,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 838/7600 [14:05<1:28:47,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 839/7600 [14:06<1:27:24,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 840/7600 [14:06<1:26:12,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 841/7600 [14:07<1:27:20,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 842/7600 [14:09<1:57:45,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 843/7600 [14:10<1:46:16,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 844/7600 [14:10<1:37:54,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 845/7600 [14:11<1:34:09,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 846/7600 [14:13<2:02:40,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 847/7600 [14:13<1:49:39,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 848/7600 [14:15<2:13:24,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 849/7600 [14:16<1:56:57,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 850/7600 [14:17<1:45:28,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 851/7600 [14:18<2:13:28,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 852/7600 [14:19<1:59:31,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 853/7600 [14:21<2:21:51,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█         | 854/7600 [14:22<2:02:59,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 855/7600 [14:22<1:49:23,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 856/7600 [14:24<2:13:01,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 857/7600 [14:25<1:56:46,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 858/7600 [14:25<1:45:12,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 859/7600 [14:26<1:37:22,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 860/7600 [14:27<1:31:33,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 861/7600 [14:27<1:27:31,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 862/7600 [14:28<1:25:12,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 863/7600 [14:29<1:23:31,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 864/7600 [14:30<1:22:28,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 865/7600 [14:30<1:22:52,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 866/7600 [14:31<1:23:02,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 867/7600 [14:33<1:57:43,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 868/7600 [14:33<1:46:09,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 869/7600 [14:34<1:37:57,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 870/7600 [14:35<1:32:00,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 871/7600 [14:36<1:27:58,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 872/7600 [14:37<1:58:03,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  11%|█▏        | 873/7600 [14:38<1:45:59,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 874/7600 [14:39<1:39:57,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 875/7600 [14:39<1:34:01,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 876/7600 [14:40<1:29:16,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 877/7600 [14:42<2:00:11,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 878/7600 [14:44<2:26:18,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 879/7600 [14:44<2:06:57,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 880/7600 [14:45<1:52:15,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 881/7600 [14:47<2:15:21,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 882/7600 [14:49<2:31:15,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 883/7600 [14:49<2:09:29,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 884/7600 [14:50<1:54:01,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 885/7600 [14:52<2:16:10,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 886/7600 [14:52<1:58:47,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 887/7600 [14:53<1:47:04,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 888/7600 [14:55<2:13:15,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 889/7600 [14:55<1:57:50,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 890/7600 [14:56<1:48:23,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 891/7600 [14:57<1:39:08,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 892/7600 [14:58<1:32:42,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 893/7600 [14:59<2:01:47,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 894/7600 [15:01<2:22:01,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 895/7600 [15:02<2:03:09,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 896/7600 [15:02<1:50:09,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 897/7600 [15:03<1:40:40,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 898/7600 [15:04<1:33:45,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 899/7600 [15:05<1:29:14,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 900/7600 [15:05<1:25:36,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 901/7600 [15:06<1:23:22,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 902/7600 [15:08<1:58:39,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 903/7600 [15:09<1:48:38,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 904/7600 [15:09<1:39:33,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 905/7600 [15:11<2:06:10,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 906/7600 [15:12<1:51:32,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 907/7600 [15:12<1:43:04,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 908/7600 [15:13<1:35:58,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 909/7600 [15:14<1:30:39,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 910/7600 [15:14<1:27:03,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 911/7600 [15:16<1:57:16,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 912/7600 [15:17<1:45:48,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 913/7600 [15:19<2:11:32,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 914/7600 [15:19<1:57:20,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 915/7600 [15:20<1:48:09,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 916/7600 [15:21<1:41:09,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 917/7600 [15:22<1:34:16,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 918/7600 [15:22<1:29:39,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 919/7600 [15:23<1:26:17,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 920/7600 [15:24<1:24:05,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 921/7600 [15:24<1:22:06,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 922/7600 [15:26<1:53:39,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 923/7600 [15:27<1:43:16,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 924/7600 [15:28<1:37:44,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 925/7600 [15:28<1:32:26,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 926/7600 [15:29<1:27:48,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 927/7600 [15:30<1:25:03,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 928/7600 [15:31<1:57:38,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 929/7600 [15:33<2:23:16,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 930/7600 [15:34<2:04:10,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 931/7600 [15:35<1:50:11,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 932/7600 [15:35<1:41:06,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 933/7600 [15:36<1:34:36,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 934/7600 [15:38<2:02:19,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 935/7600 [15:38<1:49:00,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 936/7600 [15:39<1:39:46,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 937/7600 [15:40<1:35:04,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 938/7600 [15:42<2:02:47,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 939/7600 [15:43<2:24:37,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 940/7600 [15:44<2:06:37,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 941/7600 [15:46<2:28:33,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 942/7600 [15:48<2:40:44,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 943/7600 [15:48<2:15:51,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 944/7600 [15:49<1:58:13,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 945/7600 [15:51<2:18:24,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 946/7600 [15:52<2:33:02,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 947/7600 [15:54<2:43:01,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 948/7600 [15:55<2:17:29,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▏        | 949/7600 [15:56<2:01:39,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  12%|█▎        | 950/7600 [15:56<1:49:25,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 951/7600 [15:57<1:43:13,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 952/7600 [15:59<2:08:45,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 953/7600 [15:59<1:53:31,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 954/7600 [16:00<1:42:56,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 955/7600 [16:02<2:08:39,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 956/7600 [16:03<1:53:37,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 957/7600 [16:04<2:15:38,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 958/7600 [16:05<1:58:44,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 959/7600 [16:06<1:46:35,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 960/7600 [16:06<1:38:06,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 961/7600 [16:08<2:07:20,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 962/7600 [16:09<1:55:09,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 963/7600 [16:10<1:46:36,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 964/7600 [16:10<1:38:23,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 965/7600 [16:11<1:32:21,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 966/7600 [16:12<1:30:03,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 967/7600 [16:13<1:26:23,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 968/7600 [16:13<1:25:34,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 969/7600 [16:15<1:55:43,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 970/7600 [16:17<2:16:31,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 971/7600 [16:17<1:59:01,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 972/7600 [16:19<2:18:49,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 973/7600 [16:20<2:02:16,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 974/7600 [16:21<1:49:28,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 975/7600 [16:21<1:42:18,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 976/7600 [16:22<1:36:08,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 977/7600 [16:24<2:03:18,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 978/7600 [16:25<1:49:30,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 979/7600 [16:25<1:40:05,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 980/7600 [16:27<2:06:33,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 981/7600 [16:28<1:51:55,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 982/7600 [16:29<2:14:29,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 983/7600 [16:31<2:29:33,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 984/7600 [16:32<2:07:46,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 985/7600 [16:33<1:54:59,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 986/7600 [16:34<2:21:06,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 987/7600 [16:35<2:02:02,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 988/7600 [16:36<1:48:57,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 989/7600 [16:37<2:12:34,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 990/7600 [16:38<1:56:08,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 991/7600 [16:39<1:44:42,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 992/7600 [16:40<1:36:39,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 993/7600 [16:41<2:03:41,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 994/7600 [16:42<1:50:16,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 995/7600 [16:44<2:14:22,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 996/7600 [16:45<1:59:12,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 997/7600 [16:45<1:48:19,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 998/7600 [16:47<2:16:26,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 999/7600 [16:49<2:31:07,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1000/7600 [16:49<2:08:47,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1001/7600 [16:50<1:53:15,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1002/7600 [16:51<1:42:18,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1003/7600 [16:53<2:06:46,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1004/7600 [16:53<1:51:44,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1005/7600 [16:55<2:13:21,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1006/7600 [16:57<2:30:12,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1007/7600 [16:57<2:09:37,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1008/7600 [16:58<1:55:55,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1009/7600 [16:59<1:45:53,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1010/7600 [17:01<2:10:14,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1011/7600 [17:01<1:56:16,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1012/7600 [17:02<1:44:42,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1013/7600 [17:04<2:09:20,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1014/7600 [17:05<2:26:12,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1015/7600 [17:07<2:37:34,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1016/7600 [17:09<2:47:12,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1017/7600 [17:10<2:21:13,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1018/7600 [17:11<2:38:07,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1019/7600 [17:12<2:13:31,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1020/7600 [17:14<2:28:08,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1021/7600 [17:14<2:06:52,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1022/7600 [17:15<1:51:41,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1023/7600 [17:17<2:12:54,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1024/7600 [17:18<1:55:57,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  13%|█▎        | 1025/7600 [17:18<1:43:40,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1026/7600 [17:19<1:35:23,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1027/7600 [17:20<1:29:39,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1028/7600 [17:20<1:25:34,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1029/7600 [17:21<1:23:37,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1030/7600 [17:22<1:22:45,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1031/7600 [17:23<1:23:17,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1032/7600 [17:23<1:22:27,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1033/7600 [17:25<1:52:28,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1034/7600 [17:26<1:41:27,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1035/7600 [17:26<1:34:13,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1036/7600 [17:28<2:02:50,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1037/7600 [17:29<1:48:52,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1038/7600 [17:29<1:39:20,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1039/7600 [17:30<1:32:11,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1040/7600 [17:31<1:26:55,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1041/7600 [17:32<1:23:32,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1042/7600 [17:32<1:21:38,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1043/7600 [17:33<1:19:51,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▎        | 1044/7600 [17:34<1:20:13,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1045/7600 [17:34<1:21:40,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1046/7600 [17:36<1:53:29,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1047/7600 [17:38<2:13:59,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1048/7600 [17:39<1:56:42,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1049/7600 [17:39<1:44:54,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1050/7600 [17:41<2:07:52,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1051/7600 [17:42<1:52:13,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1052/7600 [17:43<2:12:30,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1053/7600 [17:44<1:55:42,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1054/7600 [17:45<1:43:46,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1055/7600 [17:45<1:36:35,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1056/7600 [17:46<1:32:00,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1057/7600 [17:47<1:29:51,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1058/7600 [17:49<1:58:37,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1059/7600 [17:49<1:45:42,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1060/7600 [17:51<2:08:11,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1061/7600 [17:52<1:52:08,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1062/7600 [17:52<1:41:06,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1063/7600 [17:53<1:33:15,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1064/7600 [17:54<1:27:42,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1065/7600 [17:54<1:24:16,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1066/7600 [17:56<1:52:50,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1067/7600 [17:57<1:41:42,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1068/7600 [17:59<2:08:54,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1069/7600 [17:59<1:55:09,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1070/7600 [18:00<1:44:09,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1071/7600 [18:02<2:07:24,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1072/7600 [18:02<1:53:50,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1073/7600 [18:04<2:13:53,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1074/7600 [18:05<1:56:40,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1075/7600 [18:05<1:44:06,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1076/7600 [18:06<1:35:47,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1077/7600 [18:08<2:02:16,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1078/7600 [18:09<1:48:51,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1079/7600 [18:10<2:13:22,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1080/7600 [18:11<2:00:00,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1081/7600 [18:12<1:49:02,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1082/7600 [18:13<1:39:15,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1083/7600 [18:14<1:42:50,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1084/7600 [18:14<1:34:52,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1085/7600 [18:15<1:29:26,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1086/7600 [18:17<1:57:06,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1087/7600 [18:18<2:17:06,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1088/7600 [18:19<1:58:35,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1089/7600 [18:21<2:16:54,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1090/7600 [18:23<2:34:55,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1091/7600 [18:23<2:12:51,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1092/7600 [18:24<1:57:53,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1093/7600 [18:26<2:16:50,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1094/7600 [18:27<1:58:34,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1095/7600 [18:28<2:17:40,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1096/7600 [18:29<1:58:49,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1097/7600 [18:30<1:45:22,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1098/7600 [18:30<1:37:15,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1099/7600 [18:32<2:02:26,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1100/7600 [18:33<1:48:17,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1101/7600 [18:34<2:11:15,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  14%|█▍        | 1102/7600 [18:35<1:55:35,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1103/7600 [18:36<1:46:31,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1104/7600 [18:38<2:09:33,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1105/7600 [18:39<2:25:09,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1106/7600 [18:40<2:04:38,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1107/7600 [18:41<1:52:06,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1108/7600 [18:42<2:12:59,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1109/7600 [18:44<2:27:13,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1110/7600 [18:45<2:05:26,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1111/7600 [18:45<1:50:22,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1112/7600 [18:46<1:39:59,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1113/7600 [18:48<2:08:47,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1114/7600 [18:49<1:53:28,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1115/7600 [18:49<1:41:43,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1116/7600 [18:50<1:34:00,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1117/7600 [18:51<1:28:14,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1118/7600 [18:51<1:24:18,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1119/7600 [18:52<1:21:34,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1120/7600 [18:53<1:19:29,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1121/7600 [18:55<1:49:27,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1122/7600 [18:55<1:39:02,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1123/7600 [18:57<2:05:43,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1124/7600 [18:59<2:33:46,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1125/7600 [19:01<2:48:26,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1126/7600 [19:03<2:51:57,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1127/7600 [19:03<2:22:48,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1128/7600 [19:05<2:33:33,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1129/7600 [19:06<2:09:39,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1130/7600 [19:06<1:53:18,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1131/7600 [19:07<1:41:31,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1132/7600 [19:08<1:33:29,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1133/7600 [19:09<1:59:06,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1134/7600 [19:10<1:45:43,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1135/7600 [19:11<1:37:26,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1136/7600 [19:11<1:31:37,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1137/7600 [19:12<1:28:14,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1138/7600 [19:13<1:26:11,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▍        | 1139/7600 [19:14<1:22:46,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1140/7600 [19:14<1:20:10,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1141/7600 [19:15<1:18:39,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1142/7600 [19:16<1:17:47,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1143/7600 [19:16<1:16:51,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1144/7600 [19:17<1:16:00,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1145/7600 [19:18<1:15:43,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1146/7600 [19:19<1:15:13,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1147/7600 [19:19<1:15:08,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1148/7600 [19:20<1:15:05,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1149/7600 [19:21<1:14:50,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1150/7600 [19:21<1:14:18,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1151/7600 [19:22<1:14:22,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1152/7600 [19:23<1:14:16,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1153/7600 [19:23<1:15:49,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1154/7600 [19:25<1:50:44,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1155/7600 [19:27<2:10:51,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1156/7600 [19:28<1:54:04,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1157/7600 [19:28<1:42:03,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1158/7600 [19:29<1:33:40,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1159/7600 [19:30<1:28:00,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1160/7600 [19:30<1:23:53,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1161/7600 [19:31<1:21:21,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1162/7600 [19:32<1:19:24,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1163/7600 [19:33<1:49:06,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1164/7600 [19:34<1:38:33,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1165/7600 [19:35<1:32:07,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1166/7600 [19:36<1:28:44,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1167/7600 [19:36<1:26:02,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1168/7600 [19:38<1:57:04,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1169/7600 [19:39<1:44:51,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1170/7600 [19:39<1:36:00,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1171/7600 [19:40<1:29:50,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1172/7600 [19:41<1:25:24,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1173/7600 [19:42<1:22:10,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1174/7600 [19:42<1:19:52,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1175/7600 [19:43<1:18:38,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1176/7600 [19:45<1:49:22,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  15%|█▌        | 1177/7600 [19:45<1:40:52,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1178/7600 [19:46<1:32:59,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1179/7600 [19:47<1:27:27,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1180/7600 [19:48<1:25:24,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1181/7600 [19:48<1:23:49,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1182/7600 [19:49<1:24:13,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1183/7600 [19:50<1:22:00,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1184/7600 [19:51<1:19:49,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1185/7600 [19:51<1:18:23,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1186/7600 [19:52<1:17:25,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1187/7600 [19:53<1:16:42,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1188/7600 [19:54<1:47:47,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1189/7600 [19:55<1:38:09,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1190/7600 [19:56<1:31:24,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1191/7600 [19:57<1:58:06,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1192/7600 [19:58<1:45:05,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1193/7600 [19:59<1:36:05,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1194/7600 [20:00<1:30:46,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1195/7600 [20:00<1:27:17,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1196/7600 [20:01<1:26:44,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1197/7600 [20:02<1:25:28,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1198/7600 [20:03<1:21:56,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1199/7600 [20:03<1:19:30,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1200/7600 [20:04<1:18:10,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1201/7600 [20:05<1:16:59,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1202/7600 [20:05<1:16:39,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1203/7600 [20:06<1:16:02,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1204/7600 [20:07<1:15:38,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1205/7600 [20:07<1:14:50,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1206/7600 [20:08<1:14:42,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1207/7600 [20:09<1:14:34,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1208/7600 [20:10<1:14:45,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1209/7600 [20:10<1:14:31,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1210/7600 [20:11<1:14:35,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1211/7600 [20:12<1:14:31,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1212/7600 [20:13<1:50:13,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1213/7600 [20:14<1:40:25,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1214/7600 [20:15<1:32:26,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1215/7600 [20:16<1:26:55,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1216/7600 [20:16<1:23:25,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1217/7600 [20:17<1:22:20,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1218/7600 [20:18<1:19:56,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1219/7600 [20:18<1:18:20,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1220/7600 [20:19<1:17:07,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1221/7600 [20:21<1:47:37,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1222/7600 [20:22<1:37:45,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1223/7600 [20:22<1:30:37,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1224/7600 [20:23<1:25:50,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1225/7600 [20:24<1:22:19,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1226/7600 [20:24<1:21:29,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1227/7600 [20:26<1:54:49,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1228/7600 [20:27<1:43:00,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1229/7600 [20:28<1:34:11,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1230/7600 [20:28<1:27:51,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1231/7600 [20:30<1:58:56,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1232/7600 [20:32<2:17:34,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1233/7600 [20:33<1:58:41,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▌        | 1234/7600 [20:33<1:45:39,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1235/7600 [20:35<2:07:28,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1236/7600 [20:36<1:51:11,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1237/7600 [20:37<2:14:11,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1238/7600 [20:39<2:30:09,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1239/7600 [20:40<2:07:16,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1240/7600 [20:41<1:51:19,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1241/7600 [20:41<1:40:08,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1242/7600 [20:42<1:32:20,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1243/7600 [20:43<1:26:59,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1244/7600 [20:43<1:23:26,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1245/7600 [20:44<1:20:58,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1246/7600 [20:45<1:18:59,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1247/7600 [20:46<1:49:00,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1248/7600 [20:47<1:38:20,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1249/7600 [20:48<1:31:15,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1250/7600 [20:49<1:27:25,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1251/7600 [20:49<1:24:02,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1252/7600 [20:50<1:23:50,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1253/7600 [20:51<1:22:05,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  16%|█▋        | 1254/7600 [20:52<1:19:24,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1255/7600 [20:52<1:17:54,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1256/7600 [20:53<1:16:46,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1257/7600 [20:54<1:15:43,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1258/7600 [20:55<1:46:26,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1259/7600 [20:56<1:36:56,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1260/7600 [20:57<1:29:49,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1261/7600 [20:57<1:25:25,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1262/7600 [20:58<1:22:27,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1263/7600 [21:00<1:51:01,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1264/7600 [21:01<1:40:08,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1265/7600 [21:01<1:34:19,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1266/7600 [21:03<2:05:18,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1267/7600 [21:04<1:50:04,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1268/7600 [21:05<1:39:16,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1269/7600 [21:06<2:02:46,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1270/7600 [21:07<1:47:57,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1271/7600 [21:08<1:37:38,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1272/7600 [21:08<1:30:18,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1273/7600 [21:09<1:25:35,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1274/7600 [21:10<1:21:58,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1275/7600 [21:10<1:19:21,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1276/7600 [21:11<1:17:47,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1277/7600 [21:12<1:16:18,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1278/7600 [21:14<1:48:01,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1279/7600 [21:14<1:39:20,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1280/7600 [21:15<1:34:02,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1281/7600 [21:16<1:27:56,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1282/7600 [21:17<1:23:40,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1283/7600 [21:17<1:20:47,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1284/7600 [21:18<1:18:49,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1285/7600 [21:19<1:17:11,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1286/7600 [21:20<1:47:19,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1287/7600 [21:21<1:37:10,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1288/7600 [21:22<1:29:51,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1289/7600 [21:22<1:25:25,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1290/7600 [21:25<2:12:57,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1291/7600 [21:25<1:56:37,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1292/7600 [21:27<2:19:12,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1293/7600 [21:29<2:30:06,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1294/7600 [21:30<2:06:59,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1295/7600 [21:31<2:22:06,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1296/7600 [21:32<2:01:33,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1297/7600 [21:33<1:47:08,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1298/7600 [21:33<1:37:20,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1299/7600 [21:34<1:31:45,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1300/7600 [21:35<1:26:40,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1301/7600 [21:36<1:22:46,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1302/7600 [21:37<1:53:37,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1303/7600 [21:38<1:43:16,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1304/7600 [21:39<1:36:38,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1305/7600 [21:40<1:30:43,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1306/7600 [21:40<1:25:37,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1307/7600 [21:41<1:21:59,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1308/7600 [21:43<1:49:49,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1309/7600 [21:43<1:38:49,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1310/7600 [21:44<1:31:13,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1311/7600 [21:46<1:56:03,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1312/7600 [21:46<1:43:18,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1313/7600 [21:48<2:04:50,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1314/7600 [21:49<1:49:14,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1315/7600 [21:50<1:39:25,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1316/7600 [21:53<2:59:02,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1317/7600 [21:54<2:27:10,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1318/7600 [21:54<2:04:43,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1319/7600 [21:55<1:49:10,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1320/7600 [21:56<1:38:23,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1321/7600 [21:57<1:30:29,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1322/7600 [21:57<1:25:00,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1323/7600 [21:58<1:21:21,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1324/7600 [22:00<1:49:33,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1325/7600 [22:00<1:38:16,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1326/7600 [22:02<2:02:45,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1327/7600 [22:03<1:49:08,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1328/7600 [22:04<1:40:36,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  17%|█▋        | 1329/7600 [22:04<1:33:55,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1330/7600 [22:05<1:27:41,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1331/7600 [22:06<1:23:07,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1332/7600 [22:07<1:50:58,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1333/7600 [22:09<2:10:03,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1334/7600 [22:11<2:23:56,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1335/7600 [22:11<2:02:56,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1336/7600 [22:12<1:47:46,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1337/7600 [22:13<1:37:20,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1338/7600 [22:14<1:30:18,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1339/7600 [22:14<1:26:36,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1340/7600 [22:15<1:23:18,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1341/7600 [22:17<1:54:17,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1342/7600 [22:18<2:12:13,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1343/7600 [22:19<1:54:06,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1344/7600 [22:20<1:41:46,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1345/7600 [22:21<1:32:57,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1346/7600 [22:21<1:26:50,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1347/7600 [22:23<1:53:02,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1348/7600 [22:24<1:40:39,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1349/7600 [22:24<1:32:29,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1350/7600 [22:26<1:56:53,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1351/7600 [22:27<1:44:39,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1352/7600 [22:27<1:36:32,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1353/7600 [22:28<1:32:15,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1354/7600 [22:29<1:26:24,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1355/7600 [22:31<1:52:32,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1356/7600 [22:31<1:40:37,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1357/7600 [22:32<1:32:11,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1358/7600 [22:33<1:26:26,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1359/7600 [22:33<1:22:37,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1360/7600 [22:34<1:19:40,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1361/7600 [22:36<1:49:05,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1362/7600 [22:38<2:08:43,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1363/7600 [22:39<2:24:49,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1364/7600 [22:41<2:37:14,  1.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1365/7600 [22:42<2:11:53,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1366/7600 [22:42<1:54:03,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1367/7600 [22:43<1:41:34,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1368/7600 [22:44<1:33:02,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1369/7600 [22:45<1:27:07,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1370/7600 [22:45<1:23:08,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1371/7600 [22:46<1:19:48,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1372/7600 [22:47<1:17:26,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1373/7600 [22:47<1:15:59,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1374/7600 [22:48<1:16:34,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1375/7600 [22:49<1:15:11,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1376/7600 [22:49<1:14:04,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1377/7600 [22:51<1:46:23,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1378/7600 [22:53<2:10:56,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1379/7600 [22:55<2:23:30,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1380/7600 [22:55<2:02:23,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1381/7600 [22:56<1:47:11,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1382/7600 [22:57<1:36:39,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1383/7600 [22:58<1:29:29,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1384/7600 [22:59<1:54:31,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1385/7600 [23:00<1:41:49,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1386/7600 [23:02<2:03:16,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1387/7600 [23:02<1:48:15,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1388/7600 [23:03<1:39:02,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1389/7600 [23:04<1:32:14,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1390/7600 [23:05<1:28:55,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1391/7600 [23:05<1:24:18,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1392/7600 [23:07<1:50:56,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1393/7600 [23:08<1:39:12,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1394/7600 [23:09<2:01:31,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1395/7600 [23:10<1:46:51,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1396/7600 [23:11<1:36:41,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1397/7600 [23:11<1:29:37,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1398/7600 [23:13<1:54:59,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1399/7600 [23:15<2:13:13,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1400/7600 [23:16<1:56:27,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1401/7600 [23:16<1:44:42,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1402/7600 [23:18<2:07:47,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1403/7600 [23:19<1:51:12,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1404/7600 [23:19<1:39:40,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1405/7600 [23:20<1:31:26,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  18%|█▊        | 1406/7600 [23:21<1:25:54,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1407/7600 [23:22<1:21:52,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1408/7600 [23:23<1:49:50,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1409/7600 [23:24<1:38:50,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1410/7600 [23:25<1:31:12,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1411/7600 [23:25<1:25:17,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1412/7600 [23:26<1:21:10,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1413/7600 [23:28<1:51:00,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1414/7600 [23:30<2:14:03,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1415/7600 [23:30<1:55:25,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1416/7600 [23:31<1:42:26,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1417/7600 [23:32<1:33:10,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1418/7600 [23:32<1:26:50,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1419/7600 [23:33<1:22:26,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1420/7600 [23:34<1:19:32,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1421/7600 [23:35<1:17:21,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1422/7600 [23:35<1:16:23,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1423/7600 [23:36<1:15:15,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▊        | 1424/7600 [23:37<1:14:17,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1425/7600 [23:37<1:13:51,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1426/7600 [23:39<1:43:22,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1427/7600 [23:41<2:08:03,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1428/7600 [23:43<2:22:37,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1429/7600 [23:44<2:32:23,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1430/7600 [23:46<2:38:19,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1431/7600 [23:47<2:12:32,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1432/7600 [23:47<1:54:19,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1433/7600 [23:48<1:41:45,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1434/7600 [23:49<1:32:39,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1435/7600 [23:50<1:56:21,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1436/7600 [23:51<1:43:04,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1437/7600 [23:53<2:08:09,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1438/7600 [23:55<2:23:46,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1439/7600 [23:55<2:04:21,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1440/7600 [23:57<2:18:42,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1441/7600 [23:58<1:58:26,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1442/7600 [23:59<1:44:08,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1443/7600 [23:59<1:34:06,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1444/7600 [24:01<1:57:08,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1445/7600 [24:03<2:13:07,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1446/7600 [24:03<1:54:35,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1447/7600 [24:05<2:14:33,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1448/7600 [24:06<1:59:27,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1449/7600 [24:07<1:44:49,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1450/7600 [24:07<1:34:27,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1451/7600 [24:08<1:27:32,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1452/7600 [24:10<1:52:20,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1453/7600 [24:11<2:09:32,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1454/7600 [24:12<1:51:50,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1455/7600 [24:13<1:39:47,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1456/7600 [24:13<1:31:19,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1457/7600 [24:14<1:25:38,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1458/7600 [24:15<1:21:28,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1459/7600 [24:17<1:50:56,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1460/7600 [24:17<1:40:53,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1461/7600 [24:19<2:04:15,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1462/7600 [24:20<1:48:27,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1463/7600 [24:21<2:06:48,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1464/7600 [24:22<1:50:00,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1465/7600 [24:24<2:08:10,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1466/7600 [24:24<1:50:53,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1467/7600 [24:25<1:39:06,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1468/7600 [24:26<1:30:48,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1469/7600 [24:27<1:25:06,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1470/7600 [24:27<1:21:04,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1471/7600 [24:28<1:18:01,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1472/7600 [24:30<1:49:52,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1473/7600 [24:30<1:39:37,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1474/7600 [24:31<1:30:58,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1475/7600 [24:33<1:54:34,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1476/7600 [24:34<1:41:30,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1477/7600 [24:34<1:32:16,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1478/7600 [24:35<1:26:09,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1479/7600 [24:36<1:21:56,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1480/7600 [24:37<1:48:49,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  19%|█▉        | 1481/7600 [24:38<1:37:43,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1482/7600 [24:39<1:29:41,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1483/7600 [24:41<2:05:55,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1484/7600 [24:41<1:50:21,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1485/7600 [24:42<1:41:26,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1486/7600 [24:44<2:03:24,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1487/7600 [24:46<2:17:37,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1488/7600 [24:46<1:57:40,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1489/7600 [24:47<1:43:44,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1490/7600 [24:48<1:33:40,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1491/7600 [24:48<1:27:00,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1492/7600 [24:49<1:22:16,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1493/7600 [24:50<1:18:46,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1494/7600 [24:51<1:16:24,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1495/7600 [24:52<1:44:34,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1496/7600 [24:53<1:35:46,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1497/7600 [24:54<1:29:23,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1498/7600 [24:55<1:56:37,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1499/7600 [24:57<2:14:18,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1500/7600 [24:59<2:25:14,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1501/7600 [25:00<2:02:57,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1502/7600 [25:00<1:47:30,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1503/7600 [25:01<1:36:38,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1504/7600 [25:03<1:58:46,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1505/7600 [25:03<1:44:21,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1506/7600 [25:04<1:34:29,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1507/7600 [25:05<1:28:40,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1508/7600 [25:07<1:58:48,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1509/7600 [25:07<1:44:54,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1510/7600 [25:08<1:34:42,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1511/7600 [25:09<1:27:30,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1512/7600 [25:10<1:52:08,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1513/7600 [25:12<2:09:29,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1514/7600 [25:13<1:51:49,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1515/7600 [25:14<1:39:43,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1516/7600 [25:14<1:31:14,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1517/7600 [25:16<1:55:16,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1518/7600 [25:17<1:42:12,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|█▉        | 1519/7600 [25:17<1:34:53,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1520/7600 [25:18<1:28:34,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1521/7600 [25:20<1:56:31,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1522/7600 [25:21<1:42:49,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1523/7600 [25:21<1:32:48,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1524/7600 [25:23<1:56:16,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1525/7600 [25:24<1:42:46,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1526/7600 [25:25<1:43:06,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1527/7600 [25:26<2:03:33,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1528/7600 [25:28<2:17:45,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1529/7600 [25:29<1:57:55,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1530/7600 [25:30<1:45:21,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1531/7600 [25:30<1:36:17,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1532/7600 [25:31<1:31:40,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1533/7600 [25:32<1:25:57,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1534/7600 [25:33<1:21:31,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1535/7600 [25:34<1:47:46,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1536/7600 [25:35<1:36:22,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1537/7600 [25:36<1:28:38,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1538/7600 [25:36<1:23:24,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1539/7600 [25:37<1:19:38,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1540/7600 [25:38<1:17:15,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1541/7600 [25:38<1:15:10,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1542/7600 [25:40<1:43:03,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1543/7600 [25:42<2:04:21,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1544/7600 [25:44<2:22:39,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1545/7600 [25:44<2:00:29,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1546/7600 [25:45<1:45:11,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1547/7600 [25:46<1:34:41,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1548/7600 [25:46<1:27:27,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1549/7600 [25:47<1:22:37,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1550/7600 [25:48<1:19:07,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1551/7600 [25:49<1:16:13,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1552/7600 [25:49<1:14:25,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1553/7600 [25:50<1:12:59,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1554/7600 [25:51<1:12:26,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1555/7600 [25:51<1:11:34,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1556/7600 [25:52<1:12:37,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1557/7600 [25:53<1:11:49,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  20%|██        | 1558/7600 [25:53<1:11:37,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1559/7600 [25:54<1:12:43,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1560/7600 [25:56<1:45:27,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1561/7600 [25:57<1:34:42,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1562/7600 [25:57<1:27:20,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1563/7600 [25:58<1:22:21,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1564/7600 [25:59<1:18:28,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1565/7600 [25:59<1:15:41,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1566/7600 [26:00<1:13:52,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1567/7600 [26:01<1:12:24,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1568/7600 [26:02<1:11:50,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1569/7600 [26:03<1:40:52,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1570/7600 [26:04<1:32:05,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1571/7600 [26:05<1:25:23,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1572/7600 [26:05<1:20:38,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1573/7600 [26:06<1:18:37,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1574/7600 [26:08<1:51:31,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1575/7600 [26:09<1:39:09,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1576/7600 [26:09<1:30:17,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1577/7600 [26:10<1:24:08,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1578/7600 [26:11<1:20:01,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1579/7600 [26:11<1:16:54,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1580/7600 [26:13<1:44:09,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1581/7600 [26:14<1:33:51,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1582/7600 [26:15<1:26:49,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1583/7600 [26:15<1:23:32,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1584/7600 [26:16<1:19:32,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1585/7600 [26:17<1:16:31,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1586/7600 [26:17<1:14:28,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1587/7600 [26:18<1:14:12,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1588/7600 [26:19<1:14:08,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1589/7600 [26:20<1:14:30,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1590/7600 [26:21<1:44:13,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1591/7600 [26:22<1:33:58,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1592/7600 [26:23<1:26:43,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1593/7600 [26:23<1:21:28,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1594/7600 [26:24<1:17:49,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1595/7600 [26:25<1:15:29,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1596/7600 [26:26<1:43:15,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1597/7600 [26:27<1:33:06,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1598/7600 [26:28<1:26:10,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1599/7600 [26:29<1:21:31,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1600/7600 [26:29<1:17:58,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1601/7600 [26:31<1:47:18,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1602/7600 [26:32<1:38:02,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1603/7600 [26:33<1:31:26,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1604/7600 [26:34<1:54:08,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1605/7600 [26:35<1:41:00,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1606/7600 [26:37<2:01:07,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1607/7600 [26:38<2:15:50,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1608/7600 [26:39<1:56:05,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1609/7600 [26:40<1:42:03,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1610/7600 [26:41<2:01:26,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1611/7600 [26:43<2:17:21,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1612/7600 [26:44<1:58:47,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1613/7600 [26:46<2:15:54,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██        | 1614/7600 [26:46<1:56:19,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1615/7600 [26:47<1:42:18,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1616/7600 [26:48<1:32:45,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1617/7600 [26:48<1:25:39,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1618/7600 [26:49<1:20:55,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1619/7600 [26:50<1:17:47,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1620/7600 [26:52<1:44:16,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1621/7600 [26:53<2:03:07,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1622/7600 [26:55<2:17:31,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1623/7600 [26:56<1:58:17,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1624/7600 [26:57<2:16:38,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1625/7600 [26:58<1:56:27,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1626/7600 [27:00<2:11:16,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1627/7600 [27:02<2:23:59,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1628/7600 [27:02<2:01:54,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1629/7600 [27:03<1:46:05,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1630/7600 [27:05<2:03:39,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1631/7600 [27:06<2:16:10,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1632/7600 [27:08<2:27:59,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  21%|██▏       | 1633/7600 [27:10<2:37:54,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1634/7600 [27:12<2:40:14,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1635/7600 [27:13<2:41:43,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1636/7600 [27:14<2:14:09,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1637/7600 [27:15<1:54:30,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1638/7600 [27:16<2:09:41,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1639/7600 [27:17<1:51:36,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1640/7600 [27:19<2:08:05,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1641/7600 [27:20<2:22:27,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1642/7600 [27:22<2:32:40,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1643/7600 [27:24<2:36:31,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1644/7600 [27:25<2:10:08,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1645/7600 [27:25<1:51:43,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1646/7600 [27:26<1:38:44,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1647/7600 [27:27<1:29:29,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1648/7600 [27:27<1:23:00,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1649/7600 [27:28<1:18:39,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1650/7600 [27:29<1:15:55,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1651/7600 [27:29<1:13:53,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1652/7600 [27:30<1:12:17,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1653/7600 [27:31<1:11:31,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1654/7600 [27:32<1:12:17,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1655/7600 [27:32<1:12:24,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1656/7600 [27:34<1:44:17,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1657/7600 [27:35<1:33:43,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1658/7600 [27:35<1:26:09,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1659/7600 [27:37<1:49:57,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1660/7600 [27:38<1:37:31,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1661/7600 [27:40<1:57:59,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1662/7600 [27:40<1:43:23,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1663/7600 [27:41<1:33:11,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1664/7600 [27:42<1:25:53,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1665/7600 [27:42<1:21:02,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1666/7600 [27:43<1:17:34,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1667/7600 [27:44<1:15:54,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1668/7600 [27:46<1:47:15,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1669/7600 [27:46<1:35:55,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1670/7600 [27:48<1:56:38,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1671/7600 [27:49<1:42:25,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1672/7600 [27:49<1:32:47,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1673/7600 [27:50<1:25:29,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1674/7600 [27:51<1:20:59,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1675/7600 [27:51<1:17:31,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1676/7600 [27:52<1:14:40,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1677/7600 [27:54<1:41:30,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1678/7600 [27:55<1:31:39,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1679/7600 [27:55<1:24:38,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1680/7600 [27:56<1:20:48,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1681/7600 [27:57<1:18:53,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1682/7600 [27:57<1:17:28,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1683/7600 [27:58<1:16:31,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1684/7600 [28:00<1:43:15,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1685/7600 [28:02<2:01:40,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1686/7600 [28:02<1:45:42,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1687/7600 [28:04<2:03:14,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1688/7600 [28:05<1:46:39,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1689/7600 [28:06<2:03:41,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1690/7600 [28:07<1:47:09,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1691/7600 [28:08<1:35:26,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1692/7600 [28:08<1:28:51,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1693/7600 [28:09<1:26:12,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1694/7600 [28:10<1:23:19,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1695/7600 [28:11<1:21:53,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1696/7600 [28:12<1:46:42,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1697/7600 [28:13<1:35:12,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1698/7600 [28:14<1:27:08,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1699/7600 [28:15<1:21:29,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1700/7600 [28:15<1:17:40,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1701/7600 [28:17<1:43:25,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1702/7600 [28:18<1:32:41,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1703/7600 [28:19<1:54:04,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1704/7600 [28:20<1:40:29,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1705/7600 [28:22<2:03:25,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1706/7600 [28:23<1:48:50,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1707/7600 [28:23<1:36:43,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1708/7600 [28:24<1:28:06,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▏       | 1709/7600 [28:25<1:22:07,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  22%|██▎       | 1710/7600 [28:26<1:46:52,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1711/7600 [28:28<2:04:01,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1712/7600 [28:29<1:47:11,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1713/7600 [28:29<1:35:27,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1714/7600 [28:30<1:27:31,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1715/7600 [28:31<1:21:44,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1716/7600 [28:31<1:17:47,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1717/7600 [28:32<1:14:59,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1718/7600 [28:33<1:14:26,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1719/7600 [28:34<1:13:51,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1720/7600 [28:34<1:14:31,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1721/7600 [28:35<1:13:23,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1722/7600 [28:36<1:11:28,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1723/7600 [28:37<1:39:11,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1724/7600 [28:39<1:57:57,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1725/7600 [28:40<1:43:19,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1726/7600 [28:41<1:32:45,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1727/7600 [28:41<1:25:15,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1728/7600 [28:42<1:20:12,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1729/7600 [28:44<1:45:31,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1730/7600 [28:44<1:34:24,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1731/7600 [28:45<1:27:33,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1732/7600 [28:46<1:23:13,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1733/7600 [28:47<1:20:18,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1734/7600 [28:47<1:17:57,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1735/7600 [28:48<1:15:06,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1736/7600 [28:50<1:51:21,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1737/7600 [28:51<1:38:18,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1738/7600 [28:51<1:29:12,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1739/7600 [28:52<1:22:53,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1740/7600 [28:53<1:18:18,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1741/7600 [28:53<1:15:14,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1742/7600 [28:54<1:13:23,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1743/7600 [28:56<1:40:32,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1744/7600 [28:57<1:30:58,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1745/7600 [28:57<1:25:26,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1746/7600 [28:59<1:53:47,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1747/7600 [29:00<1:40:49,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1748/7600 [29:02<1:59:55,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1749/7600 [29:02<1:44:30,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1750/7600 [29:03<1:33:39,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1751/7600 [29:04<1:28:20,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1752/7600 [29:04<1:22:17,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1753/7600 [29:05<1:18:03,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1754/7600 [29:06<1:15:18,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1755/7600 [29:08<1:45:02,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1756/7600 [29:09<2:03:34,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1757/7600 [29:11<2:19:50,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1758/7600 [29:12<1:59:25,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1759/7600 [29:14<2:21:54,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1760/7600 [29:15<1:59:59,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1761/7600 [29:15<1:44:26,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1762/7600 [29:16<1:33:12,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1763/7600 [29:18<1:53:59,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1764/7600 [29:18<1:40:26,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1765/7600 [29:19<1:30:35,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1766/7600 [29:20<1:23:47,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1767/7600 [29:20<1:18:53,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1768/7600 [29:21<1:15:33,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1769/7600 [29:22<1:14:42,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1770/7600 [29:23<1:13:44,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1771/7600 [29:24<1:44:06,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1772/7600 [29:25<1:33:14,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1773/7600 [29:26<1:25:43,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1774/7600 [29:27<1:20:24,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1775/7600 [29:28<1:44:56,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1776/7600 [29:29<1:43:23,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1777/7600 [29:30<1:32:52,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1778/7600 [29:31<1:25:17,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1779/7600 [29:31<1:21:38,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1780/7600 [29:32<1:17:45,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1781/7600 [29:33<1:14:56,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1782/7600 [29:34<1:12:27,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1783/7600 [29:34<1:12:47,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1784/7600 [29:35<1:12:45,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  23%|██▎       | 1785/7600 [29:36<1:13:43,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1786/7600 [29:37<1:12:22,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1787/7600 [29:37<1:10:58,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1788/7600 [29:38<1:09:48,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1789/7600 [29:39<1:08:49,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1790/7600 [29:39<1:08:31,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1791/7600 [29:40<1:08:36,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1792/7600 [29:41<1:08:10,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1793/7600 [29:42<1:36:10,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1794/7600 [29:43<1:27:22,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1795/7600 [29:44<1:21:29,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1796/7600 [29:45<1:45:53,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1797/7600 [29:46<1:35:32,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1798/7600 [29:48<1:59:41,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1799/7600 [29:49<1:44:21,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1800/7600 [29:49<1:33:05,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1801/7600 [29:50<1:25:08,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1802/7600 [29:51<1:19:52,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1803/7600 [29:52<1:15:51,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▎       | 1804/7600 [29:52<1:13:08,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1805/7600 [29:53<1:11:24,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1806/7600 [29:54<1:09:59,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1807/7600 [29:54<1:08:54,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1808/7600 [29:55<1:08:44,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1809/7600 [29:56<1:08:10,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1810/7600 [29:56<1:08:01,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1811/7600 [29:57<1:07:52,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1812/7600 [29:58<1:07:25,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1813/7600 [29:59<1:08:38,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1814/7600 [29:59<1:10:01,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1815/7600 [30:01<1:41:08,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1816/7600 [30:02<1:30:58,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1817/7600 [30:02<1:24:01,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1818/7600 [30:03<1:19:04,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1819/7600 [30:04<1:15:20,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1820/7600 [30:05<1:12:43,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1821/7600 [30:06<1:39:27,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1822/7600 [30:07<1:29:51,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1823/7600 [30:08<1:22:56,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1824/7600 [30:09<1:46:23,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1825/7600 [30:10<1:34:37,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1826/7600 [30:12<1:59:25,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1827/7600 [30:13<1:45:43,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1828/7600 [30:13<1:33:51,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1829/7600 [30:14<1:25:56,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1830/7600 [30:15<1:20:17,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1831/7600 [30:15<1:16:07,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1832/7600 [30:16<1:15:18,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1833/7600 [30:17<1:12:47,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1834/7600 [30:19<1:38:31,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1835/7600 [30:19<1:29:04,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1836/7600 [30:20<1:22:28,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1837/7600 [30:22<1:47:20,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1838/7600 [30:22<1:35:24,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1839/7600 [30:23<1:29:02,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1840/7600 [30:24<1:23:00,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1841/7600 [30:25<1:20:27,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1842/7600 [30:25<1:16:57,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1843/7600 [30:26<1:13:46,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1844/7600 [30:27<1:11:37,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1845/7600 [30:27<1:09:58,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1846/7600 [30:28<1:08:52,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1847/7600 [30:29<1:08:19,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1848/7600 [30:29<1:07:50,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1849/7600 [30:30<1:07:33,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1850/7600 [30:31<1:07:08,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1851/7600 [30:32<1:07:11,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1852/7600 [30:32<1:07:07,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1853/7600 [30:33<1:06:39,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1854/7600 [30:34<1:06:44,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1855/7600 [30:35<1:36:11,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1856/7600 [30:36<1:28:14,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1857/7600 [30:38<1:51:57,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1858/7600 [30:40<2:06:06,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1859/7600 [30:40<1:48:17,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1860/7600 [30:41<1:36:26,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1861/7600 [30:43<1:55:27,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  24%|██▍       | 1862/7600 [30:45<2:18:11,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1863/7600 [30:45<1:56:50,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1864/7600 [30:47<2:10:14,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1865/7600 [30:49<2:22:25,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1866/7600 [30:50<2:00:36,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1867/7600 [30:50<1:44:14,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1868/7600 [30:51<1:32:58,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1869/7600 [30:53<1:54:41,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1870/7600 [30:53<1:40:15,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1871/7600 [30:54<1:29:49,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1872/7600 [30:55<1:22:57,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1873/7600 [30:56<1:46:06,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1874/7600 [30:57<1:34:09,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1875/7600 [30:59<1:53:35,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1876/7600 [31:00<1:40:43,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1877/7600 [31:00<1:31:11,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1878/7600 [31:01<1:26:05,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1879/7600 [31:03<1:50:31,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1880/7600 [31:04<2:05:07,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1881/7600 [31:05<1:47:30,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1882/7600 [31:06<1:35:05,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1883/7600 [31:07<1:26:13,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1884/7600 [31:07<1:20:28,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1885/7600 [31:08<1:16:08,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1886/7600 [31:10<1:40:52,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1887/7600 [31:11<1:58:13,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1888/7600 [31:12<1:44:46,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1889/7600 [31:14<2:05:30,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1890/7600 [31:15<1:47:38,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1891/7600 [31:15<1:35:11,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1892/7600 [31:16<1:26:44,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1893/7600 [31:17<1:20:37,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1894/7600 [31:17<1:16:37,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1895/7600 [31:19<1:41:33,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1896/7600 [31:20<1:31:02,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1897/7600 [31:20<1:23:24,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1898/7600 [31:21<1:18:06,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▍       | 1899/7600 [31:22<1:14:35,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1900/7600 [31:23<1:39:42,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1901/7600 [31:24<1:31:16,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1902/7600 [31:26<1:55:18,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1903/7600 [31:28<2:08:15,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1904/7600 [31:29<2:17:41,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1905/7600 [31:31<2:24:28,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1906/7600 [31:33<2:28:57,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1907/7600 [31:33<2:03:56,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1908/7600 [31:34<1:46:21,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1909/7600 [31:35<1:34:29,  1.00it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1910/7600 [31:37<1:56:11,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1911/7600 [31:37<1:42:15,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1912/7600 [31:39<2:01:20,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1913/7600 [31:41<2:14:16,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1914/7600 [31:42<1:53:54,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1915/7600 [31:43<2:07:50,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1916/7600 [31:44<1:49:10,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1917/7600 [31:46<2:03:47,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1918/7600 [31:46<1:46:15,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1919/7600 [31:47<1:33:53,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1920/7600 [31:48<1:25:22,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1921/7600 [31:48<1:20:50,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1922/7600 [31:49<1:17:20,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1923/7600 [31:51<1:44:32,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1924/7600 [31:53<2:02:48,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1925/7600 [31:54<2:13:19,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1926/7600 [31:55<1:53:03,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1927/7600 [31:56<1:38:48,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1928/7600 [31:56<1:28:49,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1929/7600 [31:58<1:49:30,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1930/7600 [31:59<1:36:22,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1931/7600 [31:59<1:27:14,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1932/7600 [32:00<1:20:26,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1933/7600 [32:01<1:17:16,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1934/7600 [32:02<1:15:03,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1935/7600 [32:02<1:16:29,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1936/7600 [32:04<1:40:51,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  25%|██▌       | 1937/7600 [32:05<1:30:19,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1938/7600 [32:07<1:50:29,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1939/7600 [32:07<1:36:45,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1940/7600 [32:08<1:27:32,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1941/7600 [32:10<1:49:06,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1942/7600 [32:11<2:03:01,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1943/7600 [32:12<1:45:51,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1944/7600 [32:13<1:34:49,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1945/7600 [32:14<1:57:40,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1946/7600 [32:15<1:42:49,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1947/7600 [32:16<1:31:27,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1948/7600 [32:17<1:23:39,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1949/7600 [32:17<1:18:05,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1950/7600 [32:18<1:14:03,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1951/7600 [32:19<1:11:12,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1952/7600 [32:20<1:37:10,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1953/7600 [32:21<1:27:40,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1954/7600 [32:23<1:48:11,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1955/7600 [32:24<2:02:36,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1956/7600 [32:26<2:15:50,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1957/7600 [32:27<1:56:49,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1958/7600 [32:28<1:41:29,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1959/7600 [32:28<1:33:23,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1960/7600 [32:29<1:25:00,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1961/7600 [32:30<1:19:12,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1962/7600 [32:31<1:41:57,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1963/7600 [32:32<1:30:56,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1964/7600 [32:33<1:23:12,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1965/7600 [32:34<1:44:42,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1966/7600 [32:35<1:33:02,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1967/7600 [32:36<1:25:00,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1968/7600 [32:37<1:19:01,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1969/7600 [32:38<1:45:04,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1970/7600 [32:39<1:35:02,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1971/7600 [32:40<1:25:47,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1972/7600 [32:40<1:19:38,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1973/7600 [32:41<1:15:12,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1974/7600 [32:43<1:39:31,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1975/7600 [32:44<1:56:18,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1976/7600 [32:45<1:40:58,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1977/7600 [32:46<1:29:48,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1978/7600 [32:48<1:49:40,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1979/7600 [32:49<2:04:08,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1980/7600 [32:51<2:18:09,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1981/7600 [32:52<1:56:17,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1982/7600 [32:52<1:40:58,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1983/7600 [32:53<1:29:54,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1984/7600 [32:55<1:49:23,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1985/7600 [32:57<2:05:11,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1986/7600 [32:57<1:47:16,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1987/7600 [32:59<2:01:38,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1988/7600 [33:01<2:11:35,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1989/7600 [33:02<2:21:54,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1990/7600 [33:03<2:01:27,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1991/7600 [33:05<2:11:52,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1992/7600 [33:05<1:51:42,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1993/7600 [33:06<1:37:43,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▌       | 1994/7600 [33:07<1:27:32,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 1995/7600 [33:08<1:20:37,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 1996/7600 [33:08<1:15:57,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 1997/7600 [33:10<1:39:27,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 1998/7600 [33:11<1:28:58,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 1999/7600 [33:11<1:21:39,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2000/7600 [33:13<1:44:02,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2001/7600 [33:14<1:33:52,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2002/7600 [33:14<1:26:00,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2003/7600 [33:15<1:21:51,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2004/7600 [33:16<1:16:57,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2005/7600 [33:17<1:13:13,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2006/7600 [33:17<1:10:32,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2007/7600 [33:19<1:35:51,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2008/7600 [33:20<1:26:20,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2009/7600 [33:20<1:19:56,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2010/7600 [33:22<1:42:05,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2011/7600 [33:23<1:30:48,  1.03it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2012/7600 [33:23<1:22:46,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2013/7600 [33:24<1:17:19,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  26%|██▋       | 2014/7600 [33:25<1:13:28,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2015/7600 [33:25<1:11:10,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2016/7600 [33:26<1:12:07,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2017/7600 [33:27<1:11:07,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2018/7600 [33:28<1:19:37,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2019/7600 [33:29<1:15:05,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2020/7600 [33:29<1:11:51,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2021/7600 [33:30<1:09:38,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2022/7600 [33:31<1:08:12,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2023/7600 [33:32<1:07:03,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2024/7600 [33:32<1:06:11,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2025/7600 [33:33<1:05:34,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2026/7600 [33:35<1:32:00,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2027/7600 [33:35<1:23:39,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2028/7600 [33:36<1:17:52,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2029/7600 [33:37<1:13:34,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2030/7600 [33:37<1:10:38,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2031/7600 [33:38<1:09:54,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2032/7600 [33:39<1:09:18,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2033/7600 [33:40<1:09:48,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2034/7600 [33:40<1:08:27,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2035/7600 [33:42<1:34:15,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2036/7600 [33:44<1:52:17,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2037/7600 [33:45<2:04:33,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2038/7600 [33:46<1:46:48,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2039/7600 [33:47<1:33:48,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2040/7600 [33:47<1:25:03,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2041/7600 [33:48<1:18:32,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2042/7600 [33:50<1:41:06,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2043/7600 [33:50<1:31:16,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2044/7600 [33:52<1:53:44,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2045/7600 [33:53<1:38:49,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2046/7600 [33:54<1:28:32,  1.05it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2047/7600 [33:54<1:21:13,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2048/7600 [33:56<1:43:11,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2049/7600 [33:57<1:31:37,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2050/7600 [33:57<1:23:12,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2051/7600 [33:58<1:17:13,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2052/7600 [33:59<1:13:14,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2053/7600 [33:59<1:10:29,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2054/7600 [34:00<1:08:25,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2055/7600 [34:01<1:07:27,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2056/7600 [34:02<1:06:28,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2057/7600 [34:02<1:07:12,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2058/7600 [34:03<1:07:10,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2059/7600 [34:04<1:07:53,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2060/7600 [34:04<1:07:43,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2061/7600 [34:05<1:06:43,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2062/7600 [34:07<1:32:41,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2063/7600 [34:08<1:50:58,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2064/7600 [34:09<1:37:03,  1.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2065/7600 [34:10<1:27:14,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2066/7600 [34:12<1:46:49,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2067/7600 [34:12<1:34:30,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2068/7600 [34:13<1:25:27,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2069/7600 [34:14<1:19:22,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2070/7600 [34:14<1:15:47,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2071/7600 [34:16<1:45:52,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2072/7600 [34:17<1:34:20,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2073/7600 [34:18<1:25:12,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2074/7600 [34:18<1:18:48,  1.17it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2075/7600 [34:19<1:14:04,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2076/7600 [34:20<1:11:07,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2077/7600 [34:21<1:10:18,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2078/7600 [34:21<1:08:14,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2079/7600 [34:22<1:07:02,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Baseline inference:  27%|██▋       | 2079/7600 [34:22<1:31:17,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3467411980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mfull_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mhidden_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Perform inference on test set (full dataset)\n",
        "from tqdm import tqdm\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# First, let's check the structure of test_data\n",
        "print(\"Sample example:\", test_data[0])\n",
        "\n",
        "for i in tqdm(range(len(test_data)), desc=\"Baseline inference\"):\n",
        "    example = test_data[i]\n",
        "\n",
        "    # Access the text and label from the example\n",
        "    # dataset items may be dict-like\n",
        "    if isinstance(example, dict):\n",
        "        text = example.get(\"text\") or example.get(\"content\")\n",
        "        label = example.get(\"label\")\n",
        "    else:\n",
        "        # fallback if dataset returns tuples\n",
        "        try:\n",
        "            text, label = example\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if text is None:\n",
        "        continue\n",
        "\n",
        "    prompt = create_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
        "\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the generated part (after the prompt)\n",
        "    prediction_text = full_output[len(prompt):].strip()\n",
        "\n",
        "    # Try to match prediction to a label\n",
        "    matched_label = \"Unknown\"\n",
        "    for lbl in label_map.values():\n",
        "        if lbl.lower() in prediction_text.lower():\n",
        "            matched_label = lbl\n",
        "            break\n",
        "\n",
        "    predictions.append(matched_label)\n",
        "    true_labels.append(label_map[label])\n",
        "\n",
        "print(f\"Baseline inference completed on {len(predictions)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"predictions_checkpoint.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "predictions = data[\"predictions\"]\n",
        "true_labels = data[\"true_labels\"]\n",
        "\n",
        "print(\"Recovered samples:\", len(predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "t_1HSa0Iit0t",
        "outputId": "9f37f2ff-2c82-4861-9368-6719d7400af2"
      },
      "id": "t_1HSa0Iit0t",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'predictions_checkpoint.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3806482685.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictions_checkpoint.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'predictions_checkpoint.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f70a3dcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f70a3dcd",
        "outputId": "7576b52b-8bef-422f-ebcb-ce3bff8eecb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.7787\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Business       0.66      0.83      0.74       460\n",
            "    Sci/Tech       0.66      0.68      0.67       522\n",
            "      Sports       0.96      0.87      0.91       554\n",
            "     Unknown       0.00      0.00      0.00         0\n",
            "       World       0.89      0.74      0.81       543\n",
            "\n",
            "    accuracy                           0.78      2079\n",
            "   macro avg       0.64      0.62      0.63      2079\n",
            "weighted avg       0.80      0.78      0.79      2079\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Compute accuracy and evaluate baseline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Baseline Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3ee317cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "3ee317cb",
        "outputId": "0d2e84dc-dde2-41ca-f521-8c1d79ac5f0a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix:\n",
            "[[382  57   0  20]\n",
            " [155 353   1   9]\n",
            " [ 10  40 481  21]\n",
            " [ 28  83  19 403]]\n",
            "\n",
            "Labels order: ['Business', 'Sci/Tech', 'Sports', 'World']\n"
          ]
        }
      ],
      "source": [
        "# Generate confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get unique labels\n",
        "labels_list = sorted(label_map.values())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions, labels=labels_list)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels_list,\n",
        "            yticklabels=labels_list,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - LLM Baseline Predictions')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nLabels order:\", labels_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb004e4c",
      "metadata": {
        "id": "eb004e4c"
      },
      "source": [
        "# Fine-Tuning with LoRA (Low-Rank Adaptation)\n",
        "\n",
        "## Overview\n",
        "LoRA is a parameter-efficient fine-tuning method that reduces the number of trainable parameters by adding low-rank decomposition matrices to the model. Instead of updating all model weights, we only train small adapter modules.\n",
        "\n",
        "## Key Concepts\n",
        "- **LoRA**: Low-Rank Adaptation - adds trainable low-rank matrices to pre-trained weights\n",
        "- **PEFT Library**: Parameter-Efficient Fine-Tuning library from Hugging Face\n",
        "- **Adapter Injection**: Injects LoRA modules into the model's linear layers\n",
        "- **Memory Efficient**: Reduces memory usage and training time significantly\n",
        "\n",
        "## Steps to Implement\n",
        "1. Install PEFT library\n",
        "2. Prepare the training dataset with proper formatting\n",
        "3. Create and inject LoRA adapters\n",
        "4. Configure training parameters\n",
        "5. Train the model on labeled data\n",
        "6. Evaluate on test set\n",
        "7. Compare accuracy with baseline\n",
        "\n",
        "## Expected Outcomes\n",
        "- LoRA-finetuned model should outperform the baseline prompt-based approach\n",
        "- Significant reduction in trainable parameters compared to full fine-tuning\n",
        "- Faster training and inference compared to full model updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6213b22b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6213b22b",
        "outputId": "5010a955-5872-4c36-b5ec-c2d9d0f95edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFT library installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install PEFT library for LoRA\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"peft\", \"bitsandbytes\"])\n",
        "print(\"PEFT library installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8a49292b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "2a1cc398da914be3a940d4fbda6f3cb1",
            "aa0c7f9ae84c4a6197d73cc29708d0c4",
            "b9c98e24e7214e599c953bf5b3a71426",
            "da7023602d2642d6bfa023167e8b3737",
            "d5db24f2024f40fc8a74935de0c1782b",
            "ce133797dc47496f84183ed05f741a8f",
            "8163037a07484bad8e96cd603f69b91a",
            "8c5863b497cc4743b7ab39294fa132ea",
            "c130df0cfe90446db7970bec7807354a",
            "60ef45240e0149748164fc6eef16687d",
            "c67488471ccb47b5adf2bbda6ed13192"
          ]
        },
        "id": "8a49292b",
        "outputId": "62b2d4b4-1d1c-44f4-a32c-5830099ca643"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a1cc398da914be3a940d4fbda6f3cb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset prepared: 120000 examples\n",
            "Sample formatted text:\n",
            "Classify the following news headline into one of the categories: World, Sports, Business, Sci/Tech.\n",
            "\n",
            "Text: \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\"\n",
            "Label: Business\n"
          ]
        }
      ],
      "source": [
        "# Prepare training dataset with proper formatting\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load training data\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# Create formatted dataset for training\n",
        "def format_training_data(examples):\n",
        "    \"\"\"Format examples into conversation format\"\"\"\n",
        "    formatted_texts = []\n",
        "    for i in range(len(examples[\"text\"])):\n",
        "        text = examples[\"text\"][i]\n",
        "        label = label_map[examples[\"label\"][i]]\n",
        "\n",
        "        # Format as instruction-response pair\n",
        "        formatted_text = f\"\"\"Classify the following news headline into one of the categories: World, Sports, Business, Sci/Tech.\n",
        "\n",
        "Text: \"{text}\"\n",
        "Label: {label}\"\"\"\n",
        "        formatted_texts.append(formatted_text)\n",
        "\n",
        "    return {\"text\": formatted_texts}\n",
        "\n",
        "# Format training data\n",
        "train_formatted = train_data.map(\n",
        "    format_training_data,\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    remove_columns=train_data.column_names\n",
        ")\n",
        "\n",
        "print(f\"Training dataset prepared: {len(train_formatted)} examples\")\n",
        "print(\"Sample formatted text:\")\n",
        "print(train_formatted[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "deccff83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deccff83",
        "outputId": "036753df-5d57-4c0f-f9b7-d6f8c1088bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n",
            "\n",
            "LoRA adapters injected successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create and configure LoRA adapters\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank adaptation\n",
        "    lora_alpha=32,  # Scaling factor for LoRA weights\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to apply LoRA to\n",
        "    lora_dropout=0.05,  # Dropout for LoRA layers\n",
        "    bias=\"none\",  # Don't train bias parameters\n",
        "    task_type=TaskType.CAUSAL_LM  # Task type (causal language modeling)\n",
        ")\n",
        "\n",
        "# Create PEFT model by wrapping the base model\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "peft_model.print_trainable_parameters()\n",
        "print(\"\\nLoRA adapters injected successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "61e24900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544,
          "referenced_widgets": [
            "bf0cf6aa8b424e12862e278cd6c3c841",
            "b66879115cbb4a01a7afee483202342d",
            "d3428feb77754b8c809bcbc8d55b1308",
            "f5cb76d77eb84327ad93f511280f4261",
            "d95d54f378134ebc90a934206d6e669b",
            "d7abafed340f490994826cf5cd32c931",
            "bef45f6ec2bb467c9a77601f5aed27d8",
            "2842ab4c6690468c853a631cff19a429",
            "d78b562326bd4268ab5e087288a86c04",
            "0d8793af006c4802bd0a9869e51171ea",
            "2b67df29739048d4ac3193eea302ffd6"
          ]
        },
        "id": "61e24900",
        "outputId": "6a72c791-3475-4468-cd80-f2b51bdb3b76"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf0cf6aa8b424e12862e278cd6c3c841"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-683300683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Tokenize training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m train_tokenized = train_formatted.map(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3316\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0munprocessed_kwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_kwargs_per_job\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0munprocessed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m                             \u001b[0mcheck_if_shard_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3672\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3673\u001b[0m                     \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3674\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3675\u001b[0m                         \u001b[0mnum_examples_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3676\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36miter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3624\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3545\u001b[0m             \u001b[0;34m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3546\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3547\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3548\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-683300683.py\u001b[0m in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Tokenize the text\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     encodings = tokenizer(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3022\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3108\u001b[0m                 )\n\u001b[1;32m   3109\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3110\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   3111\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m         \u001b[0;31m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3302\u001b[0;31m         padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n\u001b[0m\u001b[1;32m   3303\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3304\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2916\u001b[0m         \u001b[0;31m# Test if we have a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2919\u001b[0m                 \u001b[0;34m\"Asking to pad but the tokenizer does not have a padding token. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m                 \u001b[0;34m\"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
          ]
        }
      ],
      "source": [
        "# Prepare data for training with proper tokenization\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Tokenize the text\"\"\"\n",
        "    texts = examples[\"text\"]\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Set labels same as input_ids for causal language modeling\n",
        "    encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
        "    return encodings\n",
        "\n",
        "# Tokenize training data\n",
        "train_tokenized = train_formatted.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=8,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(f\"Training data tokenized: {len(train_tokenized)} examples\")\n",
        "print(\"Sample tokenized example keys:\", train_tokenized[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1996eed3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1996eed3",
        "outputId": "1d433621-a78d-444f-c906-4ee04d4a30b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_tokenized' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2002260380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpeft_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tokenized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_tokenized' is not defined"
          ]
        }
      ],
      "source": [
        "# Configure training parameters and trainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_model_results\",  # Output directory for model checkpoints\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=4,  # Batch size per device\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients over this many steps\n",
        "    warmup_steps=100,  # Number of warmup steps\n",
        "    weight_decay=0.01,  # Weight decay for regularization\n",
        "    logging_dir=\"./logs\",  # Directory for logs\n",
        "    logging_steps=10,  # Log every N steps\n",
        "    save_steps=50,  # Save checkpoint every N steps\n",
        "    save_total_limit=2,  # Keep only last 2 checkpoints\n",
        "    learning_rate=5e-4,  # Learning rate for training\n",
        "    optim=\"paged_adamw_8bit\",  # Optimizer\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2472cbdf",
      "metadata": {
        "id": "2472cbdf"
      },
      "outputs": [],
      "source": [
        "# Train the model with LoRA adapters\n",
        "import time\n",
        "\n",
        "print(\"Starting LoRA fine-tuning...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = (end_time - start_time) / 60  # Convert to minutes\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training completed in {training_time:.2f} minutes\")\n",
        "print(\"LoRA model saved to ./lora_model_results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1a4ce0",
      "metadata": {
        "id": "2b1a4ce0"
      },
      "outputs": [],
      "source": [
        "# Evaluate LoRA fine-tuned model on test set (full dataset)\n",
        "from tqdm import tqdm\n",
        "lora_predictions = []\n",
        "lora_true_labels = []\n",
        "\n",
        "print(\"Running inference with LoRA fine-tuned model on full test set...\")\n",
        "\n",
        "for i in tqdm(range(len(test_data)), desc=\"LoRA inference\"):\n",
        "    example = test_data[i]\n",
        "\n",
        "    if isinstance(example, dict):\n",
        "        text = example.get(\"text\") or example.get(\"content\")\n",
        "        label = example.get(\"label\")\n",
        "    else:\n",
        "        try:\n",
        "            text, label = example\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if text is None:\n",
        "        continue\n",
        "\n",
        "    prompt = create_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(peft_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = peft_model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
        "\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prediction_text = full_output[len(prompt):].strip()\n",
        "\n",
        "    matched_label = \"Unknown\"\n",
        "    for lbl in label_map.values():\n",
        "        if lbl.lower() in prediction_text.lower():\n",
        "            matched_label = lbl\n",
        "            break\n",
        "\n",
        "    lora_predictions.append(matched_label)\n",
        "    lora_true_labels.append(label_map[label])\n",
        "\n",
        "print(f\"LoRA inference completed on {len(lora_predictions)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2739f11f",
      "metadata": {
        "id": "2739f11f"
      },
      "outputs": [],
      "source": [
        "# Compare LoRA accuracy with baseline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calculate LoRA accuracy\n",
        "lora_accuracy = accuracy_score(lora_true_labels, lora_predictions)\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARISON: Baseline vs LoRA Fine-Tuned\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Baseline (Prompt-Based) Accuracy: {accuracy:.4f}\")\n",
        "print(f\"LoRA Fine-Tuned Accuracy:         {lora_accuracy:.4f}\")\n",
        "print(f\"Improvement:                      {(lora_accuracy - accuracy):.4f} ({((lora_accuracy - accuracy) / accuracy * 100):.2f}%)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nLoRA Classification Report:\")\n",
        "print(classification_report(lora_true_labels, lora_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd1f6b8",
      "metadata": {
        "id": "bdd1f6b8"
      },
      "outputs": [],
      "source": [
        "# Generate confusion matrix for LoRA model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get unique labels\n",
        "labels_list = sorted(label_map.values())\n",
        "\n",
        "# Compute confusion matrix for LoRA\n",
        "lora_cm = confusion_matrix(lora_true_labels, lora_predictions, labels=labels_list)\n",
        "\n",
        "# Plot confusion matrices side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Baseline confusion matrix\n",
        "cm_baseline = confusion_matrix(true_labels, predictions, labels=labels_list)\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels_list,\n",
        "            yticklabels=labels_list,\n",
        "            ax=axes[0],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "axes[0].set_title('Baseline (Prompt-Based) Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "\n",
        "# LoRA confusion matrix\n",
        "sns.heatmap(lora_cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=labels_list,\n",
        "            yticklabels=labels_list,\n",
        "            ax=axes[1],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "axes[1].set_title('LoRA Fine-Tuned Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLoRA Confusion Matrix:\")\n",
        "print(lora_cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d96310c",
      "metadata": {
        "id": "2d96310c"
      },
      "source": [
        "# Task 3: Fine-Tuning with QLoRA (Quantized LoRA)\n",
        "\n",
        "## Overview\n",
        "QLoRA loads a large pre-trained model in 4-bit quantized precision (using bitsandbytes) and applies LoRA adapters on top. This lets us fine-tune very large models on a single GPU with minimal memory by:\n",
        "\n",
        "- Quantizing weights to 4-bit (NF4 or similar) using `bitsandbytes` and `BitsAndBytesConfig`.\n",
        "- Wrapping the quantized model with PEFT LoRA adapters so only adapter parameters are trained.\n",
        "- Training using transformers' `Trainer` with optimizers compatible with quantized weights (e.g., 8-bit optimizers provided by bitsandbytes).\n",
        "\n",
        "## Steps in this notebook\n",
        "1. Install required libraries (bitsandbytes, accelerate, peft if not present)\n",
        "2. Prepare a BitsAndBytes quantization config and load the model in 4-bit\n",
        "3. Inject LoRA adapters into the quantized model\n",
        "4. Tokenize the formatted training dataset and create a Trainer\n",
        "5. Fine-tune the model (LoRA parameters only)\n",
        "6. Evaluate on the AG News test set and report accuracy\n",
        "7. Show confusion matrix and compare with previous approaches\n",
        "\n",
        "Notes:\n",
        "- Training a large model even with QLoRA can be slow; consider running a small subset first for debugging.\n",
        "- Adjust `bnb_4bit_*` parameters in the quantization config depending on your hardware and `transformers`/`bitsandbytes` versions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes accelerate transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2TPKsa9kWez",
        "outputId": "96183590-abd8-4146-8062-4c5a13909058"
      },
      "id": "p2TPKsa9kWez",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6e8c96af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e8c96af",
        "outputId": "f2584ce6-6863-4fda-e185-6b98c0ab25f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed/verified bitsandbytes, accelerate, peft, safetensors\n"
          ]
        }
      ],
      "source": [
        "# Install / ensure required libraries for QLoRA\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Install required packages if missing\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"bitsandbytes\", \"accelerate\", \"peft\", \"safetensors\"])\n",
        "print(\"Installed/verified bitsandbytes, accelerate, peft, safetensors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6a27fc4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a27fc4f",
        "outputId": "64c9b1ed-3f3d-44cb-f02c-0fd57f14deee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ bitsandbytes UPGRADED to latest\n",
            "✓ accelerate, peft, safetensors installed/verified\n"
          ]
        }
      ],
      "source": [
        "# (QLoRA) Install / verify extra dependencies - UPGRADE bitsandbytes\n",
        "import sys, subprocess\n",
        "\n",
        "# Force upgrade of bitsandbytes to latest version\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"])\n",
        "# Install other dependencies\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"accelerate\", \"peft\", \"safetensors\"])\n",
        "print(\"✓ bitsandbytes UPGRADED to latest\")\n",
        "print(\"✓ accelerate, peft, safetensors installed/verified\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0640a7d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "3f9a3b757b2f4c0f8cb0e5f5f2bac768",
            "99b8b4feaee64ba3be1f0f401d67767b",
            "3dd32ba9ea2e411ba0106a42bf536fa3",
            "36df0b39cdf046ebaf2ee64d0740d37f",
            "815b1d9dbd99465d8b54c043b7672830",
            "2ba7f3ef71bf40ea899d0148959bcf1e",
            "8c675ecd1b1f493c8820aa3f2164f753",
            "1f5da29764fb4ad48b4f08a7927d53c9",
            "fdef8259ec8549c9aab6f5477e1ee5c3",
            "975bb5f706a94cd38461daecbcf1d806",
            "8971cfeaec594e9cba76273ad2c5850d"
          ]
        },
        "id": "0640a7d9",
        "outputId": "1cb30802-0b91-481c-b9af-06bd35a49f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading quantized tokenizer and model (4-bit). This can take a while...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f9a3b757b2f4c0f8cb0e5f5f2bac768"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model loaded (4-bit).\n",
            "Device map: {'': 0}\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and model in 4-bit using BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "print(\"Loading quantized tokenizer and model (4-bit). This can take a while...\")\n",
        "\n",
        "tokenizer_q = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model_q = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False\n",
        ")\n",
        "\n",
        "print(\"Quantized model loaded (4-bit).\")\n",
        "if hasattr(model_q, 'hf_device_map'):\n",
        "    print(\"Device map:\", model_q.hf_device_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c1c98483",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1c98483",
        "outputId": "1f1e4445-b67d-48a5-a4ef-2a423ae1ef8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n",
            "LoRA adapters injected into 4-bit model\n"
          ]
        }
      ],
      "source": [
        "# Inject LoRA adapters into the quantized model (PEFT)\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "lora_config_q = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "peft_q_model = get_peft_model(model_q, lora_config_q)\n",
        "peft_q_model.print_trainable_parameters()\n",
        "print(\"LoRA adapters injected into 4-bit model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "train_data = dataset[\"train\"]\n",
        "test_data  = dataset[\"test\"]\n",
        "\n",
        "print(train_data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cG3VdgOmOAB",
        "outputId": "cb378c4c-95a9-4cf7-a63b-d7b0519f24e6"
      },
      "id": "4cG3VdgOmOAB",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    0: \"World\",\n",
        "    1: \"Sports\",\n",
        "    2: \"Business\",\n",
        "    3: \"Sci/Tech\"\n",
        "}\n",
        "\n",
        "def format_instruction(example):\n",
        "    return {\n",
        "        \"text\": f\"\"\"### Instruction:\n",
        "Classify the news into one of the following categories:\n",
        "World, Sports, Business, Sci/Tech.\n",
        "\n",
        "### Input:\n",
        "{example['text']}\n",
        "\n",
        "### Response:\n",
        "{label_map[example['label']]}\"\"\"\n",
        "    }\n",
        "\n",
        "train_formatted = train_data.map(format_instruction)\n",
        "\n",
        "print(\"✅ train_formatted created\")\n",
        "print(train_formatted[0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "2560c0159ca84221a0b7457a35fac571",
            "47f198d281e44a21b730f74edf96c63f",
            "47d24c4fd2c5400cb4d1dfd65c52a974",
            "3aac8bd7ac0f404f93d28c9d94d51655",
            "ae89e209a4774ea0a4c8dbe79c468db4",
            "d03acb2263c64021a2717ad7622bd611",
            "c3daca283b9f4fce9affcff883f76c16",
            "41685a9e1db746c7ba7ccf10abc0d697",
            "f530b276eabb4f788fa29b954ebfe0d1",
            "04b1931252d0411e90e8d0b76876dbcf",
            "0ba577b7437f42deb17e9e3ad29f3dac"
          ]
        },
        "id": "RDwtTPrNmQv5",
        "outputId": "cd78b1ff-aa8a-44ec-b5d8-b9ea9a415fd4"
      },
      "id": "RDwtTPrNmQv5",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2560c0159ca84221a0b7457a35fac571"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train_formatted created\n",
            "### Instruction:\n",
            "Classify the news into one of the following categories:\n",
            "World, Sports, Business, Sci/Tech.\n",
            "\n",
            "### Input:\n",
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
            "\n",
            "### Response:\n",
            "Business\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Fix for missing pad token in Mistral\n",
        "tokenizer_q.pad_token = tokenizer_q.eos_token\n"
      ],
      "metadata": {
        "id": "wyVEqzWkmzGX"
      },
      "id": "wyVEqzWkmzGX",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "823cd051",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "1a8b7b55c55f404bbbdce04159338169",
            "0fe9b6aec93e4c67a71bb3567443f8f6",
            "2df5cd67b7c54fcd9e959e91b51b9271",
            "887b19459791464e97078a5c9eaa681b",
            "78e8f895921c4099b67c893761f02afd",
            "21b76c2968e246df98fb441f9bae5525",
            "6d464c705eec499b87b294f0a101f0b9",
            "1865038fe0af4f48b855c33f65b9abab",
            "788503dcaaf943b985927583dd18c0bb",
            "d6ba7b5c6f514b7fb4032ad0465e756e",
            "ea86ed7f75564b8ead1a3e544762290b"
          ]
        },
        "id": "823cd051",
        "outputId": "b528dff7-5138-4e45-dbee-34e7233123e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a8b7b55c55f404bbbdce04159338169"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QLoRA training data tokenized: 120000 examples\n",
            "✅ Sample keys: ['label', 'input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_q(examples):\n",
        "    enc = tokenizer_q(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
        "    return enc\n",
        "\n",
        "train_tokenized_q = train_formatted.map(\n",
        "    preprocess_q,\n",
        "    batched=True,\n",
        "    batch_size=32,\n",
        "    remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "print(f\"✅ QLoRA training data tokenized: {len(train_tokenized_q)} examples\")\n",
        "print(\"✅ Sample keys:\", list(train_tokenized_q[0].keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "44c4ec16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44c4ec16",
        "outputId": "0db2cc33-b8db-420a-9527-360e28ffc25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3585193063.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_q = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QLoRA Trainer initialized\n"
          ]
        }
      ],
      "source": [
        "# Configure training arguments for QLoRA and initialize Trainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args_q = TrainingArguments(\n",
        "    output_dir=\"./qlora_model_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./qlora_logs\",\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=5e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        ")\n",
        "\n",
        "trainer_q = Trainer(\n",
        "    model=peft_q_model,\n",
        "    args=training_args_q,\n",
        "    train_dataset=train_tokenized_q,\n",
        "    tokenizer=tokenizer_q,\n",
        ")\n",
        "\n",
        "print(\"QLoRA Trainer initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "951d8af6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "951d8af6",
        "outputId": "d84ec85e-49c8-4ddc-984b-01a31d05bdd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting QLoRA fine-tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqadirju12\u001b[0m (\u001b[33mqadirju12-sukkur-iba-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251204_182747-021l4769</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/qadirju12-sukkur-iba-university/huggingface/runs/021l4769' target=\"_blank\">sunny-dust-3</a></strong> to <a href='https://wandb.ai/qadirju12-sukkur-iba-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/qadirju12-sukkur-iba-university/huggingface' target=\"_blank\">https://wandb.ai/qadirju12-sukkur-iba-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/qadirju12-sukkur-iba-university/huggingface/runs/021l4769' target=\"_blank\">https://wandb.ai/qadirju12-sukkur-iba-university/huggingface/runs/021l4769</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (1024) to match target batch_size (4).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2577479542.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting QLoRA fine-tuning...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"QLoRA training finished in {(end-start)/60:.2f} minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         return CausalLMOutputWithPast(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Enable model parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> torch.Tensor:\n\u001b[1;32m     35\u001b[0m     \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sum\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3459\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1024) to match target batch_size (4)."
          ]
        }
      ],
      "source": [
        "# Fine-tune QLoRA (LoRA on 4-bit) - run training\n",
        "import time\n",
        "\n",
        "print(\"Starting QLoRA fine-tuning...\")\n",
        "start = time.time()\n",
        "trainer_q.train()\n",
        "end = time.time()\n",
        "print(f\"QLoRA training finished in {(end-start)/60:.2f} minutes\")\n",
        "print(\"Model checkpoints saved to ./qlora_model_results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "73ce6be8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "73ce6be8",
        "outputId": "b13cbcc8-9b66-43d9-8afc-ad7801f2ece7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No checkpoint found. Starting from scratch.\n",
            "Running inference with QLoRA fine-tuned model on full test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "QLoRA inference:   7%|▋         | 555/7600 [04:00<50:54,  2.31it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1613005733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_q_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mfull_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2046\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mabsmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdequantize_blockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0mabsmax\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mdequantize_blockwise\u001b[0;34m(A, quant_state, absmax, code, out, blocksize, nested)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m     return torch.ops.bitsandbytes.dequantize_blockwise.default(\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mfunc_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_no_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/backends/cuda/ops.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(A, absmax, code, blocksize, dtype)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0m_dequantize_blockwise_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/backends/cuda/ops.py\u001b[0m in \u001b[0;36m_dequantize_blockwise_impl\u001b[0;34m(A, absmax, code, blocksize, dtype, out)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdequantize_blockwise_bf16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdequantize_blockwise_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "qlora_predictions = []\n",
        "qlora_true_labels = []\n",
        "\n",
        "CHECKPOINT_FILE = \"qlora_inference_checkpoint.json\"\n",
        "\n",
        "# ✅ Resume if checkpoint exists\n",
        "start_idx = 0\n",
        "try:\n",
        "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "        qlora_predictions = data[\"predictions\"]\n",
        "        qlora_true_labels = data[\"true_labels\"]\n",
        "        start_idx = len(qlora_predictions)\n",
        "        print(f\"✅ Resuming from index {start_idx}\")\n",
        "except:\n",
        "    print(\"✅ No checkpoint found. Starting from scratch.\")\n",
        "\n",
        "print(\"Running inference with QLoRA fine-tuned model on full test set...\")\n",
        "\n",
        "for i in tqdm(range(start_idx, len(test_data)), desc=\"QLoRA inference\"):\n",
        "    example = test_data[i]\n",
        "\n",
        "    if isinstance(example, dict):\n",
        "        text = example.get(\"text\") or example.get(\"content\")\n",
        "        label = example.get(\"label\")\n",
        "    else:\n",
        "        try:\n",
        "            text, label = example\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if text is None:\n",
        "        continue\n",
        "\n",
        "    prompt = create_prompt(text)\n",
        "    inputs = tokenizer_q(prompt, return_tensors=\"pt\").to(peft_q_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = peft_q_model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
        "\n",
        "    full_output = tokenizer_q.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred_text = full_output[len(prompt):].strip()\n",
        "\n",
        "    matched = \"Unknown\"\n",
        "    for lbl in label_map.values():\n",
        "        if lbl.lower() in pred_text.lower():\n",
        "            matched = lbl\n",
        "            break\n",
        "\n",
        "    qlora_predictions.append(matched)\n",
        "    qlora_true_labels.append(label_map[label])\n",
        "\n",
        "    # ✅ Auto-save every 100 samples\n",
        "    if (i + 1) % 100 == 0:\n",
        "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "            json.dump({\n",
        "                \"predictions\": qlora_predictions,\n",
        "                \"true_labels\": qlora_true_labels\n",
        "            }, f)\n",
        "\n",
        "# ✅ Final save\n",
        "with open(\"qlora_inference_final.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"predictions\": qlora_predictions,\n",
        "        \"true_labels\": qlora_true_labels\n",
        "    }, f)\n",
        "\n",
        "print(f\"✅ QLoRA inference completed on {len(qlora_predictions)} examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "qlora_accuracy = accuracy_score(qlora_true_labels, qlora_predictions)\n",
        "print(f\"✅ QLoRA Accuracy: {qlora_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n✅ QLoRA Classification Report:\")\n",
        "print(classification_report(qlora_true_labels, qlora_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chk4wZEEtUMN",
        "outputId": "7a59f31c-9011-40e6-e889-72bd0af27702"
      },
      "id": "Chk4wZEEtUMN",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ QLoRA Accuracy: 0.7694\n",
            "\n",
            "✅ QLoRA Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Business       0.66      0.82      0.73       114\n",
            "    Sci/Tech       0.63      0.72      0.68       144\n",
            "      Sports       0.96      0.83      0.89       155\n",
            "     Unknown       0.00      0.00      0.00         0\n",
            "       World       0.89      0.71      0.79       142\n",
            "\n",
            "    accuracy                           0.77       555\n",
            "   macro avg       0.63      0.62      0.62       555\n",
            "weighted avg       0.80      0.77      0.78       555\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6119d27dfd0b47c08725411839e6017b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ceb209032cf04430af6bacd7f6bead90",
              "IPY_MODEL_d17495a13ac541c292daf68ae4fa60d4",
              "IPY_MODEL_b213006807cd41439517197eb0ed5c64"
            ],
            "layout": "IPY_MODEL_0e989162f62b494a81cf8c9b6ab8110b"
          }
        },
        "ceb209032cf04430af6bacd7f6bead90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a623e0c54a37459b999615a60d42652f",
            "placeholder": "​",
            "style": "IPY_MODEL_e945a4d43b4740ef897b13393654cb9e",
            "value": "tokenizer_config.json: "
          }
        },
        "d17495a13ac541c292daf68ae4fa60d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3001c76684448a7ad2cfad0160a0105",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83c8b7bc9c1f43adb452875c16ec41c7",
            "value": 1
          }
        },
        "b213006807cd41439517197eb0ed5c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f620b872fa4441ae91e5cd7b27fc5165",
            "placeholder": "​",
            "style": "IPY_MODEL_7417052842ad4da8a5d6423473096805",
            "value": " 2.10k/? [00:00&lt;00:00, 212kB/s]"
          }
        },
        "0e989162f62b494a81cf8c9b6ab8110b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a623e0c54a37459b999615a60d42652f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e945a4d43b4740ef897b13393654cb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3001c76684448a7ad2cfad0160a0105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "83c8b7bc9c1f43adb452875c16ec41c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f620b872fa4441ae91e5cd7b27fc5165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7417052842ad4da8a5d6423473096805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e468970594b4cce9913c9860d8c5615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1c3447af7344d479bd3efb21da7f16d",
              "IPY_MODEL_2b3720b14b9246b19d13b5ccbbc25b4c",
              "IPY_MODEL_82ff770b4bcd4bbfb4dcf233b4265a18"
            ],
            "layout": "IPY_MODEL_55b1e60856064f0ca6dc926ae5155f80"
          }
        },
        "c1c3447af7344d479bd3efb21da7f16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cfa1cd8466b46c4a17df3544932a329",
            "placeholder": "​",
            "style": "IPY_MODEL_45a135580aa7414abd396c37b8fc8e9d",
            "value": "tokenizer.model: 100%"
          }
        },
        "2b3720b14b9246b19d13b5ccbbc25b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_542f192c978a471481d58f5125bbc1f5",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6350d857f19842b7ad535cfe9ceaf80b",
            "value": 493443
          }
        },
        "82ff770b4bcd4bbfb4dcf233b4265a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3432cca2bbb3464b8cdf811567991ba7",
            "placeholder": "​",
            "style": "IPY_MODEL_8f923868ba0a46a4a6b14df4db6e1a7c",
            "value": " 493k/493k [00:00&lt;00:00, 828kB/s]"
          }
        },
        "55b1e60856064f0ca6dc926ae5155f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cfa1cd8466b46c4a17df3544932a329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a135580aa7414abd396c37b8fc8e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "542f192c978a471481d58f5125bbc1f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6350d857f19842b7ad535cfe9ceaf80b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3432cca2bbb3464b8cdf811567991ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f923868ba0a46a4a6b14df4db6e1a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94d70e57c00248ac92de27d2714fa37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27c39124aaa042188a906955de4a55b5",
              "IPY_MODEL_296a7853739945d691a121598d5169ea",
              "IPY_MODEL_e47b6961c5b54849aa68c359fa4a7852"
            ],
            "layout": "IPY_MODEL_fbb6b783a3c048d2ab278a83eccb9114"
          }
        },
        "27c39124aaa042188a906955de4a55b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad32a47add0b4e7d8f53cbba06433d32",
            "placeholder": "​",
            "style": "IPY_MODEL_5046cfdd9c894f56b9748cf6913de717",
            "value": "tokenizer.json: "
          }
        },
        "296a7853739945d691a121598d5169ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5865c5686c57435a9616ba2aba79550b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_778711ba343d46a5a3cec93aedb9790e",
            "value": 1
          }
        },
        "e47b6961c5b54849aa68c359fa4a7852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b202f9101de492d91145a9a68d3c532",
            "placeholder": "​",
            "style": "IPY_MODEL_009b32ad1d1f40b38f7fde2d2df000d1",
            "value": " 1.80M/? [00:00&lt;00:00, 54.2MB/s]"
          }
        },
        "fbb6b783a3c048d2ab278a83eccb9114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad32a47add0b4e7d8f53cbba06433d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5046cfdd9c894f56b9748cf6913de717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5865c5686c57435a9616ba2aba79550b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "778711ba343d46a5a3cec93aedb9790e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b202f9101de492d91145a9a68d3c532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009b32ad1d1f40b38f7fde2d2df000d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b461e847326e4f17bde9f8217449c82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4af46ed46f5b48c6867fd167d127a534",
              "IPY_MODEL_14dbf4f5ce644b6abc6d8a0fba07a493",
              "IPY_MODEL_5581420550384fbf8a8e3ad602156213"
            ],
            "layout": "IPY_MODEL_d94a01a760fd40d99b56fab2f0c22e9a"
          }
        },
        "4af46ed46f5b48c6867fd167d127a534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29e5c96cdf1847d5aad9ad4b7417d86f",
            "placeholder": "​",
            "style": "IPY_MODEL_6f71af5af1bf45e4bc14212ef0e9048a",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "14dbf4f5ce644b6abc6d8a0fba07a493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36406d538cbf43dd8fc9eccb66e4004f",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a09e7ea61cdb405d851943b2082fe16c",
            "value": 414
          }
        },
        "5581420550384fbf8a8e3ad602156213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b826d673cc05450983bebedfa87ab937",
            "placeholder": "​",
            "style": "IPY_MODEL_dd25e3f0f8c147a689a6c24aea18d0a1",
            "value": " 414/414 [00:00&lt;00:00, 48.5kB/s]"
          }
        },
        "d94a01a760fd40d99b56fab2f0c22e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e5c96cdf1847d5aad9ad4b7417d86f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f71af5af1bf45e4bc14212ef0e9048a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36406d538cbf43dd8fc9eccb66e4004f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a09e7ea61cdb405d851943b2082fe16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b826d673cc05450983bebedfa87ab937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd25e3f0f8c147a689a6c24aea18d0a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4544c5700f14e5386e693fa84526ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2050c3ff65f8467ba6bf761320b3359f",
              "IPY_MODEL_c8d84654a00441bba178e7d4df51fefa",
              "IPY_MODEL_5c101a135f3643829e8562b8351d1c5b"
            ],
            "layout": "IPY_MODEL_5cea17ccb7cb416198e0cc6c97cff2e2"
          }
        },
        "2050c3ff65f8467ba6bf761320b3359f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb41df65932c4c848ef0462d6c455d37",
            "placeholder": "​",
            "style": "IPY_MODEL_4a03ef34f28042878c614d1f0da3bc2c",
            "value": "config.json: 100%"
          }
        },
        "c8d84654a00441bba178e7d4df51fefa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1839cd60794647c9941e783a70d50280",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdf793efe01c408daa20732f7864b387",
            "value": 571
          }
        },
        "5c101a135f3643829e8562b8351d1c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a48f5fe848147db8de28cedcfe13358",
            "placeholder": "​",
            "style": "IPY_MODEL_36e2ac8776d249fc9973c2941bc9b317",
            "value": " 571/571 [00:00&lt;00:00, 74.5kB/s]"
          }
        },
        "5cea17ccb7cb416198e0cc6c97cff2e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb41df65932c4c848ef0462d6c455d37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a03ef34f28042878c614d1f0da3bc2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1839cd60794647c9941e783a70d50280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf793efe01c408daa20732f7864b387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a48f5fe848147db8de28cedcfe13358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e2ac8776d249fc9973c2941bc9b317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79096278bbe4495191b2816a7074afa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4770d23e4be43fba7b4dbadbe931b8c",
              "IPY_MODEL_8c63e92a906542e3bb071e2754725cae",
              "IPY_MODEL_4fc19e5fdb6b4be49766a73dc158e411"
            ],
            "layout": "IPY_MODEL_c18a87109730434db836458855a186f9"
          }
        },
        "c4770d23e4be43fba7b4dbadbe931b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf6d672e3d704606b2a30013f2ca31dd",
            "placeholder": "​",
            "style": "IPY_MODEL_bdd099f68e22425c9771caca123bbeef",
            "value": "model.safetensors.index.json: "
          }
        },
        "8c63e92a906542e3bb071e2754725cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b73ae0565ced4f9493579ada09e6a3d8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bc20d71dfbf4700a8ce5daa63e4d098",
            "value": 1
          }
        },
        "4fc19e5fdb6b4be49766a73dc158e411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df573d3ccb07464bbb10e57e2fc06dab",
            "placeholder": "​",
            "style": "IPY_MODEL_8f5707c5bcdf47749997938497added6",
            "value": " 25.1k/? [00:00&lt;00:00, 2.17MB/s]"
          }
        },
        "c18a87109730434db836458855a186f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf6d672e3d704606b2a30013f2ca31dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd099f68e22425c9771caca123bbeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b73ae0565ced4f9493579ada09e6a3d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1bc20d71dfbf4700a8ce5daa63e4d098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df573d3ccb07464bbb10e57e2fc06dab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f5707c5bcdf47749997938497added6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "262d0eee9f6b4af3a13717991681ef76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c82d7aec3b0a46c2918e889db3bd1d2f",
              "IPY_MODEL_923e3eb0e3904a7ea97ced61cdea2881",
              "IPY_MODEL_a4b45c71d6034f6e92ac9b9e46b71216"
            ],
            "layout": "IPY_MODEL_d7a0b53317f645a78aa8e478e20048fb"
          }
        },
        "c82d7aec3b0a46c2918e889db3bd1d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b2fa84110d42cc953960d1baa474f4",
            "placeholder": "​",
            "style": "IPY_MODEL_d4269d5618494573bbb9a853cda3e87f",
            "value": "Fetching 2 files: 100%"
          }
        },
        "923e3eb0e3904a7ea97ced61cdea2881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665e0f6956204828a65175deaee5f6fb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e9d45a1a22c476ca3fc9088ae2f279e",
            "value": 2
          }
        },
        "a4b45c71d6034f6e92ac9b9e46b71216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aee1be0a32d44308a326c5cdeb0bcf43",
            "placeholder": "​",
            "style": "IPY_MODEL_184fdfd3de2c48ebacacf0967bdfb925",
            "value": " 2/2 [02:05&lt;00:00, 125.22s/it]"
          }
        },
        "d7a0b53317f645a78aa8e478e20048fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08b2fa84110d42cc953960d1baa474f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4269d5618494573bbb9a853cda3e87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665e0f6956204828a65175deaee5f6fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e9d45a1a22c476ca3fc9088ae2f279e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aee1be0a32d44308a326c5cdeb0bcf43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184fdfd3de2c48ebacacf0967bdfb925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "532bae22a562449da6965c6ba0acff26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0b9de2676854b23ba62a44a2248adfc",
              "IPY_MODEL_b8525919a3344f7ca1c57e3caa851482",
              "IPY_MODEL_8f0969995a4246378cc0d97490537591"
            ],
            "layout": "IPY_MODEL_d00e105459154af69ffe1d30e87ac176"
          }
        },
        "e0b9de2676854b23ba62a44a2248adfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570cafd424fa4703b069995c48997208",
            "placeholder": "​",
            "style": "IPY_MODEL_e99b8bc4ba384b4b8aa9558104c4ab11",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "b8525919a3344f7ca1c57e3caa851482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0af2ca69c20a458f9d08e46adbe712b5",
            "max": 4540516344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8f31108d1674ce6853b3c72d4cafa54",
            "value": 4540516344
          }
        },
        "8f0969995a4246378cc0d97490537591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bea98df8d7c4b889236e1015ae6ccd4",
            "placeholder": "​",
            "style": "IPY_MODEL_31423921f11944eaae78522791167466",
            "value": " 4.54G/4.54G [01:39&lt;00:00, 12.3MB/s]"
          }
        },
        "d00e105459154af69ffe1d30e87ac176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "570cafd424fa4703b069995c48997208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e99b8bc4ba384b4b8aa9558104c4ab11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0af2ca69c20a458f9d08e46adbe712b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8f31108d1674ce6853b3c72d4cafa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bea98df8d7c4b889236e1015ae6ccd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31423921f11944eaae78522791167466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b21deb7fd8624e2c862bc37e91a04251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3a6be21a7094fa5a81768586aaadb2f",
              "IPY_MODEL_ecbdfb480acf4015a3ed2e39f955de1d",
              "IPY_MODEL_e51ba6d2a27949099a8d87d53233b223"
            ],
            "layout": "IPY_MODEL_8c625a72483247deb27b7efd563faaf8"
          }
        },
        "e3a6be21a7094fa5a81768586aaadb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c327bbfc70e94b50b3e0aa46c96a52e6",
            "placeholder": "​",
            "style": "IPY_MODEL_f5e6ebbba2a749d1b77f0025a1526479",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "ecbdfb480acf4015a3ed2e39f955de1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2261de6586341de8defdced195c3cc7",
            "max": 9942981696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5e82eacfaf34708b803dabc17fe9cb6",
            "value": 9942981696
          }
        },
        "e51ba6d2a27949099a8d87d53233b223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88acee26436f485faab32a5290310277",
            "placeholder": "​",
            "style": "IPY_MODEL_227b5f56533a4459aadf014542e3de6b",
            "value": " 9.94G/9.94G [02:04&lt;00:00, 264MB/s]"
          }
        },
        "8c625a72483247deb27b7efd563faaf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c327bbfc70e94b50b3e0aa46c96a52e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5e6ebbba2a749d1b77f0025a1526479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2261de6586341de8defdced195c3cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5e82eacfaf34708b803dabc17fe9cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88acee26436f485faab32a5290310277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227b5f56533a4459aadf014542e3de6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82e00785ce75415d8e071f72cf399e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e0cd7b295f64d4d80e90e7e81e7aa97",
              "IPY_MODEL_4589f34f57624e52b1e1f35128baeeb1",
              "IPY_MODEL_036446151cfd4d7ca9d302a2f544fc4e"
            ],
            "layout": "IPY_MODEL_c217b9e66154473881d2aee482c1776e"
          }
        },
        "2e0cd7b295f64d4d80e90e7e81e7aa97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d44e2c9936064147a2e3e44fa62aa73c",
            "placeholder": "​",
            "style": "IPY_MODEL_1ad26f0d0a3d49ddab3ffcb0f64c643c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4589f34f57624e52b1e1f35128baeeb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54fbf3fa23e1471f8b2748967fd67e57",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e47e709d1a6e4737b3b22bfab63d01c3",
            "value": 2
          }
        },
        "036446151cfd4d7ca9d302a2f544fc4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fa0a7f059ad432b807107d3e2c80c17",
            "placeholder": "​",
            "style": "IPY_MODEL_b05e8034bf0a460a93d4c840380635dd",
            "value": " 2/2 [01:05&lt;00:00, 30.47s/it]"
          }
        },
        "c217b9e66154473881d2aee482c1776e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d44e2c9936064147a2e3e44fa62aa73c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ad26f0d0a3d49ddab3ffcb0f64c643c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54fbf3fa23e1471f8b2748967fd67e57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e47e709d1a6e4737b3b22bfab63d01c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fa0a7f059ad432b807107d3e2c80c17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b05e8034bf0a460a93d4c840380635dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9248f0b7c9d74a00b74cb3667c547ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c77fde2db75f407d87a5e4b87621e5d6",
              "IPY_MODEL_ee7665a97594402184809ffdee552653",
              "IPY_MODEL_f68ea9d50ae6458a86f19e4d6235376f"
            ],
            "layout": "IPY_MODEL_1fa3cb8390104e8b885b8329d4d162e0"
          }
        },
        "c77fde2db75f407d87a5e4b87621e5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_625e2475e4914889a03c033775d2915a",
            "placeholder": "​",
            "style": "IPY_MODEL_afbff346ae714cd382981dc911733ddc",
            "value": "generation_config.json: 100%"
          }
        },
        "ee7665a97594402184809ffdee552653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5e875f8001147a69f4913aee5d4c4d1",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3ff5c772c604c1a9dd5ca0d90c89fff",
            "value": 116
          }
        },
        "f68ea9d50ae6458a86f19e4d6235376f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_164d9ab660e64f7abbe907cc3a0bf330",
            "placeholder": "​",
            "style": "IPY_MODEL_610dffb2f45c4a8890c193044709c936",
            "value": " 116/116 [00:00&lt;00:00, 11.1kB/s]"
          }
        },
        "1fa3cb8390104e8b885b8329d4d162e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "625e2475e4914889a03c033775d2915a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afbff346ae714cd382981dc911733ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5e875f8001147a69f4913aee5d4c4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ff5c772c604c1a9dd5ca0d90c89fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "164d9ab660e64f7abbe907cc3a0bf330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610dffb2f45c4a8890c193044709c936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a1cc398da914be3a940d4fbda6f3cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa0c7f9ae84c4a6197d73cc29708d0c4",
              "IPY_MODEL_b9c98e24e7214e599c953bf5b3a71426",
              "IPY_MODEL_da7023602d2642d6bfa023167e8b3737"
            ],
            "layout": "IPY_MODEL_d5db24f2024f40fc8a74935de0c1782b"
          }
        },
        "aa0c7f9ae84c4a6197d73cc29708d0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce133797dc47496f84183ed05f741a8f",
            "placeholder": "​",
            "style": "IPY_MODEL_8163037a07484bad8e96cd603f69b91a",
            "value": "Map: 100%"
          }
        },
        "b9c98e24e7214e599c953bf5b3a71426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c5863b497cc4743b7ab39294fa132ea",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c130df0cfe90446db7970bec7807354a",
            "value": 120000
          }
        },
        "da7023602d2642d6bfa023167e8b3737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60ef45240e0149748164fc6eef16687d",
            "placeholder": "​",
            "style": "IPY_MODEL_c67488471ccb47b5adf2bbda6ed13192",
            "value": " 120000/120000 [00:02&lt;00:00, 66493.35 examples/s]"
          }
        },
        "d5db24f2024f40fc8a74935de0c1782b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce133797dc47496f84183ed05f741a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8163037a07484bad8e96cd603f69b91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c5863b497cc4743b7ab39294fa132ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c130df0cfe90446db7970bec7807354a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60ef45240e0149748164fc6eef16687d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c67488471ccb47b5adf2bbda6ed13192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf0cf6aa8b424e12862e278cd6c3c841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b66879115cbb4a01a7afee483202342d",
              "IPY_MODEL_d3428feb77754b8c809bcbc8d55b1308",
              "IPY_MODEL_f5cb76d77eb84327ad93f511280f4261"
            ],
            "layout": "IPY_MODEL_d95d54f378134ebc90a934206d6e669b"
          }
        },
        "b66879115cbb4a01a7afee483202342d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7abafed340f490994826cf5cd32c931",
            "placeholder": "​",
            "style": "IPY_MODEL_bef45f6ec2bb467c9a77601f5aed27d8",
            "value": "Map:   0%"
          }
        },
        "d3428feb77754b8c809bcbc8d55b1308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2842ab4c6690468c853a631cff19a429",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d78b562326bd4268ab5e087288a86c04",
            "value": 0
          }
        },
        "f5cb76d77eb84327ad93f511280f4261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8793af006c4802bd0a9869e51171ea",
            "placeholder": "​",
            "style": "IPY_MODEL_2b67df29739048d4ac3193eea302ffd6",
            "value": " 0/120000 [00:00&lt;?, ? examples/s]"
          }
        },
        "d95d54f378134ebc90a934206d6e669b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7abafed340f490994826cf5cd32c931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef45f6ec2bb467c9a77601f5aed27d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2842ab4c6690468c853a631cff19a429": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d78b562326bd4268ab5e087288a86c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d8793af006c4802bd0a9869e51171ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b67df29739048d4ac3193eea302ffd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f9a3b757b2f4c0f8cb0e5f5f2bac768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99b8b4feaee64ba3be1f0f401d67767b",
              "IPY_MODEL_3dd32ba9ea2e411ba0106a42bf536fa3",
              "IPY_MODEL_36df0b39cdf046ebaf2ee64d0740d37f"
            ],
            "layout": "IPY_MODEL_815b1d9dbd99465d8b54c043b7672830"
          }
        },
        "99b8b4feaee64ba3be1f0f401d67767b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ba7f3ef71bf40ea899d0148959bcf1e",
            "placeholder": "​",
            "style": "IPY_MODEL_8c675ecd1b1f493c8820aa3f2164f753",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3dd32ba9ea2e411ba0106a42bf536fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f5da29764fb4ad48b4f08a7927d53c9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdef8259ec8549c9aab6f5477e1ee5c3",
            "value": 2
          }
        },
        "36df0b39cdf046ebaf2ee64d0740d37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_975bb5f706a94cd38461daecbcf1d806",
            "placeholder": "​",
            "style": "IPY_MODEL_8971cfeaec594e9cba76273ad2c5850d",
            "value": " 2/2 [01:37&lt;00:00, 45.77s/it]"
          }
        },
        "815b1d9dbd99465d8b54c043b7672830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba7f3ef71bf40ea899d0148959bcf1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c675ecd1b1f493c8820aa3f2164f753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f5da29764fb4ad48b4f08a7927d53c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdef8259ec8549c9aab6f5477e1ee5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "975bb5f706a94cd38461daecbcf1d806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8971cfeaec594e9cba76273ad2c5850d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2560c0159ca84221a0b7457a35fac571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47f198d281e44a21b730f74edf96c63f",
              "IPY_MODEL_47d24c4fd2c5400cb4d1dfd65c52a974",
              "IPY_MODEL_3aac8bd7ac0f404f93d28c9d94d51655"
            ],
            "layout": "IPY_MODEL_ae89e209a4774ea0a4c8dbe79c468db4"
          }
        },
        "47f198d281e44a21b730f74edf96c63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d03acb2263c64021a2717ad7622bd611",
            "placeholder": "​",
            "style": "IPY_MODEL_c3daca283b9f4fce9affcff883f76c16",
            "value": "Map: 100%"
          }
        },
        "47d24c4fd2c5400cb4d1dfd65c52a974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41685a9e1db746c7ba7ccf10abc0d697",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f530b276eabb4f788fa29b954ebfe0d1",
            "value": 120000
          }
        },
        "3aac8bd7ac0f404f93d28c9d94d51655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04b1931252d0411e90e8d0b76876dbcf",
            "placeholder": "​",
            "style": "IPY_MODEL_0ba577b7437f42deb17e9e3ad29f3dac",
            "value": " 120000/120000 [00:10&lt;00:00, 22186.41 examples/s]"
          }
        },
        "ae89e209a4774ea0a4c8dbe79c468db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03acb2263c64021a2717ad7622bd611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3daca283b9f4fce9affcff883f76c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41685a9e1db746c7ba7ccf10abc0d697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f530b276eabb4f788fa29b954ebfe0d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04b1931252d0411e90e8d0b76876dbcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba577b7437f42deb17e9e3ad29f3dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a8b7b55c55f404bbbdce04159338169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fe9b6aec93e4c67a71bb3567443f8f6",
              "IPY_MODEL_2df5cd67b7c54fcd9e959e91b51b9271",
              "IPY_MODEL_887b19459791464e97078a5c9eaa681b"
            ],
            "layout": "IPY_MODEL_78e8f895921c4099b67c893761f02afd"
          }
        },
        "0fe9b6aec93e4c67a71bb3567443f8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b76c2968e246df98fb441f9bae5525",
            "placeholder": "​",
            "style": "IPY_MODEL_6d464c705eec499b87b294f0a101f0b9",
            "value": "Map: 100%"
          }
        },
        "2df5cd67b7c54fcd9e959e91b51b9271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1865038fe0af4f48b855c33f65b9abab",
            "max": 120000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_788503dcaaf943b985927583dd18c0bb",
            "value": 120000
          }
        },
        "887b19459791464e97078a5c9eaa681b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ba7b5c6f514b7fb4032ad0465e756e",
            "placeholder": "​",
            "style": "IPY_MODEL_ea86ed7f75564b8ead1a3e544762290b",
            "value": " 120000/120000 [00:42&lt;00:00, 3408.03 examples/s]"
          }
        },
        "78e8f895921c4099b67c893761f02afd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b76c2968e246df98fb441f9bae5525": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d464c705eec499b87b294f0a101f0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1865038fe0af4f48b855c33f65b9abab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788503dcaaf943b985927583dd18c0bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6ba7b5c6f514b7fb4032ad0465e756e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea86ed7f75564b8ead1a3e544762290b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}