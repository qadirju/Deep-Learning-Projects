{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ab7ce0",
   "metadata": {},
   "source": [
    "# Context-Aware Chatbot Using LangChain and RAG\n",
    "\n",
    "This project builds a conversational chatbot that retrieves information from a knowledge base using Retrieval-Augmented Generation (RAG) while maintaining conversation context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27e69f",
   "metadata": {},
   "source": [
    "## Project Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52acc522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "âœ“ All packages installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain-huggingface\",\n",
    "    \"langchain-community\",\n",
    "    \"langchain-text-splitters\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-cpu\",\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"streamlit\"\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"âœ“ All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359ed215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import Core Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9e500",
   "metadata": {},
   "source": [
    "## 1. Create Sample Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0508eb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f4f99075264903aa122ab1cdfee779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750cb95177084f48b255b7bfb75f8c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d350377a88cd4fd699a609de6045a746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd9f7998ae1439480c6606e76695c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28998c3fd97467a8d45d177c72545e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a21658c28a94c2290db034ad04605dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44944f327ec045b2b2759dc54b9ae908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bb7c75a6cb48f78edf62f89afa9bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf5a37048a54db481fc49ae9be0dc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e6f99228f24b3c9c3aae02a19574f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd01ef2b858470a8c7a002decdef628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06fdbac545b447eab565323e5f53530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Split documents into 4 chunks\n",
      "âœ“ Generated embeddings with dimension: 384\n",
      "âœ“ Created FAISS vector store successfully\n"
     ]
    }
   ],
   "source": [
    "# Process documents: Split into chunks and create embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "document_chunks = text_splitter.create_documents(sample_documents)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(document_chunks, embeddings)\n",
    "\n",
    "print(f\"âœ“ Split documents into {len(document_chunks)} chunks\")\n",
    "print(f\"âœ“ Generated embeddings with dimension: {embeddings.embed_query('test').__len__()}\")\n",
    "print(f\"âœ“ Created FAISS vector store successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9e4f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 4 sample documents for knowledge base\n",
      "Sample preview: LangChain is a framework for developing applications powered by language models. \n",
      "    It enables app...\n"
     ]
    }
   ],
   "source": [
    "# Create sample knowledge base\n",
    "sample_documents = [\n",
    "    \"\"\"LangChain is a framework for developing applications powered by language models. \n",
    "    It enables applications that are data-aware and agentic, allowing them to interact with \n",
    "    their environment and use external tools for computation and information retrieval.\"\"\",\n",
    "    \n",
    "    \"\"\"Retrieval-Augmented Generation (RAG) combines retrieval and generation capabilities. \n",
    "    It retrieves relevant documents from a knowledge base and uses them to augment the prompt \n",
    "    for better, more contextual responses from language models.\"\"\",\n",
    "    \"\"\"Vector databases like FAISS store embeddings of documents, enabling semantic search. \n",
    "    \n",
    "    When a user query is converted to embeddings, the database finds similar documents \n",
    "    based on vector similarity, which is faster than traditional keyword matching.\"\"\",\n",
    "    \n",
    "    \"\"\"Sentence Transformers are pre-trained models that encode text into dense vector representations. \n",
    "    These embeddings capture semantic meaning, allowing documents with similar meaning to have \n",
    "    similar vectors regardless of exact wording.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(sample_documents)} sample documents for knowledge base\")\n",
    "print(f\"Sample preview: {sample_documents[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2627699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa2ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Conversation memory initialized successfully\n",
      "Memory configured with: chat_history\n",
      "Ready to store multi-turn conversations\n"
     ]
    }
   ],
   "source": [
    "# Create conversation memory for context awareness\n",
    "# Simple memory implementation to store chat history\n",
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"Custom memory to store chat history for context awareness\"\"\"\n",
    "    def __init__(self, memory_key: str = \"chat_history\"):\n",
    "        self.memory_key = memory_key\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add a message to memory\"\"\"\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def get_memory(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve stored messages\"\"\"\n",
    "        return self.messages\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear memory\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationMemory(memory_key=\"chat_history\")\n",
    "\n",
    "print(\"âœ“ Conversation memory initialized successfully\")\n",
    "print(f\"Memory configured with: {memory.memory_key}\")\n",
    "print(f\"Ready to store multi-turn conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07408fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09193672647a439a94706a4b80851387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541f04d2bb9f4dd186d3b5187c95b04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7dc2cc0d044897b84a2e9c9286198e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adafa2bc54444619b3594d45fb72e973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d39940581664d1aa913e71767501fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f72175d25aa412bb36a9e90724769cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e88b4534ab4213938402fa9dbe61b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808cf286c2ed4229838e5e1340c7e88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LLM initialized successfully\n",
      "Model: google/flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "# Initialize Language Model (LLM) for response generation\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# Load text generation model\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "# Wrap with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "print(\"âœ“ LLM initialized successfully\")\n",
    "print(f\"Model: google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c6dee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG chain initialized successfully\n",
      "Ready for conversational retrieval tasks\n"
     ]
    }
   ],
   "source": [
    "# Create RAG chain for conversational retrieval\n",
    "# Custom implementation for combining retriever + LLM + memory\n",
    "\n",
    "class RAGChain:\n",
    "    \"\"\"Custom RAG chain combining retriever, LLM, and memory\"\"\"\n",
    "    def __init__(self, llm, retriever, memory):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.memory = memory\n",
    "    \n",
    "    def retrieve_documents(self, query: str) -> List[str]:\n",
    "        \"\"\"Retrieve relevant documents for query\"\"\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        return [doc.page_content for doc in docs]\n",
    "    \n",
    "    def format_context(self, retrieved_docs: List[str]) -> str:\n",
    "        \"\"\"Format retrieved documents as context\"\"\"\n",
    "        return \"\\n\\n\".join([f\"Document: {doc[:200]}...\" for doc in retrieved_docs])\n",
    "    \n",
    "    def generate_response(self, query: str) -> Dict:\n",
    "        \"\"\"Generate response using RAG approach\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve_documents(query)\n",
    "        \n",
    "        # Format context\n",
    "        context = self.format_context(retrieved_docs)\n",
    "        \n",
    "        # Get chat history\n",
    "        chat_history = self.memory.get_memory()\n",
    "        history_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
    "        \n",
    "        # Combine prompt\n",
    "        rag_prompt = f\"\"\"Context from knowledge base:\n",
    "{context}\n",
    "\n",
    "Chat History:\n",
    "{history_text}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Provide a helpful response:\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.llm.invoke(rag_prompt)\n",
    "        \n",
    "        # Store in memory\n",
    "        self.memory.add_message(\"user\", query)\n",
    "        self.memory.add_message(\"assistant\", response)\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"source_documents\": retrieved_docs\n",
    "        }\n",
    "\n",
    "# Initialize RAG chain\n",
    "rag_chain = RAGChain(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG chain initialized successfully\")\n",
    "print(\"Ready for conversational retrieval tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77aa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG chain with example queries\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTING RAG CHATBOT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example queries\n",
    "test_queries = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"Tell me about FAISS\",\n",
    "]\n",
    "\n",
    "# Test each query\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[Query {i}] {query}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = rag_chain.generate_response(query)\n",
    "    \n",
    "    print(f\"Response: {result['response'][:200]}...\")\n",
    "    print(f\"Retrieved {len(result['source_documents'])} source documents\")\n",
    "\n",
    "# Display conversation memory\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONVERSATION MEMORY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total messages in memory: {len(memory.get_memory())}\")\n",
    "for i, msg in enumerate(memory.get_memory(), 1):\n",
    "    print(f\"{i}. {msg['role'].upper()}: {msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit app code for deployment\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from typing import List, Dict\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(page_title=\"Context-Aware RAG Chatbot\", layout=\"wide\")\n",
    "st.title(\"ðŸ¤– Context-Aware Chatbot with RAG\")\n",
    "\n",
    "# Sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"Settings\")\n",
    "    st.write(\"RAG Chatbot Configuration\")\n",
    "    model_name = st.selectbox(\n",
    "        \"Select Model\",\n",
    "        [\"google/flan-t5-small\", \"google/flan-t5-base\"]\n",
    "    )\n",
    "    max_tokens = st.slider(\"Max Tokens\", 50, 512, 256)\n",
    "\n",
    "# Initialize session state\n",
    "if \"rag_chain\" not in st.session_state:\n",
    "    with st.spinner(\"ðŸ”„ Initializing RAG pipeline...\"):\n",
    "        # Knowledge base\n",
    "        documents = [\n",
    "            \"LangChain is a framework...\",\n",
    "            \"RAG combines retrieval...\",\n",
    "            \"FAISS enables semantic search...\"\n",
    "        ]\n",
    "        \n",
    "        # RAG pipeline initialization\n",
    "        st.session_state.rag_initialized = True\n",
    "        st.success(\"âœ“ RAG pipeline initialized!\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Chat interface\n",
    "st.subheader(\"ðŸ’¬ Chat with your documents\")\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "# User input\n",
    "if user_input := st.chat_input(\"Ask a question...\"):\n",
    "    # Add user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(user_input)\n",
    "    \n",
    "    # Generate response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"ðŸ¤” Thinking...\"):\n",
    "            # Simulate response (replace with actual RAG chain)\n",
    "            response = f\"Response to: {user_input}\"\n",
    "            st.write(response)\n",
    "            \n",
    "            # Store message\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Footer\n",
    "st.divider()\n",
    "st.caption(\"ðŸš€ Context-Aware RAG Chatbot | Powered by LangChain + FAISS\")\n",
    "'''\n",
    "\n",
    "# Save Streamlit app\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(\"âœ“ Streamlit app created: app.py\")\n",
    "print(\"\\nTo run the Streamlit app:\")\n",
    "print(\"  streamlit run app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d9742",
   "metadata": {},
   "source": [
    "## 3. Deploy with Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603f640",
   "metadata": {},
   "source": [
    "## 2. Test RAG Chain with Example Queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
