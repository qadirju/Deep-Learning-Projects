{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ab7ce0",
   "metadata": {},
   "source": [
    "# Context-Aware Chatbot Using LangChain and RAG\n",
    "\n",
    "This project builds a conversational chatbot that retrieves information from a knowledge base using Retrieval-Augmented Generation (RAG) while maintaining conversation context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27e69f",
   "metadata": {},
   "source": [
    "## Project Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52acc522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages...\n",
      "✓ All packages installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain-huggingface\",\n",
    "    \"langchain-community\",\n",
    "    \"langchain-text-splitters\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-cpu\",\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"streamlit\"\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"✓ All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359ed215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import Core Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9e500",
   "metadata": {},
   "source": [
    "## 1. Create Sample Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0508eb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f4f99075264903aa122ab1cdfee779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750cb95177084f48b255b7bfb75f8c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d350377a88cd4fd699a609de6045a746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd9f7998ae1439480c6606e76695c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28998c3fd97467a8d45d177c72545e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a21658c28a94c2290db034ad04605dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44944f327ec045b2b2759dc54b9ae908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bb7c75a6cb48f78edf62f89afa9bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf5a37048a54db481fc49ae9be0dc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e6f99228f24b3c9c3aae02a19574f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd01ef2b858470a8c7a002decdef628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06fdbac545b447eab565323e5f53530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Split documents into 4 chunks\n",
      "✓ Generated embeddings with dimension: 384\n",
      "✓ Created FAISS vector store successfully\n"
     ]
    }
   ],
   "source": [
    "# Process documents: Split into chunks and create embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "document_chunks = text_splitter.create_documents(sample_documents)\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vector_store = FAISS.from_documents(document_chunks, embeddings)\n",
    "\n",
    "print(f\"✓ Split documents into {len(document_chunks)} chunks\")\n",
    "print(f\"✓ Generated embeddings with dimension: {embeddings.embed_query('test').__len__()}\")\n",
    "print(f\"✓ Created FAISS vector store successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9e4f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 4 sample documents for knowledge base\n",
      "Sample preview: LangChain is a framework for developing applications powered by language models. \n",
      "    It enables app...\n"
     ]
    }
   ],
   "source": [
    "# Create sample knowledge base\n",
    "sample_documents = [\n",
    "    \"\"\"LangChain is a framework for developing applications powered by language models. \n",
    "    It enables applications that are data-aware and agentic, allowing them to interact with \n",
    "    their environment and use external tools for computation and information retrieval.\"\"\",\n",
    "    \n",
    "    \"\"\"Retrieval-Augmented Generation (RAG) combines retrieval and generation capabilities. \n",
    "    It retrieves relevant documents from a knowledge base and uses them to augment the prompt \n",
    "    for better, more contextual responses from language models.\"\"\",\n",
    "    \"\"\"Vector databases like FAISS store embeddings of documents, enabling semantic search. \n",
    "    \n",
    "    When a user query is converted to embeddings, the database finds similar documents \n",
    "    based on vector similarity, which is faster than traditional keyword matching.\"\"\",\n",
    "    \n",
    "    \"\"\"Sentence Transformers are pre-trained models that encode text into dense vector representations. \n",
    "    These embeddings capture semantic meaning, allowing documents with similar meaning to have \n",
    "    similar vectors regardless of exact wording.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"✓ Created {len(sample_documents)} sample documents for knowledge base\")\n",
    "print(f\"Sample preview: {sample_documents[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2627699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa2ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Conversation memory initialized successfully\n",
      "Memory configured with: chat_history\n",
      "Ready to store multi-turn conversations\n"
     ]
    }
   ],
   "source": [
    "# Create conversation memory for context awareness\n",
    "# Simple memory implementation to store chat history\n",
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"Custom memory to store chat history for context awareness\"\"\"\n",
    "    def __init__(self, memory_key: str = \"chat_history\"):\n",
    "        self.memory_key = memory_key\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add a message to memory\"\"\"\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def get_memory(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve stored messages\"\"\"\n",
    "        return self.messages\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear memory\"\"\"\n",
    "        self.messages = []\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationMemory(memory_key=\"chat_history\")\n",
    "\n",
    "print(\"✓ Conversation memory initialized successfully\")\n",
    "print(f\"Memory configured with: {memory.memory_key}\")\n",
    "print(f\"Ready to store multi-turn conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07408fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09193672647a439a94706a4b80851387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541f04d2bb9f4dd186d3b5187c95b04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7dc2cc0d044897b84a2e9c9286198e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adafa2bc54444619b3594d45fb72e973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d39940581664d1aa913e71767501fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f72175d25aa412bb36a9e90724769cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e88b4534ab4213938402fa9dbe61b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808cf286c2ed4229838e5e1340c7e88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM initialized successfully\n",
      "Model: google/flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "# Initialize Language Model (LLM) for response generation\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# Load text generation model\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "# Wrap with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "print(\"✓ LLM initialized successfully\")\n",
    "print(f\"Model: google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6dee80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3046657619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m rag_chain = ConversationalRetrievalChain.from_llm(\n\u001b[1;32m      4\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.chains'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create RAG chain for conversational retrieval\n",
    "# Custom implementation for combining retriever + LLM + memory\n",
    "\n",
    "class RAGChain:\n",
    "    \"\"\"Custom RAG chain combining retriever, LLM, and memory\"\"\"\n",
    "    def __init__(self, llm, retriever, memory):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "        self.memory = memory\n",
    "    \n",
    "    def retrieve_documents(self, query: str) -> List[str]:\n",
    "        \"\"\"Retrieve relevant documents for query\"\"\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        return [doc.page_content for doc in docs]\n",
    "    \n",
    "    def format_context(self, retrieved_docs: List[str]) -> str:\n",
    "        \"\"\"Format retrieved documents as context\"\"\"\n",
    "        return \"\\n\\n\".join([f\"Document: {doc[:200]}...\" for doc in retrieved_docs])\n",
    "    \n",
    "    def generate_response(self, query: str) -> Dict:\n",
    "        \"\"\"Generate response using RAG approach\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve_documents(query)\n",
    "        \n",
    "        # Format context\n",
    "        context = self.format_context(retrieved_docs)\n",
    "        \n",
    "        # Get chat history\n",
    "        chat_history = self.memory.get_memory()\n",
    "        history_text = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
    "        \n",
    "        # Combine prompt\n",
    "        rag_prompt = f\"\"\"Context from knowledge base:\n",
    "{context}\n",
    "\n",
    "Chat History:\n",
    "{history_text}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Provide a helpful response:\"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.llm.invoke(rag_prompt)\n",
    "        \n",
    "        # Store in memory\n",
    "        self.memory.add_message(\"user\", query)\n",
    "        self.memory.add_message(\"assistant\", response)\n",
    "        \n",
    "        return {\n",
    "            \"response\": response,\n",
    "            \"source_documents\": retrieved_docs\n",
    "        }\n",
    "\n",
    "# Initialize RAG chain\n",
    "rag_chain = RAGChain(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain initialized successfully\")\n",
    "print(\"Ready for conversational retrieval tasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
