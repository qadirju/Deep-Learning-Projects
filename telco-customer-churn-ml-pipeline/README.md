# ğŸ“Š Telco Customer Churn ML Pipeline

A comprehensive machine learning pipeline designed to predict customer churn in a telecommunications company. This project includes data preprocessing, model training with hyperparameter tuning, and an interactive Streamlit web application for real-time predictions.

## ğŸ¯ Project Overview

**Objective:** Build a predictive model to identify customers likely to churn (cancel their service) so the company can take proactive retention measures.

**Problem Statement:** Customer churn is a critical business challenge for telecom companies. Predicting which customers are at risk of leaving allows the company to implement targeted retention strategies, ultimately reducing revenue loss and improving customer lifetime value.

**Dataset:** Telco Customer Churn dataset containing approximately 7,000+ customer records with 20+ features including:
- Customer demographics (age, gender, location)
- Service subscriptions (internet, phone, streaming services)
- Account information (tenure, contract type, monthly charges)
- Churn status (target variable)

## âœ¨ Key Features

- **Automated Data Preprocessing Pipeline**: Handles missing values, feature encoding, and scaling
- **Multiple ML Models**: Logistic Regression, Random Forest with GridSearchCV optimization
- **Interactive Web App**: Streamlit-based UI for single and batch predictions
- **Model Evaluation**: Comprehensive metrics and visualizations
- **Production-Ready**: Serialized models for deployment

## ğŸ“ Project Structure

```
telco-customer-churn-ml-pipeline/
â”œâ”€â”€ README.md                                    # This file
â”œâ”€â”€ LICENSE                                      # Project license
â”œâ”€â”€ requirements.txt                             # Python dependencies
â”œâ”€â”€ STREAMLIT_GUIDE.md                          # Web app setup guide
â”œâ”€â”€ run_streamlit.py                            # Application launcher script
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py                        # Streamlit web application
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ telco_churn_dataset.csv            # Original dataset
â”‚   â””â”€â”€ processed/                              # Preprocessed data (generated)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ churn_pipeline.joblib                   # Trained model (generated)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_churn_pipeline_training.ipynb       # Training & analysis notebook
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py                   # Data cleaning & preparation
â”‚   â”œâ”€â”€ train_model.py                          # Model creation & training
â”‚   â””â”€â”€ predict.py                              # Inference module
â”‚
â””â”€â”€ reports/
    â””â”€â”€ model_evaluation.txt                    # Model performance metrics
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.7+
- pip or conda package manager

### Installation

1. **Clone the repository** (if applicable)
```bash
cd telco-customer-churn-ml-pipeline
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

### Running the Web Application

#### Option 1: Using the Launcher Script (Recommended)
```bash
python run_streamlit.py
```

#### Option 2: Direct Streamlit Command
```bash
streamlit run app/streamlit_app.py
```

The app will open automatically at `http://localhost:8501`

### Training the Model

1. **Open the Jupyter notebook:**
```bash
jupyter notebook notebooks/01_churn_pipeline_training.ipynb
```

2. **Execute all cells** to:
   - Load and explore the data
   - Preprocess features
   - Train and tune the model
   - Evaluate performance
   - Save the model

**Note:** Training includes GridSearchCV which may take 30-60 minutes depending on your hardware.

## ğŸ“š Usage

### Web Application Tabs

#### 1. **Single Prediction**
- Input individual customer information through an interactive form
- Get instant churn prediction with probability score
- Receive recommended retention actions based on the prediction

#### 2. **Batch Prediction**
- Upload a CSV file containing multiple customer records
- Process all customers at once
- Download results as a CSV file for further analysis

#### 3. **About**
- View model information and architecture
- Understanding the methodology and feature importance
- Review key performance metrics and model accuracy

## ğŸ”§ Technical Details

### Data Preprocessing Pipeline

```python
1. Data Loading & Cleaning
   â”œâ”€â”€ Load raw CSV dataset
   â”œâ”€â”€ Handle missing values
   â””â”€â”€ Convert data types

2. Feature Engineering
   â”œâ”€â”€ Encode categorical variables (One-Hot Encoding)
   â”œâ”€â”€ Handle numerical features
   â””â”€â”€ Feature scaling (StandardScaler)

3. Train-Test Split
   â””â”€â”€ 80-20 split stratified by target variable
```

### Model Training Pipeline

**Models Used:**
- **Logistic Regression**: Fast baseline model for binary classification
- **Random Forest Classifier**: Ensemble method for improved accuracy

**Hyperparameter Tuning:**
- GridSearchCV for systematic parameter optimization
- Cross-validation for robust evaluation

**Performance Metrics:**
- Accuracy
- Precision & Recall
- F1-Score
- ROC-AUC Score
- Confusion Matrix

### Feature Importance

The model considers approximately 20+ features including:
- **Demographics**: Senior Citizen, Gender
- **Services**: Internet Service, Phone Service, Streaming Services
- **Account**: Contract Type, Tenure, Monthly Charges, Total Charges
- **Support**: Tech Support, Online Security, Device Protection

## ğŸ“Š Model Evaluation

The trained model evaluation results are stored in:
```
reports/model_evaluation.txt
```

Typical performance metrics:
- Accuracy: ~80%+
- ROC-AUC: ~0.85+
- Precision: ~65%+
- Recall: ~55%+

*(Actual metrics depend on model version and training parameters)*

## ğŸ› Troubleshooting

### "Model not found" Error
**Solution:** Train the model first by running the Jupyter notebook:
```bash
jupyter notebook notebooks/01_churn_pipeline_training.ipynb
```

### Port 8501 Already in Use
**Solution:** Use a different port:
```bash
streamlit run app/streamlit_app.py --server.port 8502
```

### Dependencies Missing
**Solution:** Reinstall all requirements:
```bash
pip install -r requirements.txt --upgrade
```

### Streamlit Cache Issues
**Solution:** Clear the cache:
```bash
streamlit cache clear
```

## ğŸ“– Documentation

- **[Streamlit Guide](STREAMLIT_GUIDE.md)** - Detailed web app setup and usage
- **[Training Notebook](notebooks/01_churn_pipeline_training.ipynb)** - In-depth analysis and model development
- **[Source Code](src/)** - Modular Python modules for data processing and predictions

## ğŸ”‘ Key Technologies

| Technology | Purpose |
|-----------|---------|
| **scikit-learn** | Machine learning models and preprocessing |
| **pandas** | Data manipulation and analysis |
| **numpy** | Numerical computations |
| **joblib** | Model serialization |
| **Streamlit** | Interactive web application |
| **Jupyter** | Development and exploration |

## ğŸ“ˆ Future Enhancements

- [ ] Add Deep Learning models (Neural Networks)
- [ ] Implement SHAP for model interpretability
- [ ] Add time-series analysis
- [ ] Deploy to cloud platform (AWS, GCP, Azure)
- [ ] Add customer segmentation analysis
- [ ] Implement automated retraining pipeline

## ğŸ“ Project Flow

```
Raw Data
    â†“
[Data Preprocessing] â†’ Cleaned Data
    â†“
[Train-Test Split]
    â”œâ”€â”€ Training Set (80%)
    â””â”€â”€ Testing Set (20%)
    â†“
[Model Training & Tuning]
    â”œâ”€â”€ Logistic Regression
    â””â”€â”€ Random Forest
    â†“
[Model Evaluation]
    â”œâ”€â”€ Metrics
    â””â”€â”€ Visualizations
    â†“
[Serialized Model] â†’ Streamlit App â†’ User Predictions
```

## ğŸ‘¨â€ğŸ’» Development

### Code Structure

- **data_preprocessing.py**: Data loading, cleaning, and feature engineering
- **train_model.py**: Model creation, training, and hyperparameter tuning
- **predict.py**: Inference and predictions on new data
- **streamlit_app.py**: Interactive web interface
- **01_churn_pipeline_training.ipynb**: Comprehensive notebook with EDA and training

### Running Tests

To validate the pipeline with sample predictions:
```bash
python src/predict.py
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! To contribute:

1. Create a new branch (`git checkout -b feature/improvement`)
2. Make your changes
3. Commit with clear messages (`git commit -m 'Add improvement'`)
4. Push to the branch (`git push origin feature/improvement`)
5. Open a Pull Request

## ğŸ“§ Contact & Support

For questions or support regarding this project:
- Review the [Streamlit Guide](STREAMLIT_GUIDE.md)
- Check the [Jupyter Notebook](notebooks/01_churn_pipeline_training.ipynb) for detailed explanations
- Review source code documentation in the `src/` directory

## ğŸ“ Learning Outcomes

By exploring this project, you'll learn:
- Building end-to-end ML pipelines
- Data preprocessing and feature engineering techniques
- Model training and hyperparameter tuning
- Model evaluation and metrics interpretation
- Creating production-ready web applications with Streamlit
- Best practices for project organization

---

**Last Updated:** February 2026  
**Status:** âœ… Active & Maintained
