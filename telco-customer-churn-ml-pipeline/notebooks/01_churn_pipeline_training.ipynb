{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484e6033",
   "metadata": {},
   "source": [
    "# Telco Customer Churn ML Pipeline\n",
    "\n",
    "This notebook implements a complete machine learning pipeline for predicting customer churn in a telecommunications company. The pipeline includes data loading, preprocessing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b084378",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import essential Python libraries for data manipulation, visualization, and machine learning model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7bd09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e55f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd62f4d",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the Telco Customer Churn dataset from the raw data directory into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set working directory to the notebook directory\n",
    "os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01084bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data files: []\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Deep-Learning-Projects\\\\telco-customer-churn-ml-pipeline\\\\data\\\\raw\\\\telco_churn_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-61158014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Try alternative path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0malt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C:\\\\Users\\\\Deep-Learning-Projects\\\\telco-customer-churn-ml-pipeline\\\\data\\\\raw\\\\telco_churn_dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded from: {alt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Deep-Learning-Projects\\\\telco-customer-churn-ml-pipeline\\\\data\\\\raw\\\\telco_churn_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# First, let's find where the data file is\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nDirectory contents:\")\n",
    "for root, dirs, files in os.walk('..'):\n",
    "    level = root.replace('..', '').count(os.sep)\n",
    "    if level > 2:  # Limit depth\n",
    "        continue\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files\n",
    "        print(f'{subindent}{file}')\n",
    "\n",
    "# Try using find command to locate CSV\n",
    "try:\n",
    "    result = subprocess.run(['find', '..', '-name', '*.csv', '-type', 'f'], \n",
    "                           capture_output=True, text=True, timeout=5)\n",
    "    csv_files = [f for f in result.stdout.strip().split('\\n') if f]\n",
    "    print(f\"\\nCSV files found: {csv_files}\")\n",
    "    \n",
    "    if csv_files:\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        print(f\"Loaded dataset from: {csv_files[0]}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "    else:\n",
    "        print(\"Creating sample data for testing...\")\n",
    "        # Create minimal test data if file not found\n",
    "        df = pd.DataFrame({\n",
    "            'tenure': [1, 2, 3],\n",
    "            'MonthlyCharges': [29.85, 56.95, 53.85],\n",
    "            'TotalCharges': [29.85, 1397.475, 108.15],\n",
    "            'Churn': ['No', 'Yes', 'No']\n",
    "        })\n",
    "        print(f\"Using sample data. Shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error finding file: {e}\")\n",
    "    # Create minimal test data\n",
    "    df = pd.DataFrame({\n",
    "        'tenure': [1, 2, 3],\n",
    "        'MonthlyCharges': [29.85, 56.95, 53.85],\n",
    "        'TotalCharges': [29.85, 1397.475, 108.15],\n",
    "        'Churn': ['No', 'Yes', 'No']\n",
    "    })\n",
    "    print(f\"Using sample data. Shape: {df.shape}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d9398",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview\n",
    "\n",
    "Display fundamental information about the dataset structure, dimensions, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb4244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET SHAPE\n",
      "================================================================================\n",
      "Number of rows: 7043\n",
      "Number of columns: 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display dataset shape\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET SHAPE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c595e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIRST 5 ROWS\n",
      "================================================================================\n",
      "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
      "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
      "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
      "3  7795-CFOCW    Male              0      No         No      45           No   \n",
      "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
      "0  No phone service             DSL             No  ...               No   \n",
      "1                No             DSL            Yes  ...              Yes   \n",
      "2                No             DSL            Yes  ...               No   \n",
      "3  No phone service             DSL            Yes  ...              Yes   \n",
      "4                No     Fiber optic             No  ...               No   \n",
      "\n",
      "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
      "0          No          No              No  Month-to-month              Yes   \n",
      "1          No          No              No        One year               No   \n",
      "2          No          No              No  Month-to-month              Yes   \n",
      "3         Yes          No              No        One year               No   \n",
      "4          No          No              No  Month-to-month              Yes   \n",
      "\n",
      "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
      "0           Electronic check          29.85         29.85    No  \n",
      "1               Mailed check          56.95        1889.5    No  \n",
      "2               Mailed check          53.85        108.15   Yes  \n",
      "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
      "4           Electronic check          70.70        151.65   Yes  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"=\" * 80)\n",
    "print(\"FIRST 5 ROWS\")\n",
    "print(\"=\" * 80)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de01a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET INFORMATION\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   gender            7043 non-null   object \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   object \n",
      " 4   Dependents        7043 non-null   object \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   object \n",
      " 7   MultipleLines     7043 non-null   object \n",
      " 8   InternetService   7043 non-null   object \n",
      " 9   OnlineSecurity    7043 non-null   object \n",
      " 10  OnlineBackup      7043 non-null   object \n",
      " 11  DeviceProtection  7043 non-null   object \n",
      " 12  TechSupport       7043 non-null   object \n",
      " 13  StreamingTV       7043 non-null   object \n",
      " 14  StreamingMovies   7043 non-null   object \n",
      " 15  Contract          7043 non-null   object \n",
      " 16  PaperlessBilling  7043 non-null   object \n",
      " 17  PaymentMethod     7043 non-null   object \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7043 non-null   object \n",
      " 20  Churn             7043 non-null   object \n",
      "dtypes: float64(1), int64(2), object(18)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e036fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STATISTICAL SUMMARY\n",
      "================================================================================\n",
      "       SeniorCitizen       tenure  MonthlyCharges\n",
      "count    7043.000000  7043.000000     7043.000000\n",
      "mean        0.162147    32.371149       64.761692\n",
      "std         0.368612    24.559481       30.090047\n",
      "min         0.000000     0.000000       18.250000\n",
      "25%         0.000000     9.000000       35.500000\n",
      "50%         0.000000    29.000000       70.350000\n",
      "75%         0.000000    55.000000       89.850000\n",
      "max         1.000000    72.000000      118.750000\n"
     ]
    }
   ],
   "source": [
    "# Display statistical summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b5c49",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "The Telco dataset contains hidden missing values represented as blank strings (\" \")\n",
    "instead of proper NaN values. Machine learning algorithms and imputers cannot detect\n",
    "blank strings as missing data.\n",
    "\n",
    "Therefore, we first convert all blank entries into NaN so that the preprocessing\n",
    "pipeline (SimpleImputer) can handle them correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f87c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before cleaning:\n",
      "customerID          0\n",
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after replacing blanks:\n",
      "customerID           0\n",
      "gender               0\n",
      "SeniorCitizen        0\n",
      "Partner              0\n",
      "Dependents           0\n",
      "tenure               0\n",
      "PhoneService         0\n",
      "MultipleLines        0\n",
      "InternetService      0\n",
      "OnlineSecurity       0\n",
      "OnlineBackup         0\n",
      "DeviceProtection     0\n",
      "TechSupport          0\n",
      "StreamingTV          0\n",
      "StreamingMovies      0\n",
      "Contract             0\n",
      "PaperlessBilling     0\n",
      "PaymentMethod        0\n",
      "MonthlyCharges       0\n",
      "TotalCharges        11\n",
      "Churn                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# check missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# replace blank strings with NaN\n",
    "df = df.replace(\" \", np.nan)\n",
    "\n",
    "print(\"\\nMissing values after replacing blanks:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bff3cd",
   "metadata": {},
   "source": [
    "## Converting TotalCharges to Numeric\n",
    "\n",
    "The 'TotalCharges' column is incorrectly stored as an object (string) datatype due to\n",
    "the presence of blank values. Machine learning models require numeric input.\n",
    "\n",
    "We convert this column to a numeric datatype. Any non-convertible values will be\n",
    "automatically converted into NaN, which will later be handled by the imputer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "112d0c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "\n",
      "Missing values in TotalCharges:\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# convert to numeric\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# verify datatype\n",
    "print(df.dtypes['TotalCharges'])\n",
    "\n",
    "# check new missing values created\n",
    "print(\"\\nMissing values in TotalCharges:\")\n",
    "print(df['TotalCharges'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867eb7b7",
   "metadata": {},
   "source": [
    "## Feature‚ÄìTarget Separation\n",
    "\n",
    "To train a machine learning model, we separate the dataset into:\n",
    "\n",
    "X (features): all input variables describing a customer  \n",
    "y (target): the variable we want to predict (Churn)\n",
    "\n",
    "The model will learn the relationship between X and y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc389a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (7043, 20)\n",
      "Target shape: (7043,)\n"
     ]
    }
   ],
   "source": [
    "# features and target\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c691a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aea98ad",
   "metadata": {},
   "source": [
    "## 8. Split Dataset into Training and Testing Sets\n",
    "\n",
    "Split the data into training (80%) and testing (20%) sets using train_test_split with a fixed random state for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dcce980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 5634 samples (80.0%)\n",
      "Testing set size: 1409 samples (20.0%)\n",
      "\n",
      "Training features shape: (5634, 20)\n",
      "Testing features shape: (1409, 20)\n",
      "\n",
      "Class distribution in training set:\n",
      "Churn\n",
      "No     4139\n",
      "Yes    1495\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in testing set:\n",
      "Churn\n",
      "No     1035\n",
      "Yes     374\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nClass distribution in testing set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24cd28f",
   "metadata": {},
   "source": [
    "## 9. Automatically Detect Categorical and Numerical Feature Columns\n",
    "\n",
    "Separate features into categorical and numerical columns based on their data types for appropriate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "451a8a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features (4):\n",
      "['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "\n",
      "Categorical features (16):\n",
      "['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "\n",
      "Data types in training set:\n",
      "customerID           object\n",
      "gender               object\n",
      "SeniorCitizen         int64\n",
      "Partner              object\n",
      "Dependents           object\n",
      "tenure                int64\n",
      "PhoneService         object\n",
      "MultipleLines        object\n",
      "InternetService      object\n",
      "OnlineSecurity       object\n",
      "OnlineBackup         object\n",
      "DeviceProtection     object\n",
      "TechSupport          object\n",
      "StreamingTV          object\n",
      "StreamingMovies      object\n",
      "Contract             object\n",
      "PaperlessBilling     object\n",
      "PaymentMethod        object\n",
      "MonthlyCharges      float64\n",
      "TotalCharges        float64\n",
      "dtype: object\n",
      "\n",
      "Shape of training features: (5634, 20)\n",
      "Number of numerical features: 4\n",
      "Number of categorical features: 16\n"
     ]
    }
   ],
   "source": [
    "# Automatically detect categorical and numerical columns\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}):\")\n",
    "print(numerical_features)\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "print(categorical_features)\n",
    "\n",
    "# Display data types\n",
    "print(f\"\\nData types in training set:\")\n",
    "print(X_train.dtypes)\n",
    "print(f\"\\nShape of training features: {X_train.shape}\")\n",
    "print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12734ab4",
   "metadata": {},
   "source": [
    "## 10. Create Numerical Preprocessing Pipeline\n",
    "\n",
    "Create a preprocessing pipeline for numerical features using SimpleImputer (for missing values) and StandardScaler (for normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46bedbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Preprocessing Pipeline created:\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('scaler', StandardScaler())])\n",
      "\n",
      "This pipeline will be applied to 4 numerical features:\n",
      "['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "\n",
      "Pipeline steps:\n",
      "  1. SimpleImputer: Handles missing values using median strategy\n",
      "  2. StandardScaler: Normalizes features to have mean=0 and standard deviation=1\n"
     ]
    }
   ],
   "source": [
    "# Import preprocessing tools\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create numerical preprocessing pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill missing values with median\n",
    "    ('scaler', StandardScaler())  # Standardize features (mean=0, std=1)\n",
    "])\n",
    "\n",
    "print(\"Numerical Preprocessing Pipeline created:\")\n",
    "print(numerical_pipeline)\n",
    "print(f\"\\nThis pipeline will be applied to {len(numerical_features)} numerical features:\")\n",
    "print(numerical_features)\n",
    "print(\"\\nPipeline steps:\")\n",
    "print(\"  1. SimpleImputer: Handles missing values using median strategy\")\n",
    "print(\"  2. StandardScaler: Normalizes features to have mean=0 and standard deviation=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde1052",
   "metadata": {},
   "source": [
    "## 11. Create Categorical Preprocessing Pipeline\n",
    "\n",
    "Create a preprocessing pipeline for categorical features using SimpleImputer (to handle missing values) and OneHotEncoder (to convert categories into numerical format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c29abfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Preprocessing Pipeline created:\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('onehot',\n",
      "                 OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
      "\n",
      "This pipeline will be applied to 16 categorical features:\n",
      "['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "\n",
      "Pipeline steps:\n",
      "  1. SimpleImputer: Handles missing values using most_frequent strategy\n",
      "  2. OneHotEncoder: Converts categorical variables into binary (one-hot) encoded features\n"
     ]
    }
   ],
   "source": [
    "# Create categorical preprocessing pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill with most frequent value\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Convert to binary features\n",
    "])\n",
    "\n",
    "print(\"Categorical Preprocessing Pipeline created:\")\n",
    "print(categorical_pipeline)\n",
    "print(f\"\\nThis pipeline will be applied to {len(categorical_features)} categorical features:\")\n",
    "print(categorical_features)\n",
    "print(\"\\nPipeline steps:\")\n",
    "print(\"  1. SimpleImputer: Handles missing values using most_frequent strategy\")\n",
    "print(\"  2. OneHotEncoder: Converts categorical variables into binary (one-hot) encoded features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fb64f",
   "metadata": {},
   "source": [
    "## 12. Combine Both Pipelines Using ColumnTransformer\n",
    "\n",
    "Combine the numerical and categorical preprocessing pipelines into a single ColumnTransformer that applies the appropriate preprocessing to each feature type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73fb8642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer created combining both pipelines:\n",
      "\n",
      "Numerical features (4): ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "Categorical features (16): ['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "\n",
      "Transformers:\n",
      "  1. 'num': Applies numerical_pipeline to numerical features\n",
      "  2. 'cat': Applies categorical_pipeline to categorical features\n"
     ]
    }
   ],
   "source": [
    "# Combine both preprocessing pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns as-is if any\n",
    ")\n",
    "\n",
    "print(\"ColumnTransformer created combining both pipelines:\")\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(\"\\nTransformers:\")\n",
    "print(\"  1. 'num': Applies numerical_pipeline to numerical features\")\n",
    "print(\"  2. 'cat': Applies categorical_pipeline to categorical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ae24b",
   "metadata": {},
   "source": [
    "## 13. Build Full Pipeline with Preprocessing + Logistic Regression\n",
    "\n",
    "Create a complete pipeline that combines the preprocessor with a Logistic Regression model. This pipeline will handle all data transformations and model training end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27914c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Logistic Regression Pipeline created:\n",
      "\n",
      "Pipeline structure:\n",
      "  Step 1: Preprocessor (ColumnTransformer)\n",
      "    - Numerical features ‚Üí Impute (median) ‚Üí Scale (StandardScaler)\n",
      "    - Categorical features ‚Üí Impute (most_frequent) ‚Üí OneHotEncoder\n",
      "  Step 2: Classifier (LogisticRegression)\n",
      "    - Algorithm: Logistic Regression\n",
      "    - max_iter: 1000\n",
      "    - random_state: 42 (for reproducibility)\n",
      "\n",
      "This pipeline will:\n",
      "  1. Automatically preprocess features based on their types\n",
      "  2. Train a Logistic Regression model on the preprocessed data\n"
     ]
    }
   ],
   "source": [
    "# Build full pipeline with preprocessing + Logistic Regression\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42, verbose=0))\n",
    "])\n",
    "\n",
    "print(\"Full Logistic Regression Pipeline created:\")\n",
    "print(\"\\nPipeline structure:\")\n",
    "print(f\"  Step 1: Preprocessor (ColumnTransformer)\")\n",
    "print(f\"    - Numerical features ‚Üí Impute (median) ‚Üí Scale (StandardScaler)\")\n",
    "print(f\"    - Categorical features ‚Üí Impute (most_frequent) ‚Üí OneHotEncoder\")\n",
    "print(f\"  Step 2: Classifier (LogisticRegression)\")\n",
    "print(f\"    - Algorithm: Logistic Regression\")\n",
    "print(f\"    - max_iter: 1000\")\n",
    "print(f\"    - random_state: 42 (for reproducibility)\")\n",
    "print(f\"\\nThis pipeline will:\")\n",
    "print(f\"  1. Automatically preprocess features based on their types\")\n",
    "print(f\"  2. Train a Logistic Regression model on the preprocessed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters and best CV score\n",
    "print(\"=\"*80)\n",
    "print(\"BEST HYPERPARAMETERS FOR LOGISTIC REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in lr_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation Score (accuracy): {lr_grid_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest Estimator:\")\n",
    "print(lr_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59884f69",
   "metadata": {},
   "source": [
    "## 17. Print Best Parameters and Best CV Score\n",
    "\n",
    "Display the best hyperparameters found and the corresponding cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ba68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GridSearchCV... (This may take a few minutes)\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    }
   ],
   "source": [
    "# Train GridSearchCV on training data\n",
    "print(\"Training GridSearchCV... (This may take a few minutes)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GridSearchCV training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f5b87",
   "metadata": {},
   "source": [
    "## 16. Train GridSearch on Training Data\n",
    "\n",
    "Fit the GridSearchCV object on the training data to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40b2d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV configured:\n",
      "  Estimator: Logistic Regression Pipeline\n",
      "  Param Grid: 54 combinations\n",
      "  Cross-Validation: 5-fold\n",
      "  Scoring Metric: accuracy\n",
      "  N_jobs: -1 (use all processors)\n",
      "\n",
      "GridSearchCV will test all hyperparameter combinations using cross-validation\n"
     ]
    }
   ],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create GridSearchCV object\n",
    "lr_grid_search = GridSearchCV(\n",
    "    estimator=lr_pipeline,\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',  # Use accuracy as the scoring metric\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"GridSearchCV configured:\")\n",
    "print(f\"  Estimator: Logistic Regression Pipeline\")\n",
    "print(f\"  Param Grid: 54 combinations\")\n",
    "print(f\"  Cross-Validation: 5-fold\")\n",
    "print(f\"  Scoring Metric: accuracy\")\n",
    "print(f\"  N_jobs: -1 (use all processors)\")\n",
    "print(f\"\\nGridSearchCV will test all hyperparameter combinations using cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f48d68",
   "metadata": {},
   "source": [
    "## 15. Run GridSearchCV with Cross-Validation\n",
    "\n",
    "Create and configure GridSearchCV to find the best hyperparameters using 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f7a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Hyperparameter Grid defined:\n",
      "\n",
      "C (Inverse Regularization Strength): [0.001, 0.01, 0.1, 1, 10, 100]\n",
      "Solver (Optimization Algorithm): ['lbfgs', 'liblinear', 'saga']\n",
      "Max Iterations: [500, 1000, 2000]\n",
      "\n",
      "Total combinations to test: 54 = 54\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for Logistic Regression\n",
    "lr_param_grid = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse regularization strength\n",
    "    'classifier__solver': ['lbfgs', 'liblinear', 'saga'],  # Optimization algorithm\n",
    "    'classifier__max_iter': [500, 1000, 2000]  # Max iterations for convergence\n",
    "}\n",
    "\n",
    "print(\"Logistic Regression Hyperparameter Grid defined:\")\n",
    "print(f\"\\nC (Inverse Regularization Strength): {lr_param_grid['classifier__C']}\")\n",
    "print(f\"Solver (Optimization Algorithm): {lr_param_grid['classifier__solver']}\")\n",
    "print(f\"Max Iterations: {lr_param_grid['classifier__max_iter']}\")\n",
    "print(f\"\\nTotal combinations to test: {len(lr_param_grid['classifier__C']) * len(lr_param_grid['classifier__solver']) * len(lr_param_grid['classifier__max_iter'])} = {6 * 3 * 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55cc7c",
   "metadata": {},
   "source": [
    "## 14. Define Hyperparameter Grid for Logistic Regression\n",
    "\n",
    "Define a grid of hyperparameters to test for optimizing the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568c4324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ WARNING: GridSearchCV models are still training or not yet executed.\n",
      "Please wait for the GridSearchCV cells (21 and 26) to complete first.\n",
      "\n",
      "Expected training time: 30-60 minutes depending on your system.\n",
      "\n",
      "You can check the status of running cells in the notebook output.\n"
     ]
    }
   ],
   "source": [
    "# Check if GridSearch objects have been created and trained\n",
    "if 'grid_search_rf' not in locals() or 'grid_search_lr' not in locals():\n",
    "    print(\"‚è≥ WARNING: GridSearchCV models are still training or not yet executed.\")\n",
    "    print(\"Please wait for the GridSearchCV cells (21 and 26) to complete first.\")\n",
    "    print(\"\\nExpected training time: 30-60 minutes depending on your system.\")\n",
    "    print(\"\\nYou can check the status of running cells in the notebook output.\")\n",
    "else:\n",
    "    try:\n",
    "        # Evaluate Random Forest on test set\n",
    "        y_pred_rf = grid_search_rf.predict(X_test)\n",
    "        y_pred_proba_rf = grid_search_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "        precision_rf = precision_score(y_test, y_pred_rf, pos_label='Yes')\n",
    "        recall_rf = recall_score(y_test, y_pred_rf, pos_label='Yes')\n",
    "        f1_rf = f1_score(y_test, y_pred_rf, pos_label='Yes')\n",
    "        roc_auc_rf = roc_auc_score(y_test.map({'No': 0, 'Yes': 1}), y_pred_proba_rf)\n",
    "\n",
    "        # Create comparison dataframe\n",
    "        comparison_data = {\n",
    "            'Model': ['Logistic Regression', 'Random Forest'],\n",
    "            'Accuracy': [accuracy_lr, accuracy_rf],\n",
    "            'Precision': [precision_lr, precision_rf],\n",
    "            'Recall': [recall_lr, recall_rf],\n",
    "            'F1-Score': [f1_lr, f1_rf],\n",
    "            'ROC-AUC': [roc_auc_lr, roc_auc_rf]\n",
    "        }\n",
    "\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"MODEL COMPARISON - LOGISTIC REGRESSION vs RANDOM FOREST\")\n",
    "        print(\"=\" * 100)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "\n",
    "        # Select better model\n",
    "        best_model_name = 'Random Forest' if accuracy_rf > accuracy_lr else 'Logistic Regression'\n",
    "        best_model = grid_search_rf if accuracy_rf > accuracy_lr else grid_search_lr\n",
    "\n",
    "        print(f\"\\n\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "        print(f\"   Accuracy: {max(accuracy_rf, accuracy_lr):.4f}\")\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"‚ùå Error: Missing required variables: {str(e)}\")\n",
    "        print(\"\\nMake sure you have run ALL previous cells in order:\")\n",
    "        print(\"  1. GridSearchCV for Logistic Regression (cell 26)\")\n",
    "        print(\"  2. Train GridSearchCV on training data (cell 27)\")\n",
    "        print(\"  3. Make predictions on test set (cell 29)\")\n",
    "        print(\"  4. Evaluate Logistic Regression (cell 31)\")\n",
    "        print(\"  5. Random Forest pipeline, hyperparameters, and GridSearchCV cells\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {str(e)}\")\n",
    "        print(\"\\nPlease ensure all dependent cells have completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ecba01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STREAMLIT DEPLOYMENT GUIDE\n",
      "================================================================================\n",
      "\n",
      "To deploy this model as a web application, follow these steps:\n",
      "\n",
      "1. Install Streamlit:\n",
      "   pip install streamlit\n",
      "\n",
      "2. Create a file 'streamlit_app.py' with the following structure:\n",
      "\n",
      "   import streamlit as st\n",
      "   import joblib\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "\n",
      "   # Load the saved pipeline\n",
      "   pipeline = joblib.load('churn_pipeline.joblib')\n",
      "\n",
      "   st.title('Telco Customer Churn Predictor')\n",
      "\n",
      "   # Create input fields for customer data\n",
      "   gender = st.selectbox('Gender', ['Male', 'Female'])\n",
      "   tenure = st.number_input('Tenure (months)', 0, 72, value=12)\n",
      "   monthly_charges = st.number_input('Monthly Charges', 0.0, 200.0, value=89.45)\n",
      "   # ... add more input fields for all features ...\n",
      "\n",
      "   # Make prediction\n",
      "   if st.button('Predict Churn'):\n",
      "       customer_data = pd.DataFrame({...})  # Organize all inputs\n",
      "       prediction = pipeline.predict(customer_data)[0]\n",
      "       probability = pipeline.predict_proba(customer_data)[0]\n",
      "       \n",
      "       if prediction == 'Yes':\n",
      "           st.error(f'‚ö†Ô∏è Customer is likely to CHURN')\n",
      "       else:\n",
      "           st.success(f'‚úÖ Customer is likely to STAY')\n",
      "\n",
      "3. Run the app:\n",
      "   streamlit run streamlit_app.py\n",
      "\n",
      "4. Access the app in your browser at: http://localhost:8501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STREAMLIT DEPLOYMENT GUIDE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "To deploy this model as a web application, follow these steps:\n",
    "\n",
    "1. Install Streamlit:\n",
    "   pip install streamlit\n",
    "\n",
    "2. Create a file 'streamlit_app.py' with the following structure:\n",
    "\n",
    "   import streamlit as st\n",
    "   import joblib\n",
    "   import pandas as pd\n",
    "   import numpy as np\n",
    "\n",
    "   # Load the saved pipeline\n",
    "   pipeline = joblib.load('churn_pipeline.joblib')\n",
    "\n",
    "   st.title('Telco Customer Churn Predictor')\n",
    "\n",
    "   # Create input fields for customer data\n",
    "   gender = st.selectbox('Gender', ['Male', 'Female'])\n",
    "   tenure = st.number_input('Tenure (months)', 0, 72, value=12)\n",
    "   monthly_charges = st.number_input('Monthly Charges', 0.0, 200.0, value=89.45)\n",
    "   # ... add more input fields for all features ...\n",
    "\n",
    "   # Make prediction\n",
    "   if st.button('Predict Churn'):\n",
    "       customer_data = pd.DataFrame({...})  # Organize all inputs\n",
    "       prediction = pipeline.predict(customer_data)[0]\n",
    "       probability = pipeline.predict_proba(customer_data)[0]\n",
    "       \n",
    "       if prediction == 'Yes':\n",
    "           st.error(f'‚ö†Ô∏è Customer is likely to CHURN')\n",
    "       else:\n",
    "           st.success(f'‚úÖ Customer is likely to STAY')\n",
    "\n",
    "3. Run the app:\n",
    "   streamlit run streamlit_app.py\n",
    "\n",
    "4. Access the app in your browser at: http://localhost:8501\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6f1bd",
   "metadata": {},
   "source": [
    "## 26. (BONUS) Streamlit Deployment\n",
    "\n",
    "To deploy this model as a web app using Streamlit, create a separate `streamlit_app.py` file with the following code structure. This allows real-time predictions through an interactive UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed47c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ WARNING: Pipeline not yet loaded\n",
      "\n",
      "Please make sure you have executed these cells in order:\n",
      "  1. Cell 40: Save the Best Pipeline using joblib\n",
      "  2. Cell 42: Load the Saved Pipeline\n",
      "\n",
      "You can skip the GridSearchCV cells and use the provided sample pipeline instead.\n",
      "\n",
      "To use a sample trained model for testing predictions:\n",
      "  - Use: from src.predict import create_sample_customer, predict_customer\n",
      "  - Then test with sample data\n"
     ]
    }
   ],
   "source": [
    "# Check if pipeline has been loaded\n",
    "if 'loaded_pipeline' not in locals():\n",
    "    print(\"‚è≥ WARNING: Pipeline not yet loaded\")\n",
    "    print(\"\\nPlease make sure you have executed these cells in order:\")\n",
    "    print(\"  1. Cell 40: Save the Best Pipeline using joblib\")\n",
    "    print(\"  2. Cell 42: Load the Saved Pipeline\")\n",
    "    print(\"\\nYou can skip the GridSearchCV cells and use the provided sample pipeline instead.\")\n",
    "    print(\"\\nTo use a sample trained model for testing predictions:\")\n",
    "    print(\"  - Use: from src.predict import create_sample_customer, predict_customer\")\n",
    "    print(\"  - Then test with sample data\")\n",
    "else:\n",
    "    try:\n",
    "        # Create a sample new customer (raw unprocessed data)\n",
    "        new_customer = pd.DataFrame({\n",
    "            'customerID': ['CUST12345'],\n",
    "            'gender': ['Male'],\n",
    "            'SeniorCitizen': [0],\n",
    "            'Partner': ['No'],\n",
    "            'Dependents': ['No'],\n",
    "            'tenure': [12],\n",
    "            'PhoneService': ['Yes'],\n",
    "            'MultipleLines': ['No'],\n",
    "            'InternetService': ['Fiber optic'],\n",
    "            'OnlineSecurity': ['No'],\n",
    "            'OnlineBackup': ['No'],\n",
    "            'DeviceProtection': ['Yes'],\n",
    "            'TechSupport': ['No'],\n",
    "            'StreamingTV': ['Yes'],\n",
    "            'StreamingMovies': ['No'],\n",
    "            'Contract': ['Month-to-month'],\n",
    "            'PaperlessBilling': ['Yes'],\n",
    "            'PaymentMethod': ['Electronic check'],\n",
    "            'MonthlyCharges': [89.45],\n",
    "            'TotalCharges': [1185.50]\n",
    "        })\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"NEW CUSTOMER SAMPLE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nCustomer Details (Raw Input):\")\n",
    "        print(new_customer.to_string(index=False))\n",
    "\n",
    "        # Make prediction using the loaded pipeline\n",
    "        churn_prediction = loaded_pipeline.predict(new_customer)[0]\n",
    "        churn_probability = loaded_pipeline.predict_proba(new_customer)[0]\n",
    "\n",
    "        print(f\"\\n\\n{'=' * 80}\")\n",
    "        print(\"PREDICTION RESULT\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        print(f\"\\nChurn Prediction: {churn_prediction}\")\n",
    "        print(f\"  - Probability of NOT churning (No): {churn_probability[list(loaded_pipeline.classes_).index('No')]:.4f}\")\n",
    "        print(f\"  - Probability of CHURNING (Yes): {churn_probability[list(loaded_pipeline.classes_).index('Yes')]:.4f}\")\n",
    "\n",
    "        # Human readable output\n",
    "        if churn_prediction == 'Yes':\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: This customer is likely to CHURN\")\n",
    "            print(f\"    Recommended action: Provide retention offer\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ GOOD NEWS: This customer is likely to STAY\")\n",
    "            print(f\"    Recommended action: Continue standard service\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during prediction: {str(e)}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"  1. The pipeline file exists and is valid\")\n",
    "        print(\"  2. All customer columns match the training data\")\n",
    "        print(\"  3. Previous cells have executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c95ef",
   "metadata": {},
   "source": [
    "## 25. Create Sample Customer Input and Make Predictions\n",
    "\n",
    "Create a new customer sample in raw (unprocessed) format and use the loaded pipeline to predict churn probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abbee3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ WARNING: Model path not yet defined\n",
      "\n",
      "Please run Cell 36 (Save the Best Pipeline) first.\n",
      "\n",
      "Dependency chain:\n",
      "  1. GridSearchCV cells (21 & 26) must complete ‚úì/‚è≥\n",
      "  2. Cell 40 - Model Comparison\n",
      "  3. Cell 47 - Save the Best Pipeline\n",
      "  4. Cell 45 - Load the Saved Pipeline (current)\n",
      "\n",
      "Expected wait time: 30-60 minutes for GridSearchCV to complete.\n",
      "\n",
      "Once Cell 47 saves the model, this cell will load it successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if model_path has been defined\n",
    "if 'model_path' not in locals():\n",
    "    print(\"‚è≥ WARNING: Model path not yet defined\")\n",
    "    print(\"\\nPlease run Cell 36 (Save the Best Pipeline) first.\")\n",
    "    print(\"\\nDependency chain:\")\n",
    "    print(\"  1. GridSearchCV cells (21 & 26) must complete ‚úì/‚è≥\")\n",
    "    print(\"  2. Cell 40 - Model Comparison\")\n",
    "    print(\"  3. Cell 47 - Save the Best Pipeline\")\n",
    "    print(\"  4. Cell 45 - Load the Saved Pipeline (current)\")\n",
    "    print(\"\\nExpected wait time: 30-60 minutes for GridSearchCV to complete.\")\n",
    "    print(\"\\nOnce Cell 47 saves the model, this cell will load it successfully.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the saved pipeline\n",
    "        loaded_pipeline = joblib.load(model_path)\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"PIPELINE LOADED\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nPipeline loaded successfully from: {model_path}\")\n",
    "        print(f\"\\nLoaded pipeline structure:\")\n",
    "        print(loaded_pipeline)\n",
    "        print(f\"\\nThis loaded pipeline is ready for making predictions on new customers!\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Error: Model file not found at {model_path}\")\n",
    "        print(\"\\nMake sure Cell 47 (Save the Best Pipeline) has been executed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading pipeline: {str(e)}\")\n",
    "        print(\"\\nPlease ensure the model file is valid and all dependencies are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0572c",
   "metadata": {},
   "source": [
    "## 24. Load the Saved Pipeline\n",
    "\n",
    "Load the saved pipeline from the joblib file to demonstrate how it would be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b618611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ WARNING: Best model not yet available\n",
      "\n",
      "Please run Cell 40 (Model Comparison) first.\n",
      "\n",
      "Dependency chain:\n",
      "  1. GridSearchCV cells (21 & 26) must complete ‚úì/‚è≥\n",
      "  2. Cell 30 - Make predictions on test set\n",
      "  3. Cell 32 - Evaluate Logistic Regression\n",
      "  4. Cell 36 - Random Forest pipeline and hyperparameters\n",
      "  5. Cell 40 - Model Comparison (creates best_model)\n",
      "  6. Cell 47 - Save the Best Pipeline (current)\n",
      "\n",
      "Expected wait time: 30-60 minutes for GridSearchCV to complete.\n",
      "\n",
      "Once Cell 40 identifies the best model, this cell will save it successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if best_model exists\n",
    "if 'best_model' not in locals() or 'best_model_name' not in locals():\n",
    "    print(\"‚è≥ WARNING: Best model not yet available\")\n",
    "    print(\"\\nPlease run Cell 40 (Model Comparison) first.\")\n",
    "    print(\"\\nDependency chain:\")\n",
    "    print(\"  1. GridSearchCV cells (21 & 26) must complete ‚úì/‚è≥\")\n",
    "    print(\"  2. Cell 30 - Make predictions on test set\")\n",
    "    print(\"  3. Cell 32 - Evaluate Logistic Regression\")\n",
    "    print(\"  4. Cell 36 - Random Forest pipeline and hyperparameters\")\n",
    "    print(\"  5. Cell 40 - Model Comparison (creates best_model)\")\n",
    "    print(\"  6. Cell 47 - Save the Best Pipeline (current)\")\n",
    "    print(\"\\nExpected wait time: 30-60 minutes for GridSearchCV to complete.\")\n",
    "    print(\"\\nOnce Cell 40 identifies the best model, this cell will save it successfully.\")\n",
    "else:\n",
    "    try:\n",
    "        # Define the save path\n",
    "        model_path = './models/churn_pipeline.joblib'\n",
    "        \n",
    "        # Create models directory if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs('./models', exist_ok=True)\n",
    "        \n",
    "        # Save the best model\n",
    "        joblib.dump(best_model, model_path)\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"MODEL SAVED\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nBest Model: {best_model_name}\")\n",
    "        print(f\"Model saved to: {model_path}\")\n",
    "        print(f\"File size: {os.path.getsize(model_path) / 1024:.2f} KB\")\n",
    "        print(f\"\\nThis pipeline contains:\")\n",
    "        print(f\"  - ColumnTransformer (preprocessor)\")\n",
    "        print(f\"  - Classifier ({best_model_name})\")\n",
    "        print(f\"  - All trained parameters and weights\")\n",
    "        \n",
    "    except NameError as e:\n",
    "        print(f\"‚ùå Error: Missing required variables: {str(e)}\")\n",
    "        print(\"\\nMake sure Cell 40 (Model Comparison) has been executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving model: {str(e)}\")\n",
    "        print(\"\\nPlease ensure: \")\n",
    "        print(\"  1. The best_model and best_model_name variables are defined\")\n",
    "        print(\"  2. You have write permissions to the current directory\")\n",
    "        print(\"  3. The models directory can be created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63244dd",
   "metadata": {},
   "source": [
    "## 23. Save the Best Pipeline using joblib\n",
    "\n",
    "Save the best trained pipeline to a joblib file for later use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd27ad",
   "metadata": {},
   "source": [
    "## 22. Compare Logistic Regression vs Random Forest\n",
    "\n",
    "Compare performance of both models and select the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6dcf81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3377087163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Run GridSearchCV for Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m grid_search_rf = GridSearchCV(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrf_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mparam_grid_rf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Check if prerequisites exist\n",
    "if 'rf_pipeline' not in locals() or 'param_grid_rf' not in locals():\n",
    "    print(\"‚è≥ WARNING: Random Forest Pipeline not yet created\")\n",
    "    print(\"\\nPlease run Cell 54 (Create Random Forest Pipeline) first.\")\n",
    "    print(\"\\nDependency chain:\")\n",
    "    print(\"  1. Cell 23 - Detect feature types\")\n",
    "    print(\"  2. Cell 25 - Create numerical pipeline\")\n",
    "    print(\"  3. Cell 27 - Create categorical pipeline\")\n",
    "    print(\"  4. Cell 29 - Combine with ColumnTransformer\")\n",
    "    print(\"  5. Cell 54 - Create Random Forest Pipeline\")\n",
    "    print(\"  6. Cell 52 - Define Random Forest Hyperparameter Grid\")\n",
    "    print(\"  7. Cell 50 - Run GridSearchCV for Random Forest (current)\")\n",
    "else:\n",
    "    try:\n",
    "        if 'X_train' not in locals() or 'y_train' not in locals():\n",
    "            print(\"‚ùå Error: Training data not available\")\n",
    "            print(\"Please run data loading and preprocessing cells first.\")\n",
    "        else:\n",
    "            # Run GridSearchCV for Random Forest\n",
    "            grid_search_rf = GridSearchCV(\n",
    "                rf_pipeline,\n",
    "                param_grid_rf,\n",
    "                cv=5,\n",
    "                scoring='f1_weighted',\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            print(\"Starting Random Forest GridSearchCV...\")\n",
    "            print(f\"CV folds: 5\")\n",
    "            print(f\"Scoring metric: f1_weighted\")\n",
    "            print(f\"Total combinations to test: {len(param_grid_rf['classifier__n_estimators']) * len(param_grid_rf['classifier__max_depth']) * len(param_grid_rf['classifier__min_samples_split']) * len(param_grid_rf['classifier__min_samples_leaf'])}\")\n",
    "            print(f\"This may take 30-60 minutes depending on your system...\")\n",
    "            print()\n",
    "            \n",
    "            grid_search_rf.fit(X_train, y_train)\n",
    "            print(\"\\nRandom Forest GridSearchCV completed!\")\n",
    "\n",
    "            # Print best parameters and score\n",
    "            print(f\"\\nBest Random Forest Parameters:\")\n",
    "            for param, value in grid_search_rf.best_params_.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "            print(f\"\\nBest CV Score (F1-Weighted): {grid_search_rf.best_score_:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during Random Forest GridSearchCV: {str(e)}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"  1. rf_pipeline is defined\")\n",
    "        print(\"  2. param_grid_rf is defined\")\n",
    "        print(\"  3. X_train and y_train are available\")\n",
    "        print(\"  4. All preprocessing has been completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883da36c",
   "metadata": {},
   "source": [
    "## 21. Run GridSearchCV for Random Forest\n",
    "\n",
    "Execute GridSearchCV to find optimal hyperparameters for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753fe668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Hyperparameter Grid defined:\n",
      "\n",
      "n_estimators: [50, 100, 200]\n",
      "max_depth: [10, 20, 30, None]\n",
      "min_samples_split: [2, 5, 10]\n",
      "min_samples_leaf: [1, 2, 4]\n",
      "\n",
      "Total combinations: 108\n"
     ]
    }
   ],
   "source": [
    "# Random Forest hyperparameter grid\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [10, 20, 30, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "print(\"Random Forest Hyperparameter Grid defined:\")\n",
    "print(f\"\\nn_estimators: {param_grid_rf['classifier__n_estimators']}\")\n",
    "print(f\"max_depth: {param_grid_rf['classifier__max_depth']}\")\n",
    "print(f\"min_samples_split: {param_grid_rf['classifier__min_samples_split']}\")\n",
    "print(f\"min_samples_leaf: {param_grid_rf['classifier__min_samples_leaf']}\")\n",
    "print(f\"\\nTotal combinations: {len(param_grid_rf['classifier__n_estimators']) * len(param_grid_rf['classifier__max_depth']) * len(param_grid_rf['classifier__min_samples_split']) * len(param_grid_rf['classifier__min_samples_leaf'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb89ae",
   "metadata": {},
   "source": [
    "## 20. Define Random Forest Hyperparameter Grid\n",
    "\n",
    "Define hyperparameter grid for Random Forest tuning with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d884d07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4225177155.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m rf_pipeline = Pipeline(steps=[\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'classifier'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Random Forest Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "if 'preprocessor' not in locals():\n",
    "    print(\"‚è≥ WARNING: Preprocessor not yet available\")\n",
    "    print(\"\\nPlease run previous cells to create the preprocessor:\")\n",
    "    print(\"  1. Cell 25 - Create numerical pipeline\")\n",
    "    print(\"  2. Cell 27 - Create categorical pipeline\")\n",
    "    print(\"  3. Cell 29 - Combine with ColumnTransformer\")\n",
    "    print(\"\\nOnce the preprocessor is created, this cell will build the RF pipeline.\")\n",
    "else:\n",
    "    try:\n",
    "        rf_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "        print(\"Random Forest Pipeline created:\")\n",
    "        print(\"\\nPipeline structure:\")\n",
    "        print(f\"  Step 1: Preprocessor (ColumnTransformer)\")\n",
    "        print(f\"  Step 2: Classifier (RandomForestClassifier)\")\n",
    "        print(f\"\\nRandom Forest Parameters:\")\n",
    "        print(f\"  - n_estimators: 100 (default)\")\n",
    "        print(f\"  - max_depth: None (default)\")\n",
    "        print(f\"  - min_samples_split: 2 (default)\")\n",
    "        print(f\"  - min_samples_leaf: 1 (default)\")\n",
    "        print(f\"  - random_state: 42 (for reproducibility)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating Random Forest Pipeline: {str(e)}\")\n",
    "        print(\"\\nPlease ensure preprocessor is defined and all dependencies are complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ad6da",
   "metadata": {},
   "source": [
    "## 19. Create Random Forest Pipeline\n",
    "\n",
    "Create a new pipeline with Random Forest classifier for comparison with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbda2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1655499157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maccuracy_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprecision_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrecall_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "if 'y_pred_lr' not in locals() or 'y_pred_proba_lr' not in locals():\n",
    "    print(\"‚è≥ WARNING: Predictions not yet available\")\n",
    "    print(\"\\nPlease run Cell 58 (Make predictions on test set) first.\")\n",
    "    print(\"\\nThis cell depends on:\")\n",
    "    print(\"  1. y_pred_lr - predictions from Logistic Regression\")\n",
    "    print(\"  2. y_pred_proba_lr - prediction probabilities\")\n",
    "    print(\"\\nExpected wait time: 15-30 minutes for GridSearchCV to complete.\")\n",
    "else:\n",
    "    try:\n",
    "        accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "        precision_lr = precision_score(y_test, y_pred_lr, pos_label='Yes')\n",
    "        recall_lr = recall_score(y_test, y_pred_lr, pos_label='Yes')\n",
    "        f1_lr = f1_score(y_test, y_pred_lr, pos_label='Yes')\n",
    "        roc_auc_lr = roc_auc_score(y_test.map({'No': 0, 'Yes': 1}), y_pred_proba_lr)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"LOGISTIC REGRESSION - MODEL EVALUATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nAccuracy:   {accuracy_lr:.4f}\")\n",
    "        print(f\"Precision:  {precision_lr:.4f}\")\n",
    "        print(f\"Recall:     {recall_lr:.4f}\")\n",
    "        print(f\"F1-Score:   {f1_lr:.4f}\")\n",
    "        print(f\"ROC-AUC:    {roc_auc_lr:.4f}\")\n",
    "\n",
    "        print(f\"\\n\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_test, y_pred_lr)\n",
    "        print(cm)\n",
    "\n",
    "        print(f\"\\n\\nDetailed Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred_lr))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {str(e)}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"  1. Predictions (y_pred_lr, y_pred_proba_lr) are available\")\n",
    "        print(\"  2. y_test data is available\")\n",
    "        print(\"  3. All previous cells have executed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4531f36",
   "metadata": {},
   "source": [
    "## 18. Evaluate Logistic Regression Model\n",
    "\n",
    "Evaluate the Logistic Regression model using accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85594831",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-173288924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred_proba_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search_lr' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "if 'grid_search_lr' not in locals():\n",
    "    print(\"‚è≥ WARNING: Logistic Regression GridSearchCV not yet available\")\n",
    "    print(\"\\nPlease run Cell 26 (Define LR Hyperparameter Grid) first.\")\n",
    "    print(\"\\nDependency chain:\")\n",
    "    print(\"  1. Cell 31 - Build LR pipeline\")\n",
    "    print(\"  2. Cell 38 - Define LR hyperparameter grid\")\n",
    "    print(\"  3. Cell 36 - Configure LR GridSearchCV\")\n",
    "    print(\"  4. Cell 34 - Train LR GridSearchCV on training data\")\n",
    "    print(\"  5. Cell 58 - Make predictions on test set (current)\")\n",
    "    print(\"\\nExpected wait time: 15-30 minutes for GridSearchCV to complete.\")\n",
    "else:\n",
    "    try:\n",
    "        y_pred_lr = grid_search_lr.predict(X_test)\n",
    "        y_pred_proba_lr = grid_search_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"LOGISTIC REGRESSION - TEST SET PREDICTIONS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nTotal test samples: {len(y_test)}\")\n",
    "        print(f\"Predicted churners: {sum(y_pred_lr == 'Yes')}\")\n",
    "        print(f\"Predicted non-churners: {sum(y_pred_lr == 'No')}\")\n",
    "        print(f\"\\nFirst 10 predictions:\")\n",
    "        print(y_pred_lr[:10])\n",
    "        print(f\"\\nFirst 10 prediction probabilities:\")\n",
    "        print(y_pred_proba_lr[:10])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during prediction: {str(e)}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"  1. grid_search_lr has completed training\")\n",
    "        print(\"  2. X_test and y_test are available\")\n",
    "        print(\"  3. All preprocessing has been completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f384f2b",
   "metadata": {},
   "source": [
    "## 17. Predict Churn on Test Dataset\n",
    "\n",
    "Use the best trained Logistic Regression model to make predictions on the test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
